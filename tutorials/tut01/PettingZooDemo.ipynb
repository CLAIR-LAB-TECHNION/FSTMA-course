{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "614b3c58-f27e-425b-b8ec-2f1fd2cce84b",
   "metadata": {
    "tags": []
   },
   "source": [
    "# PettingZoo Speaker-Listener Environment Demonstration\n",
    "\n",
    "## Introduction\n",
    "\n",
    "[PettingZoo](https://www.pettingzoo.ml/) is a Python library for conducting research in multi-agent reinforcement learning, akin to a multi-agent version of [Gym](https://github.com/openai/gym). It implements a variety of environments, including:\n",
    "- [Atari](https://www.pettingzoo.ml/atari): Multi-player Atari 2600 games (cooperative, competitive and mixed sum)\n",
    "- [Butterfly](https://www.pettingzoo.ml/butterfly): Cooperative graphical games developed by us, requiring a high degree of coordination\n",
    "- [Classic](https://www.pettingzoo.ml/classic): Classical games including card games, board games, etc.\n",
    "- [MAgent](https://www.pettingzoo.ml/magent): Configurable environments with massive numbers of particle agents, originally from https://github.com/geek-ai/MAgent\n",
    "- [MPE](https://www.pettingzoo.ml/mpe): A set of simple nongraphical communication tasks, originally from https://github.com/openai/multiagent-particle-envs\n",
    "- [SISL](https://www.pettingzoo.ml/sisl): 3 cooperative environments, originally from https://github.com/sisl/MADRL\n",
    "\n",
    "<img src=\"https://www.pettingzoo.ml/mpe/mpe_simple_speaker_listener.gif\" width=\"500\" align=\"center\"/>\n",
    "\n",
    "The [Simple Speaker Listener Environment](https://www.pettingzoo.ml/mpe/simple_speaker_listener) is implemented in the MPE library. It is a 2-agent environment in which one agent, the \"speaker\", has information about the goal and has a limmited mode of communication with the second agent, the \"listener\", which must use the speaker's communications and its limitted observations to navigate a 2D space toward the goal. The speaker agent cannot navigate, and the listener object cannot communicate.\n",
    "\n",
    "<img src=\"images/speaker_listener_screenshot.png\" width=\"500\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26db1ca7-314b-4d99-8837-67296eda297f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pettingzoo[mpe] in /usr/local/Caskroom/miniconda/base/envs/fstma-tut01/lib/python3.8/site-packages (1.17.0)\n",
      "Requirement already satisfied: numpy>=1.18.0 in /usr/local/Caskroom/miniconda/base/envs/fstma-tut01/lib/python3.8/site-packages (from pettingzoo[mpe]) (1.22.3)\n",
      "Requirement already satisfied: gym>=0.21.0 in /usr/local/Caskroom/miniconda/base/envs/fstma-tut01/lib/python3.8/site-packages (from pettingzoo[mpe]) (0.22.0)\n",
      "Requirement already satisfied: pyglet>=1.4.0 in /usr/local/Caskroom/miniconda/base/envs/fstma-tut01/lib/python3.8/site-packages (from pettingzoo[mpe]) (1.5.23)\n",
      "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/Caskroom/miniconda/base/envs/fstma-tut01/lib/python3.8/site-packages (from gym>=0.21.0->pettingzoo[mpe]) (0.0.6)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/Caskroom/miniconda/base/envs/fstma-tut01/lib/python3.8/site-packages (from gym>=0.21.0->pettingzoo[mpe]) (2.0.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.10.0 in /usr/local/Caskroom/miniconda/base/envs/fstma-tut01/lib/python3.8/site-packages (from gym>=0.21.0->pettingzoo[mpe]) (4.11.3)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/Caskroom/miniconda/base/envs/fstma-tut01/lib/python3.8/site-packages (from importlib-metadata>=4.10.0->gym>=0.21.0->pettingzoo[mpe]) (3.7.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install 'pettingzoo[mpe]'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac223ec-ee40-4223-bfb0-4bad186323b0",
   "metadata": {},
   "source": [
    "## Environment Description\n",
    "\n",
    "The latest implementation of the speaker-listener environment is `simple_speaker_listener_v3`. We can create an environment instance using the `env` function which accepts two parameters:\n",
    "1. `max_cycles` - the number of actions each agent can perform before the end of the episode. _default=25_\n",
    "2. `continuous_actions` - if `True`, both the speaker and the listener have a continuous action space. otherwise they are discrete, finite spaces. _default=False_\n",
    "\n",
    "The environment object implements many usful tools to help understand and properly utilize the environment. Below we use the `agents` attribute toiterate over the agent names, and the `observation_space` and `action_space` functions to show the [gym spaces](https://gym.openai.com/docs/#spaces) for the agents observation and action spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "063ae7f1-49fa-46fe-b475-a67ad8fb99c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "discrete actions:\n",
      "- agent 1: speaker_0\n",
      "\t- observation space: Box(-inf, inf, (3,), float32)\n",
      "\t- action space: Discrete(3)\n",
      "- agent 2: listener_0\n",
      "\t- observation space: Box(-inf, inf, (11,), float32)\n",
      "\t- action space: Discrete(5)\n",
      "\n",
      "continuous actions:\n",
      "- agent 1: speaker_0\n",
      "\t- observation space: Box(-inf, inf, (3,), float32)\n",
      "\t- action space: Box(0.0, 1.0, (3,), float32)\n",
      "- agent 2: listener_0\n",
      "\t- observation space: Box(-inf, inf, (11,), float32)\n",
      "\t- action space: Box(0.0, 1.0, (5,), float32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from pettingzoo.mpe import simple_speaker_listener_v3\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def print_env_info(continuous_actions):\n",
    "    env = simple_speaker_listener_v3.env(continuous_actions=continuous_actions)\n",
    "    env.reset()\n",
    "    \n",
    "    print('continuous actions:' if continuous_actions else 'discrete actions:')\n",
    "    \n",
    "    for i, agent in enumerate(env.agents, 1):\n",
    "        print(f'- agent {i}: {agent}')\n",
    "        print(f'\\t- observation space: {env.observation_space(agent)}')\n",
    "        print(f'\\t- action space: {env.action_space(agent)}')\n",
    "\n",
    "\n",
    "print_env_info(continuous_actions=False)\n",
    "print()\n",
    "print_env_info(continuous_actions=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a0adb1a-678e-4084-9a8b-652c6b8605b8",
   "metadata": {},
   "source": [
    "### Observation Spaces\n",
    "\n",
    "The `Box(low, high, shape, dtype)` space contains any vector of shape `shape` that contains only values within the closed interval between `low` and `high` represented as type `dtype`. Both the speaker and the listener receive one dimensional `Box` observations of different sizes with any 32-bit floating point value. Note that the observation spaces remain the same regardless of the action space type (continuous / discrete). We can get the next acting agent's observation using the environment's `last` function, which returns the previous observation, reward, \"done\" flag, info dictionary. The current acting agent is chosen sequentially according to the agents' order in the `agents` attribute. The next agent is chosen when calling the `step` function which is sets the agent's action.\n",
    "\n",
    "#### Speaker\n",
    "The speaker observation is of type `Box(-inf, inf, (3,), float32)`, which is any vector of 3 dimensions. The values represent the RGB color of the goal to which the listener must navigate to maximize rewards.\n",
    "\n",
    "#### Listener\n",
    "The speaker observation is of type `Box(-inf, inf, (11,), float32)`, which is any vector of 11 dimensions. The first two values are the agent's velocity in 2D space. The next six values are the red, blue, and green landmarks' positions relative to the listener. The last three values correspond to communication received from the speaker. Below is a precise ordering of the values in the observation vector:\n",
    "1. listener agent velocity X\n",
    "2. listener agent velocity Y\n",
    "3. red landmark X pos - listener agent X pos\n",
    "4. red landmark Y pos - listener agent Y pos\n",
    "5. blue landmark X pos - listener agent X pos\n",
    "6. blue landmark Y pos - listener agent Y pos\n",
    "7. green landmark X pos - listener agent X pos\n",
    "8. green landmark Y pos - listener agent Y pos\n",
    "9. communication channel 1\n",
    "10. communication channel 2\n",
    "11. communication channel 3\n",
    "\n",
    "Note that the communication observation (values 9, 10, and 11) will always be 0 in the first round since no communication has yet been received from the speaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c5cd7be0-def6-4917-ace2-3433e6d32aaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agnet: speaker_0\n",
      "observation: [0.65 0.15 0.15]\n",
      "\n",
      "agnet: listener_0\n",
      "observation: [ 0.          0.         -1.1573222   0.05878095  0.4423617  -0.9294968\n",
      "  0.70731    -1.4845002   0.          0.          0.        ]\n",
      "\n",
      "agnet: speaker_0\n",
      "observation: [0.65 0.15 0.15]\n",
      "\n",
      "agnet: listener_0\n",
      "observation: [-0.5         0.         -1.1073222   0.05878095  0.4923617  -0.9294968\n",
      "  0.75731    -1.4845002   1.          0.          0.        ]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "env = simple_speaker_listener_v3.env()\n",
    "env.reset()  # reset the environment, selected agent is \"speaker_0\"\n",
    "\n",
    "# run twice to show the chnage in the communication vector\n",
    "for i in range(2):\n",
    "    #speaker obs\n",
    "    obs, _, _, _ = env.last()  # get speaker observation vector\n",
    "    print(f'agnet: {env.agents[0]}')\n",
    "    print(f'observation: {obs}')\n",
    "    print()\n",
    "    env.step(0)  # send discrete message \"A\". next agent is selected (listener_0)\n",
    "    \n",
    "    obs, _, _, _ = env.last()  # get listener observation vector\n",
    "    print(f'agnet: {env.agents[1]}')\n",
    "    print(f'observation: {obs}')\n",
    "    print()\n",
    "    env.step(1)  # perform the \"go left\" action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "964817d1-264a-4c0f-b571-1a8f3dd65653",
   "metadata": {},
   "source": [
    "### Action Spaces\n",
    "\n",
    "The action agents' action spaces can be either discrete or continuous, depending on the `continuous_actions` parameter. If discrete, the action spaces are of type `Discrete(n)`, which contains the integer values 0 to n-1. Otherwise, the action spaces are of type `Box` (like the observation spaces), but with values constrained between 0 and 1. Actions are given sequentially by agent order according the `agents` attribute by using the `step` function. The given action must be one from the corresponding agent's action space.\n",
    "\n",
    "#### Discrete Actions\n",
    "\n",
    "##### Speaker\n",
    "The action space is `Discrete(3)` containing the values 0, 1, 2. Each value corresponds to a possible message. Value 0 corresponds to message A, which can be seen in the communication vector of the listener's observation as \\[1, 0, 0\\]. Similarly, values 1 and 2 correspond to messages B and C and appear as \\[0, 1, 0\\] and \\[0, 0, 1\\] in the listener's observation respectively.\n",
    "\n",
    "##### Listener\n",
    "Discrete:  \n",
    "The action space is `Discrete(5)` containing the values 0 - 4. Each value applies force on the agent, increasing its velocity to some direction. The velocity will slowly deteriorate until the agent stops, unless constant force is applied. The values' meanings are as follows:\n",
    "* 0 - do nothing\n",
    "* 1 - push left (add velocity in negative x-axis direction)\n",
    "* 2 - push right (add velocity in positive x-axis direction)\n",
    "* 3 - push down (add velocity in negative y-axis direction)\n",
    "* 4 - push up (add velocity in positive t-axis direction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "37f69779-5ff1-4f1e-96d6-c1851dcc3fd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agnet: speaker_0\n",
      "observation: [0.65 0.15 0.15]\n",
      "\n",
      "agnet: listener_0\n",
      "observation: [ 0.          0.         -1.1739286   1.2495128  -0.5537686   1.1765938\n",
      " -1.0880105   0.92581254  0.          0.          0.        ]\n",
      "\n",
      "agnet: speaker_0\n",
      "observation: [0.65 0.15 0.15]\n",
      "\n",
      "agnet: listener_0\n",
      "observation: [-0.5         0.         -1.1239287   1.2495128  -0.50376856  1.1765938\n",
      " -1.0380106   0.92581254  1.          0.          0.        ]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# speaker action-to-index dict\n",
    "SPEAKER_DISCRETE_ACTIONS = {\n",
    "    'A': 0,\n",
    "    'B': 1,\n",
    "    'C': 2\n",
    "}\n",
    "\n",
    "# listener action-to-index dict\n",
    "LISTENER_DISCRETE_ACTIONS = {\n",
    "    'nothing': 0,\n",
    "    'left':    1,\n",
    "    'right':   2,\n",
    "    'down':    3,\n",
    "    'up':      4\n",
    "}\n",
    "\n",
    "env = simple_speaker_listener_v3.env(continuous_actions=False)  # discrete actions env\n",
    "env.reset()\n",
    "\n",
    "# CHANGE ACTION ACCORDING TO THE SPEADER TABLE AND SEE THE LISTENER'S COMMUNICATION OBSERVATIONS CHANGE\n",
    "chosen_speaker_action = SPEAKER_DISCRETE_ACTIONS['A']\n",
    "\n",
    "# CHANGE ACTION ACCORDING TO THE LISTENER TABLE AND SEE THE VELOCITY OBSERVATIONS CHANGE\n",
    "chosen_listener_action = LISTENER_DISCRETE_ACTIONS['left']\n",
    "\n",
    "# run twice to show the chnage in the communication vector\n",
    "for i in range(2):\n",
    "    #speaker action\n",
    "    obs, _, _, _ = env.last()\n",
    "    print(f'agnet: {env.agents[0]}')\n",
    "    print(f'observation: {obs}')\n",
    "    print()\n",
    "    env.step(chosen_speaker_action)  \n",
    "    \n",
    "    # listener action\n",
    "    obs, _, _, _ = env.last()\n",
    "    print(f'agnet: {env.agents[1]}')\n",
    "    print(f'observation: {obs}')\n",
    "    print()\n",
    "    env.step(chosen_listener_action)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f9486a6-7673-46fc-98a6-be75c04bcd00",
   "metadata": {},
   "source": [
    "#### Continuous Actions\n",
    "\n",
    "##### Speaker\n",
    "The action space is `Box(0.0, 1.0, (3,), float32)` containing 3D vectors of values in \\[0, 1\\]. The values have no specific meaning, and are given, as is, as the listener observation's communication vector.\n",
    "\n",
    "##### Listener\n",
    "The action space is `Box(0.0, 1.0, (5,), float32)` containing 3D vectors of values in \\[0, 1\\]. Each value describes the amount of force applied in each direction (left, right, up, and down). Below is a precise ordering of the values in the action vector:\n",
    "1. No force (useful for some implementations)\n",
    "2. Force right (positive in x-axis)\n",
    "3. Force left (negative in x-axis)\n",
    "4. Force up (positive in y-axis)\n",
    "5. Force down (negative in y-axis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6337af45-51fe-437d-8cac-323a7106997c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agnet: speaker_0\n",
      "observation: [0.15 0.15 0.65]\n",
      "\n",
      "agnet: listener_0\n",
      "observation: [ 0.          0.         -0.5547875   0.23933914  1.2868022  -0.4283366\n",
      "  0.5035991   0.9161092   0.          0.          0.        ]\n",
      "\n",
      "agnet: speaker_0\n",
      "observation: [0.15 0.15 0.65]\n",
      "\n",
      "agnet: listener_0\n",
      "observation: [ 0.         -0.09999999 -0.5547875   0.24933913  1.2868022  -0.4183366\n",
      "  0.5035991   0.9261092   0.5         1.          0.2       ]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# speaker continuous action function\n",
    "def speaker_continuous_action(v1, v2, v3):\n",
    "    return np.array([v1, v2, v3], dtype=np.float32)\n",
    "\n",
    "# listener continuous action function\n",
    "def listener_continuous_action(right, left, up, down):\n",
    "    return np.array([0, right, left, up, down], dtype=np.float32)\n",
    "\n",
    "env = simple_speaker_listener_v3.env(continuous_actions=True)  # continuous actions env\n",
    "env.reset()\n",
    "\n",
    "# CHANGE ACTION AS NEEDED\n",
    "chosen_speaker_action = speaker_continuous_action(v1=0.5, v2=1.0, v3=0.2)\n",
    "\n",
    "# CHANGE ACTION AS NEEDED\n",
    "chosen_listener_action = listener_continuous_action(right=0.8, left=0.8, up=0.5, down=0.7)\n",
    "\n",
    "# run twice to show the chnage in the communication vector\n",
    "for i in range(2):\n",
    "    #speaker action\n",
    "    obs, _, _, _ = env.last()\n",
    "    print(f'agnet: {env.agents[0]}')\n",
    "    print(f'observation: {obs}')\n",
    "    print()\n",
    "    env.step(chosen_speaker_action)  \n",
    "    \n",
    "    # listener action\n",
    "    obs, _, _, _ = env.last()\n",
    "    print(f'agnet: {env.agents[1]}')\n",
    "    print(f'observation: {obs}')\n",
    "    print()\n",
    "    env.step(chosen_listener_action)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae4ed6b-ff44-4f51-b4a0-6a9283ab965a",
   "metadata": {},
   "source": [
    "## Running the Environment\n",
    "\n",
    "We can run a game simulation by resetting an environment and playing out the episode. This is done by iterating over the agents repeatedly, providing an action for each agent at every iteration, until we wish to stop or until the `max_cycles` limit has been reached. In the example below, we define a policy function to generate random actions within each agent's action space.\n",
    "\n",
    "### Policies\n",
    "\n",
    "We implement a policy as a function that, given the current observation, returns an action to perform. Below we define a policy class that supports both `Discrete` and `Box` type action spaces that completely ignores the observation and samples a random valid action from the given action space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "77711b6a-8570-46e5-bc5e-88edac9aa21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym.spaces import Discrete, Box\n",
    "\n",
    "# define a random policy for continuous action agents.\n",
    "# the policy returns a numpy array of the action space shape with random values between 0 and 1.\n",
    "class RandomPolicy:\n",
    "    def __init__(self, action_space):\n",
    "        # choose a policy function for this action space type\n",
    "        if isinstance(action_space, Discrete):  # discrete action policy\n",
    "            self.policy_fn = self.__discrete_policy\n",
    "        elif isinstance(action_space, Box):  # continuous action policy\n",
    "            self.policy_fn = self.__continuous_policy\n",
    "        else:  # other types are not supported\n",
    "            raise TypeError(f'action_space must be of type Box or Discrete. got {type(action_space).__name__}')\n",
    "        \n",
    "        self.action_space = action_space\n",
    "        \n",
    "    def __call__(self, observation):\n",
    "        # we completely ignore the observation and create a random valid action.\n",
    "        return self.policy_fn()\n",
    "    \n",
    "    def __discrete_policy(self):\n",
    "        # a random number within the discrete action range\n",
    "        return np.random.randint(self.action_space.n)\n",
    "    \n",
    "    def __continuous_policy(self):\n",
    "        # a random vector within the continuous range of the appropriate dimensionality\n",
    "        # convert to the right dtype to avoid clipping warnings (e.g. float64 to float32)\n",
    "        return np.random.uniform(self.action_space.low, self.action_space.high, self.action_space.shape).astype(self.action_space.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeed1034-f3a8-45db-ae98-e023993fb294",
   "metadata": {},
   "source": [
    "### Simulation\n",
    "\n",
    "We now define an environment with either a discrete or continuous action space that limits to a small number of steps, for the purposes of this demonstration, using the `max_cycles` parameter. We then create a policy for each agent and start iterating over them repeatedly. For this, the environment implements the `agent_iter` function. This function simply iterates over the list of agents `max_cycles` times, allowing us to perform `max_cycles` steps for each agent. An added bonus of using `agent_iter` is that it raises an error if there was no `step` call within the iteration (which can prevent horrible bugs). After the episode has ended, the agents' \"done\" status will be true, and . In this case the episode is complete and the environment must be reset if we wish to run it again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2b9ff00b-0939-47f9-acf8-eb3a7de90e82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speaker_0 reward:      0.0\n",
      "speaker_0 observation: [0.15 0.65 0.15]\n",
      "speaker_0 action:      [0.7946723  0.82636803 0.71717334]\n",
      "\n",
      "listener_0 reward:      0.0\n",
      "listener_0 observation: [ 0.          0.         -1.0930883  -1.4339958  -1.3082466  -1.5381738\n",
      " -0.51077074 -0.5750493   0.          0.          0.        ]\n",
      "listener_0 action:      [0.17187975 0.81018907 0.45053533 0.829903   0.79029036]\n",
      "\n",
      "speaker_0 reward:      -4.130959731095398\n",
      "speaker_0 observation: [0.15 0.65 0.15]\n",
      "speaker_0 action:      [0.3222627 0.4749429 0.4140355]\n",
      "\n",
      "listener_0 reward:      -4.130959731095398\n",
      "listener_0 observation: [ 0.17982687  0.01980633 -1.111071   -1.4359765  -1.3262293  -1.5401543\n",
      " -0.5287534  -0.57702994  0.7946723   0.82636803  0.71717334]\n",
      "listener_0 action:      [0.19847904 0.7819158  0.15683617 0.35229257 0.48536792]\n",
      "\n",
      "speaker_0 reward:      -4.235741904178669\n",
      "speaker_0 observation: [0.15 0.65 0.15]\n",
      "speaker_0 action:      [0.8946533  0.27243754 0.10652413]\n",
      "\n",
      "listener_0 reward:      -4.235741904178669\n",
      "listener_0 observation: [ 0.44740996 -0.05168293 -1.155812   -1.4308082  -1.3709704  -1.5349861\n",
      " -0.57349443 -0.5718616   0.3222627   0.4749429   0.4140355 ]\n",
      "listener_0 action:      [0.95802563 0.01640942 0.76173276 0.152433   0.28706357]\n",
      "\n",
      "speaker_0 reward:      -4.1931289496859545\n",
      "speaker_0 observation: [0.15 0.65 0.15]\n",
      "speaker_0 action:      [0.03315373 0.14675078 0.28907424]\n",
      "\n",
      "listener_0 reward:      -4.1931289496859545\n",
      "listener_0 observation: [-0.0371042  -0.10607749 -1.1521016  -1.4202005  -1.3672599  -1.5243783\n",
      " -0.569784   -0.5612539   0.8946533   0.27243754  0.10652413]\n",
      "listener_0 action:      [0.95760113 0.34873953 0.19533828 0.1108053  0.7394257 ]\n",
      "\n",
      "speaker_0 reward:      -4.087987560236064\n",
      "speaker_0 observation: [0.15 0.65 0.15]\n",
      "speaker_0 action:      [0.53344965 0.04202735 0.14983924]\n",
      "\n",
      "listener_0 reward:      -4.087987560236064\n",
      "listener_0 observation: [ 0.04887247 -0.39386833 -1.1569889  -1.3808136  -1.3721472  -1.4849916\n",
      " -0.57467127 -0.52186704  0.03315373  0.14675078  0.28907424]\n",
      "listener_0 action:      [0.9721993  0.90194416 0.843312   0.36552763 0.5204974 ]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# CHOOSE MAX CYCLES AND DISCRETE OR CONTINUOUS ACTION SPACE\n",
    "env = simple_speaker_listener_v3.env(max_cycles=5, continuous_actions=True)\n",
    "env.reset()\n",
    "\n",
    "# create a random policy for both agents' action spaces.\n",
    "policies = {\n",
    "    env.agents[0]: RandomPolicy(env.action_space(env.agents[0])),\n",
    "    env.agents[1]: RandomPolicy(env.action_space(env.agents[1]))\n",
    "}\n",
    "\n",
    "# iterate over agents until the episode is complete\n",
    "for agent in env.agent_iter():\n",
    "    observation, reward, done, info = env.last()\n",
    "\n",
    "    # if done, the episode is complete. no more actions can be taken\n",
    "    if done:\n",
    "        break\n",
    "    \n",
    "    # choose an action and execute\n",
    "    action = policies[agent](observation)\n",
    "    env.step(action)\n",
    "    \n",
    "    # log everything\n",
    "    print(f'{agent} reward:      {reward}')\n",
    "    print(f'{agent} observation: {observation}')\n",
    "    print(f'{agent} action:      {action}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f1adc9c-a532-4fc1-9f01-61c912cda81a",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Rendering\n",
    "\n",
    "We can render the environment to see visualize the observation space. We must call the `render` function at every iteration to create and update a rendering in a separate window. Below we show a 100-step episode controlled by our random policies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "699fcf4a-9bbe-4c76-b045-e85228919a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# long episode for interesting rendering.\n",
    "env = simple_speaker_listener_v3.env(max_cycles=100, continuous_actions=True)\n",
    "env.reset()\n",
    "\n",
    "# create a random policy for both agents' action spaces.\n",
    "policies = {\n",
    "    env.agents[0]: RandomPolicy(env.action_space(env.agents[0])),\n",
    "    env.agents[1]: RandomPolicy(env.action_space(env.agents[1]))\n",
    "}\n",
    "\n",
    "# run an episode\n",
    "for agent in env.agent_iter():\n",
    "    observation, reward, done, info = env.last()\n",
    "    \n",
    "    # stop if done\n",
    "    if done:\n",
    "        break\n",
    "    \n",
    "    # choose and execute action\n",
    "    action = policies[agent](observation)\n",
    "    env.step(action)\n",
    "    \n",
    "    # render the environment\n",
    "    env.render('human')\n",
    "\n",
    "# This line is SUPPOSED to close the rendering window, but it does not.\n",
    "# restart the kernel close the window, and then don't run this cell again.\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d1477e-81d8-4ac3-9bf8-2923060dfd6d",
   "metadata": {},
   "source": [
    "## Advanced Tools\n",
    "\n",
    "### World Model\n",
    "\n",
    "All MPE environments define a world with customizable physical properties that make up the world model. The `pettingzoo.mpe._mpe_utils.core.World` object is the template for such a world. It contains a collection of `Entity` objects, divided into agents and landmarks. Both the world object and the different entities have physical attributes that affect transitions within the environment.\n",
    "\n",
    "Let us explore the Simple Speaker Listener environment's world attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "204cc623-b088-4f06-b2ab-7388c0c59ddd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'agents': [<pettingzoo.mpe._mpe_utils.core.Agent at 0x7f9c079268e0>,\n",
       "  <pettingzoo.mpe._mpe_utils.core.Agent at 0x7f9c079263d0>],\n",
       " 'landmarks': [<pettingzoo.mpe._mpe_utils.core.Landmark at 0x7f9c079269a0>,\n",
       "  <pettingzoo.mpe._mpe_utils.core.Landmark at 0x7f9c07926a90>,\n",
       "  <pettingzoo.mpe._mpe_utils.core.Landmark at 0x7f9c07926130>],\n",
       " 'dim_c': 3,\n",
       " 'dim_p': 2,\n",
       " 'dim_color': 3,\n",
       " 'dt': 0.1,\n",
       " 'damping': 0.25,\n",
       " 'contact_force': 100.0,\n",
       " 'contact_margin': 0.001,\n",
       " 'collaborative': True}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = simple_speaker_listener_v3.env(continuous_actions=True)\n",
    "env.reset()\n",
    "\n",
    "# list environment \"world\" attributes\n",
    "vars(env.unwrapped.world)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b32545a0-1982-42be-8fcf-c5126db00ba7",
   "metadata": {},
   "source": [
    "`agents` and `landmarks` are the world entities. The `dim_X` attributes define the dimensions of values in the environment, e.g., `dim_p` defines the world position dimensions (the default is a 2d world). The rest are physical properties that affect transitions:\n",
    "- `dt` - time units per step\n",
    "- `damping` - applies a multiplicative drag on moving agents.\n",
    "- `contact_force` and `contact_margin` - used to calculate collision force\n",
    "\n",
    "To complete our view of the world, let us explore an entity. Specifically, below are the attributes of the listener agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7df31c8f-7ba9-423b-993b-065afb6ba988",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'listener_0',\n",
       " 'size': 0.075,\n",
       " 'movable': True,\n",
       " 'collide': False,\n",
       " 'density': 25.0,\n",
       " 'color': array([0.6, 1.1, 0.6]),\n",
       " 'max_speed': None,\n",
       " 'accel': None,\n",
       " 'state': <pettingzoo.mpe._mpe_utils.core.AgentState at 0x7f9c079263a0>,\n",
       " 'initial_mass': 1.0,\n",
       " 'silent': True,\n",
       " 'blind': False,\n",
       " 'u_noise': None,\n",
       " 'c_noise': None,\n",
       " 'u_range': 1.0,\n",
       " 'action': <pettingzoo.mpe._mpe_utils.core.Action at 0x7f9c07926250>,\n",
       " 'action_callback': None,\n",
       " 'goal_a': None,\n",
       " 'goal_b': None}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# list \"listener\" agent entity attributes\n",
    "vars(env.unwrapped.world.agents[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de3e7274-09d5-47fa-8bec-9beebe5a1ce2",
   "metadata": {},
   "source": [
    "Some interesting attributes:\n",
    "\n",
    "- `movable` - if `True`, the entity can change position via actions / collisions.\n",
    "- `collide` - if `True`, the entity can collide with other entities with `collide=True`\n",
    "- `max_speed` - a maximal limit on the velocity norm. if `None`, the speed is not bounded.\n",
    "- `accel` - a constant value that scales the force acceleration. The default is `None` which defaults to `5`.\n",
    "- `state` - contains the position, velocity, and communications vector of the entity.\n",
    "- `initial_mass` - the entity's mass\n",
    "- `u_noise` and `c_noise` - the standard deviation for additive, zero-mean Gaussian noise for the action force and communications respectively. `None` is equivalent to 0.\n",
    "- `u_range` - The maximal force that can be applied to the agent in any axis.\n",
    "\n",
    "As we can see, this environment has one movable agent that cannot collide with other entities, and whos actions are deterministic (i.e., no noise). In this scenario, when the listener performs its action `[x, right, left, up, down]` at time step $t$, the velocity and position of the agent is calculated as follows:\n",
    "$$u_{t} = [\\text{right} - \\text{left}, \\text{up} - \\text{down}]$$\n",
    "$$v_{t + 1} = v_{t}\\cdot (1 - \\text{damping}) + \\frac{u_{t} \\cdot\\text{accel}}{\\text{mass}}\\cdot \\text{dt}$$\n",
    "$$x_{t + 1} = x_{t} + v_{t + 1}\\cdot \\text{dt}$$\n",
    "\n",
    "The example below demonstrates how to extract use the world's physical attributes to determine the next velocity and position of the listener, and compares them to the actual environment update. We also show that it is possible to customize the environment by directly changing the `damping` value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dac11d40-96b9-46f6-8758-108658b0da77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "expected next v: [0.25 0.  ]\n",
      "actual next v:   [0.25 0.  ]\n",
      "\n",
      "expected next x: [0.44215233 0.92596952]\n",
      "actual next x:   [0.44215233 0.92596952]\n"
     ]
    }
   ],
   "source": [
    "# get world and listener agent instances\n",
    "world = env.unwrapped.world\n",
    "listener = world.agents[1]\n",
    "\n",
    "# change world damping\n",
    "world.damping = 0.05\n",
    "\n",
    "# get constants\n",
    "dt = world.dt\n",
    "damping = world.damping\n",
    "accel = listener.accel or 5  # defaults to 5 if None\n",
    "mass = listener.mass\n",
    "v_prev = listener.state.p_vel\n",
    "x_prev = listener.state.p_pos\n",
    "\n",
    "# SET ACTION\n",
    "# values must be within [0, u_range]\n",
    "# x - nothing\n",
    "# r - right\n",
    "# l - left\n",
    "# u - up\n",
    "# d - down        [x, r,   l,    u,    d]\n",
    "ACTION = np.array([0, 1, 0.5, 0.75, 0.75], dtype=np.float32)\n",
    "\n",
    "# calculate the applied force\n",
    "# [right - left, up - down]\n",
    "u = ACTION[1::2] - ACTION[2::2]\n",
    "\n",
    "# calculate v_{t+1} and x_{t+1}\n",
    "expected_next_v = v_prev * (1 - damping) + ((u * accel) / mass) * dt\n",
    "expected_next_x = x_prev + expected_next_v * dt\n",
    "\n",
    "# do speaker step (ignoring this value in comunication)\n",
    "env.step(np.array([0, 0, 1], dtype=np.float32))\n",
    "\n",
    "# do listener step\n",
    "env.step(ACTION)\n",
    "\n",
    "actual_next_v = listener.state.p_vel\n",
    "actual_next_x = listener.state.p_pos\n",
    "\n",
    "print(f'expected next v: {expected_next_v}')\n",
    "print(f'actual next v:   {actual_next_v}')\n",
    "print()\n",
    "print(f'expected next x: {expected_next_x}')\n",
    "print(f'actual next x:   {actual_next_x}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08764396-5bc7-44e7-a8b4-e4d1266cb540",
   "metadata": {},
   "source": [
    "Only by understanding the environment model can we hope to implement model-based algorithms, e.g., planning (BFS, DFS, A*, etc.). The above example showcases only the listener agent of a single environment using one configuration. To better understand the effect of different configurations (and in different MPE environments), e.g., introducing collisions between the speaker and listener agents, one must dive into the [MPE code](https://github.com/Farama-Foundation/PettingZoo/tree/master/pettingzoo/mpe). <span style=\"color:yellow\">Be warned</span> that this code is not fully documented."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded75bf7-9524-4bf1-be5b-5197aaaa9109",
   "metadata": {},
   "source": [
    "### Wrappers\n",
    "\n",
    "PettingZoo provides utilities called wrappers. They are used to alter the behavior of the environment with minimal effort wihtout changing the general environment API. In fact, the Simple Speaker Listener environment already wrapped upon creation. In the below demonstration, we can see that the environment type is actually a wrapper called `OrderEnforcingWrapper`. This class works like the original environment and adds checks that enforce the agent order, and adds extra functionality, e.g. the `agent_iter` function is implemented in this wrapper (and is not available in the raw environment). The underlying environment, revealed using the environment's `env` attribute, is actually wrapped by another wrapper called `AssertOutOfBoundsWrapper`, which checks that given actions are compatible with the agents' discrete action space (similarly, the continuous action space uses another wrapper called `ClipOutOfBoundsWrapper`). Only under this wrapper do we find the raw environment object. However, we can jump directly to the raw environment by using the `unwrapped` property."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d0496a79-fd0e-43dc-90af-6eb987ecc886",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "external wrapper:      <class 'pettingzoo.utils.wrappers.order_enforcing.OrderEnforcingWrapper'>\n",
      "inner wrapper:         <class 'pettingzoo.utils.wrappers.assert_out_of_bounds.AssertOutOfBoundsWrapper'>\n",
      "raw environment:       <class 'pettingzoo.mpe.simple_speaker_listener_v3.raw_env'>\n",
      "environment unwrapped: <class 'pettingzoo.mpe.simple_speaker_listener_v3.raw_env'>\n"
     ]
    }
   ],
   "source": [
    "env = simple_speaker_listener_v3.env()\n",
    "print(f'external wrapper:      {type(env)}')\n",
    "print(f'inner wrapper:         {type(env.env)}')\n",
    "print(f'raw environment:       {type(env.env.env)}')\n",
    "print(f'environment unwrapped: {type(env.unwrapped)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be00eb9-eb5c-4301-983e-b13c5ad918ab",
   "metadata": {},
   "source": [
    "#### Custom Wrappers\n",
    "\n",
    "Using the `BaseWrapper` abstraction, we can create our own wrappers. We demonstrate this below with a custom wrapper that turns this environment into a single-agent environment. This is done by skipping the speaker's step before the user has a chance to do so. The action will be the goal color if continuous or some constant message for each color if discrete. We the user to  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "193718b3-71e3-47dd-b4ce-e3a74c52405e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pettingzoo.utils import wrappers\n",
    "import numpy as np\n",
    "\n",
    "class ListenerOnlyWrapper(wrappers.BaseWrapper):\n",
    "    \n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        \n",
    "        # reset to skip speaker before new game\n",
    "        self.reset()\n",
    "        \n",
    "        # set single agent list\n",
    "        self.agents = self.agents[1:]\n",
    "        \n",
    "    def reset(self):\n",
    "        super().reset()\n",
    "        \n",
    "        # skip speaker action\n",
    "        self.__step_speaker()\n",
    "    \n",
    "    def step(self, action):\n",
    "        super().step(action)  # do listener action\n",
    "        \n",
    "        # skip speaker action\n",
    "        self.__step_speaker()\n",
    "        \n",
    "    def __step_speaker(self):\n",
    "        _, _, done, _ = self.env.last()\n",
    "        goal_color, _, done, _ = self.env.last()\n",
    "        \n",
    "        # speaker is done before the listener.\n",
    "        if done:\n",
    "            return\n",
    "        \n",
    "        # step with the correct action type\n",
    "        if self.env.unwrapped.continuous_actions:\n",
    "            super().step(goal_color)\n",
    "        else:\n",
    "            super().step(np.argmax(goal_color))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adfc89c6-9834-4b7c-a1ad-5c5f821ab51f",
   "metadata": {},
   "source": [
    "We can add wrap a new environment by creating a new wrapper instance initialized with the wrapped environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "19732f47-9b7b-4666-b056-59395338bfa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "custom wrapped environment: ListenerOnlyWrapper<simple_speaker_listener_v3>\n",
      "list of agents:             ['listener_0']\n"
     ]
    }
   ],
   "source": [
    "env = simple_speaker_listener_v3.env(max_cycles=5, continuous_actions=True)\n",
    "env = ListenerOnlyWrapper(env)\n",
    "print(f'custom wrapped environment: {env}')\n",
    "print(f'list of agents:             {env.agents}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94da8c5f-9b21-426e-943f-80de0396c683",
   "metadata": {},
   "source": [
    "Like before, we can simulate an episode with `agent_iter` which will now always select the listener. Note that the initial observation contains a communication vector of 0 because no communication is received before the first step. We can see the listener's observation in the following observations until the end of the episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cc154024-5e7c-4008-8e9b-ab32e39dab19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "listener_0 reward:      0.0\n",
      "listener_0 observation: [ 0.          0.         -0.34709144  0.967302   -0.01558756  0.34916973\n",
      " -0.01930392  0.6651746   0.          0.          0.        ]\n",
      "listener_0 action:      [0.29963553 0.42740586 0.9696676  0.94981784 0.36658645]\n",
      "\n",
      "listener_0 reward:      -0.10253806598032186\n",
      "listener_0 observation: [-0.27113086  0.2916157  -0.31997836  0.93814045  0.01152553  0.32000816\n",
      "  0.00780917  0.63601303  0.15        0.65        0.15      ]\n",
      "listener_0 action:      [0.25012997 0.70812196 0.6843732  0.01767832 0.87168497]\n",
      "\n",
      "listener_0 reward:      -0.11711090866196903\n",
      "listener_0 observation: [-0.19147377 -0.20829156 -0.30083096  0.95896965  0.0306729   0.34083733\n",
      "  0.02695655  0.6568422   0.15        0.65        0.15      ]\n",
      "listener_0 action:      [0.47302386 0.75251347 0.12384099 0.40512335 0.7007381 ]\n",
      "\n",
      "listener_0 reward:      -0.1380040380749845\n",
      "listener_0 observation: [ 0.17073092 -0.30402604 -0.31790406  0.98937225  0.01359981  0.37123993\n",
      "  0.00988346  0.6872448   0.15        0.65        0.15      ]\n",
      "listener_0 action:      [0.7628567  0.7953184  0.71841943 0.6246751  0.02808642]\n",
      "\n",
      "listener_0 reward:      -0.132660007285681\n",
      "listener_0 observation: [ 0.16649768  0.07027481 -0.33455384  0.98234475 -0.00304996  0.36421245\n",
      " -0.00676631  0.6802173   0.15        0.65        0.15      ]\n",
      "listener_0 action:      [0.28871357 0.803164   0.32337195 0.21853152 0.17726184]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "policy = RandomPolicy(env.action_space(env.agents[0]))\n",
    "for agent in env.agent_iter():\n",
    "    observation, reward, done, info = env.last()\n",
    "    \n",
    "    # stop if done\n",
    "    if done:\n",
    "        break\n",
    "    \n",
    "    # choose and execute action\n",
    "    action = policy(observation)\n",
    "    env.step(action)\n",
    "    \n",
    "    # log everything\n",
    "    print(f'{agent} reward:      {reward}')\n",
    "    print(f'{agent} observation: {observation}')\n",
    "    print(f'{agent} action:      {action}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "547b7dee-de48-4ea8-8591-e5fba0316e9d",
   "metadata": {},
   "source": [
    "### Parallel Environments\n",
    "\n",
    "Up until now we were able to view each agent's observations and act individually, even though the actions were only applied after a full cycle through all the agents. Many PettingZoo environments support acting in parallel using yet another wrapper, including Simple Speaker Listener. This is important for implementing algorithms that consider joint actions, e.g., centralized control. We can create a parallel environment by invoking the `parallel_env` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "16ebf152-3750-42ef-bb15-6585074e72bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = simple_speaker_listener_v3.parallel_env()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4214b307-5b96-49a3-8051-399da21d987b",
   "metadata": {},
   "source": [
    "In this environment, both the observations of the speaker and listener agents are bundled together in a dictionary. The initial environment observations is returned from the `reset` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "063cdeba-ad2c-41d4-99d9-c187bde4cb96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'speaker_0': array([0.15, 0.65, 0.15], dtype=float32),\n",
       " 'listener_0': array([ 0.        ,  0.        ,  0.35934502,  0.12687765,  0.8555883 ,\n",
       "         0.44403875,  0.3446538 , -0.3340047 ,  0.        ,  0.        ,\n",
       "         0.        ], dtype=float32)}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "observations = env.reset()\n",
    "observations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d8e41f-9bb7-417a-99fe-3aec12d20a68",
   "metadata": {},
   "source": [
    "Actions are performed jointly using the `step` function. This will return the next observation bundle together with separate dictionaries for the reward and \"done\" status for each agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7b6d600f-c229-48fa-b970-861bd03d0d61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new observations:\n",
      "{'speaker_0': array([0.15, 0.65, 0.15], dtype=float32), 'listener_0': array([ 0.5       ,  0.        ,  0.30934504,  0.12687765,  0.8055883 ,\n",
      "        0.44403875,  0.29465377, -0.3340047 ,  0.        ,  1.        ,\n",
      "        0.        ], dtype=float32)}\n",
      "step rewards:\n",
      "defaultdict(<class 'int'>, {'speaker_0': -0.8461429721052728, 'listener_0': -0.8461429721052728})\n",
      "done status:\n",
      "{'speaker_0': False, 'listener_0': False}\n"
     ]
    }
   ],
   "source": [
    "joint_action = {'speaker_0': 1, 'listener_0': 2}\n",
    "observations, rewards, done, info = env.step(joint_action)\n",
    "\n",
    "print('new observations:')\n",
    "print(observations)\n",
    "print('step rewards:')\n",
    "print(rewards)\n",
    "print('done status:')\n",
    "print(done)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b8f35c-e50f-4a8b-bb75-d18bb384d3bf",
   "metadata": {},
   "source": [
    "Putting it all together, a simulation might look something like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1b9d18ab-dc00-47b2-b053-cf06367f471d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rewards:      defaultdict(<class 'int'>, {'speaker_0': -2.360987262366699, 'listener_0': -2.360987262366699})\n",
      "observations: {'speaker_0': array([0.65, 0.15, 0.15], dtype=float32), 'listener_0': array([ 0.        , -0.5       , -0.09914988, -1.5333482 ,  0.63261783,\n",
      "       -1.0574328 , -1.0721401 , -1.3541496 ,  0.        ,  0.        ,\n",
      "        1.        ], dtype=float32)}\n",
      "actions:      {'speaker_0': 2, 'listener_0': 3}\n",
      "\n",
      "rewards:      defaultdict(<class 'int'>, {'speaker_0': -2.247392400772925, 'listener_0': -2.247392400772925})\n",
      "observations: {'speaker_0': array([0.65, 0.15, 0.15], dtype=float32), 'listener_0': array([ 0.        , -0.375     , -0.09914988, -1.4958482 ,  0.63261783,\n",
      "       -1.0199329 , -1.0721401 , -1.3166496 ,  1.        ,  0.        ,\n",
      "        0.        ], dtype=float32)}\n",
      "actions:      {'speaker_0': 0, 'listener_0': 0}\n",
      "\n",
      "rewards:      defaultdict(<class 'int'>, {'speaker_0': -2.1640419577025956, 'listener_0': -2.1640419577025956})\n",
      "observations: {'speaker_0': array([0.65, 0.15, 0.15], dtype=float32), 'listener_0': array([ 0.        , -0.28125   , -0.09914988, -1.4677231 ,  0.63261783,\n",
      "       -0.9918078 , -1.0721401 , -1.2885246 ,  0.        ,  1.        ,\n",
      "        0.        ], dtype=float32)}\n",
      "actions:      {'speaker_0': 1, 'listener_0': 0}\n",
      "\n",
      "rewards:      defaultdict(<class 'int'>, {'speaker_0': -2.1025673334076598, 'listener_0': -2.1025673334076598})\n",
      "observations: {'speaker_0': array([0.65, 0.15, 0.15], dtype=float32), 'listener_0': array([ 0.        , -0.2109375 , -0.09914988, -1.4466294 ,  0.63261783,\n",
      "       -0.9707141 , -1.0721401 , -1.2674309 ,  0.        ,  0.        ,\n",
      "        1.        ], dtype=float32)}\n",
      "actions:      {'speaker_0': 2, 'listener_0': 0}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "env = simple_speaker_listener_v3.parallel_env(max_cycles=5)\n",
    "observations = env.reset()\n",
    "\n",
    "# a random policy for each agent\n",
    "policies = {\n",
    "    env.agents[0]: RandomPolicy(env.action_space(env.agents[0])),\n",
    "    env.agents[1]: RandomPolicy(env.action_space(env.agents[1]))\n",
    "}\n",
    "\n",
    "for _ in range(env.unwrapped.max_cycles):\n",
    "    joint_action = {agent: policies[agent](obs) for agent, obs in observations.items()}\n",
    "    observations, rewards, done, info = env.step(joint_action)\n",
    "    \n",
    "    if any(done.values()):\n",
    "        break\n",
    "    \n",
    "    # log everything\n",
    "    print(f'rewards:      {rewards}')\n",
    "    print(f'observations: {observations}')\n",
    "    print(f'actions:      {joint_action}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb81bbb-a237-49ad-ac77-462524359b24",
   "metadata": {},
   "source": [
    "### Other Environments\n",
    "\n",
    "There already exists a single-agent and single-landmark version of Simple Speaker Listener, called Simple. Here, the one agent acts like the listener and has a simplified observation space containing the agent's velocity and the landmark's relative location. There are other, more complex environments in the MPE library and other environment libraries in PettingZoo. You can explore them in the [PettingZoo Website](https://www.pettingzoo.ml/envs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4d1577e7-21b8-47a2-9536-5b6455bd9c91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "discrete actions:\n",
      "- agent 1: speaker_0\n",
      "\t- observation space: Box(-inf, inf, (3,), float32)\n",
      "\t- action space: Discrete(3)\n",
      "- agent 2: listener_0\n",
      "\t- observation space: Box(-inf, inf, (11,), float32)\n",
      "\t- action space: Discrete(5)\n",
      "\n",
      "continuous actions:\n",
      "- agent 1: speaker_0\n",
      "\t- observation space: Box(-inf, inf, (3,), float32)\n",
      "\t- action space: Box(0.0, 1.0, (3,), float32)\n",
      "- agent 2: listener_0\n",
      "\t- observation space: Box(-inf, inf, (11,), float32)\n",
      "\t- action space: Box(0.0, 1.0, (5,), float32)\n"
     ]
    }
   ],
   "source": [
    "from pettingzoo.mpe import simple_v2\n",
    "\n",
    "\n",
    "print_env_info(continuous_actions=False)\n",
    "print()\n",
    "print_env_info(continuous_actions=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3806e1b3-f662-4bc3-9efd-2f54d6aa0499",
   "metadata": {},
   "source": [
    "### Single-Agent Reduction to Gym\n",
    "\n",
    "The MPE Simple environment shown above is a single-agent environment. It would be beneficial to align its API with that of gym, thus allowing us to use other algorithms implemented for gym. This is similar to the parallel environment API, except that these accept dictionaries of actions and return dictionaries of observations, rewards, etc.\n",
    "\n",
    "Below is a wrapper for this environment that does exactly that! It overrides the `reset` method to the single observation, and the `step` method to accept a single action and return a single tuple of step results. Note that for parallel environments must be wrapped with the dedicated wrapper of type `BaseParallelWrapper`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "117bd6e2-c5e6-4cc0-8ff5-1b969fec6926",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use a different wrapper base class for parallel environments\n",
    "from pettingzoo.utils.wrappers import BaseParallelWraper\n",
    "\n",
    "class SingleAgentParallelEnvGymWrapper(BaseParallelWraper):\n",
    "    \"\"\"\n",
    "    A wrapper for single-agent parallel environments aligning the environments'\n",
    "    API with OpenAI Gym.\n",
    "    \"\"\"\n",
    "\n",
    "    def reset(self):\n",
    "        # run `reset` as usual.\n",
    "        # returned value is a dictionary of observations with a single entry\n",
    "        obs = self.env.reset()\n",
    "\n",
    "        # return the single entry value as is.\n",
    "        # no need for the key (only one agent)\n",
    "        return next(iter(obs.values()))\n",
    "\n",
    "    def step(self, action):\n",
    "        # step using \"joint action\" of a single agnet as a dictionary\n",
    "        step_rets = self.env.step({self.env.agents[0]: action})\n",
    "\n",
    "        # unpack step return values from their dictionaries\n",
    "        return tuple(next(iter(ret.values())) for ret in step_rets)\n",
    "\n",
    "    @property  # make property for gym-like access\n",
    "    def action_space(self, _=None):  # ignore second argument in API\n",
    "        # get action space of the single agent\n",
    "        return self.env.action_space(self.env.possible_agents[0])\n",
    "\n",
    "    @property  # make property for gym-like access\n",
    "    def observation_space(self, _=None):  # ignore second argument in API\n",
    "        # get observation space of the single agent\n",
    "        return self.env.observation_space(self.env.possible_agents[0])\n",
    "    \n",
    "simple_gym_env = simple_v2.parallel_env(max_cycles=5, continuous_actions=True)\n",
    "simple_gym_env = SingleAgentParallelEnvGymWrapper(simple_gym_env)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d8e605-9c46-4ba1-a36f-4bbf6b357c97",
   "metadata": {},
   "source": [
    "And we can now use the environment as we would any other gym environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "953b28a7-64c1-497e-a4af-b8b16ec1e0d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0\n",
      "reward:      -1.062627617748555\n",
      "observation: [0.3680436  0.32354856 0.9243896  0.45621428]\n",
      "action:      [0.32831636 0.81992656 0.08383939 0.6577298  0.01063272]\n",
      "\n",
      "step 1\n",
      "reward:      -0.9718449655002888\n",
      "observation: [0.39994454 0.20679925 0.8843952  0.43553433]\n",
      "action:      [0.27477986 0.86535984 0.6175362  0.25367478 0.3253991 ]\n",
      "\n",
      "step 2\n",
      "reward:      -0.994246645519305\n",
      "observation: [-0.08008812 -0.09282242  0.89240396  0.4448166 ]\n",
      "action:      [0.8639809  0.2307866  0.99087965 0.21249507 0.7083388 ]\n",
      "\n",
      "step 3\n",
      "reward:      -1.0399584335001262\n",
      "observation: [-0.3644977   0.23878156  0.92885375  0.42093843]\n",
      "action:      [0.12598233 0.00312544 0.6119886  0.7112211  0.09442434]\n",
      "\n",
      "step 4\n",
      "reward:      -1.1326313435091178\n",
      "observation: [-0.45510942 -0.071326    0.9743647   0.42807102]\n",
      "action:      [0.69730085 0.5179444  0.88141674 0.14974637 0.6505707 ]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# CHOOSE MAX CYCLES AND DISCRETE OR CONTINUOUS ACTION SPACE\n",
    "observation = simple_gym_env.reset()\n",
    "\n",
    "# create a random policy for both agents' action spaces.\n",
    "policy = RandomPolicy(simple_gym_env.action_space)\n",
    "\n",
    "# iterate over agents until the episode is complete\n",
    "for i in range(simple_gym_env.unwrapped.max_cycles):\n",
    "    # choose an action and execute\n",
    "    action = policy(observation)\n",
    "    observation, reward, done, info = simple_gym_env.step(action)\n",
    "    \n",
    "    # log everything\n",
    "    print(f'step {i}')\n",
    "    print(f'reward:      {reward}')\n",
    "    print(f'observation: {observation}')\n",
    "    print(f'action:      {action}')\n",
    "    print()\n",
    "    \n",
    "    # if done, the episode is complete. no more actions can be taken\n",
    "    if done:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "75246c9d-f05f-4a1a-84c2-6e66a8b69598",
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_gym_env.render()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
