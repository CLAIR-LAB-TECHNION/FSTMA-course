{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0142571-da54-45e2-9995-d843da5b12fd",
   "metadata": {},
   "source": [
    "<img src='https://cdn.analyticsvidhya.com/wp-content/uploads/2021/06/PyTorch-1536x864.png'/>\n",
    "\n",
    "# Introduction\n",
    "\n",
    "[Pytorch](https://pytorch.org/) is a popular, open source deep learning framework for python developed by [Facebook AI Research](https://ai.facebook.com/) (FAIR). This framework serves three main purposes:\n",
    "1. Provide an efficient implementation for high dimensional tensor operations that are GPU compatible.\n",
    "2. Integrates seemless automatic differentiation into tensor operations for easy parameter optimization.\n",
    "3. Contains implementations for a variety of common deep learning module.\n",
    "\n",
    "Pytorch differs itself from other deep learning frameworks such as [TensorFlow](https://www.tensorflow.org) in that it builds its computation graphs dynamically at runtime while others may need to build a static computation graph before the actual computation. This makes the code flow more smoothely and debugging much friendlier, making this framework perfect for studying and researching deep learning algorithms. This, however, comes at the cost of the performance optimization required for deployment.\n",
    "\n",
    "Below, we highlihght some of the main components of pytorch. This presentation is loosely based on the pytorch [60 minute tutorial](https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html). There is a more detailed \n",
    "[youtube series](https://pytorch.org/tutorials/beginner/introyt.html) on pytorch which is guaranteed to make you a pytorch master!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24c299fa-0a95-48d0-860b-a2775faeaaa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first and foremost... import the pytorch library!\n",
    "# running this line for the first time may take a while\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc5eb49-de8b-4d80-96c9-5e60e3efb549",
   "metadata": {},
   "source": [
    "# Tensors\n",
    "<img src=\"https://pytorch.org/tutorials/_images/tensor_illustration_flat.png\" />\n",
    "\n",
    "A tensor is a multi-dimensional array. Some common mathematical structures are tensors in disguise, e.g., a vector is a one-dimensional tensor and a matrix is a two-dimensional tensor. A matrix of shape $n\\times m$ whose values are vecotrs of length $3$ (e.g. an $n\\times m$ RGB image), is a three-dimensional tensor of shape $n\\times m\\times 3$. If we stack $k$ matrices of shape $n\\times m$ we get a three-dimensional tensor of shape $k\\times n\\times m$. These kinds of intuitions are embedded into the core of the pytorch `Tensor` object.\n",
    "\n",
    "\n",
    "## Numpy\n",
    "\n",
    "[Numpy](https://numpy.org) is a python package containing fundemental tools for scientific computation, implemented in c/c++ for maximum efficiency. Its intuitive API, generality, scalability, detailed documentation, and top-notch performance are one of the reasons python is vastly popular for scientific programming. Asside from a multitude of useful tools and algorithms, numpy implements the the `ndarray` class for multi-dimensional arrays. We cannot discuss pytorch without mentioning numpy since the `Tensor` object is simply a wrapper for a numpy array that implements additional features.\n",
    "\n",
    "Below are some examples of basic numpy operations in pytorch. For more operations, a separate notebook is provided on numpy. Although not all funcionality translates directly to pytorch (e.g. torch.cat is the pytorch equivalent of np.concatenate), the differences are well documented on the web."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6926d86c-bbbb-42af-970a-3d316be0b472",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t1 = \n",
      "tensor([[0, 1],\n",
      "        [1, 0]])\n",
      "t1 = \n",
      "tensor([[-1,  0],\n",
      "        [ 0, -1]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# tensor from list data\n",
    "t1 = torch.tensor([[0, 1], [1, 0]])\n",
    "t2 = torch.tensor([[-1, 0], [0, -1]])\n",
    "print(f't1 = \\n{t1}')\n",
    "print(f't1 = \\n{t2}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "61761a02-d1bf-4a46-aad2-dc15a751036f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the value of t1 at index (1, 0) is 1\n",
      "\n",
      "row 0 of t1 is tensor([0, 1])\n",
      "\n",
      "column 1 of t1 is tensor([1, 0])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# indexing\n",
    "t1_v10 = t1[1, 0]\n",
    "print(f'the value of t1 at index (1, 0) is {t1_v10}\\n')\n",
    "\n",
    "t1_row0 = t1[0]\n",
    "print(f'row 0 of t1 is {t1_row0}\\n')\n",
    "\n",
    "t1_col1 = t1[:, 1]\n",
    "print(f'column 1 of t1 is {t1_col1}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b8698634-7847-44cf-a566-95d85a561606",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reshaped 3x5x5 matrix \n",
      "tensor([[[ 0,  1,  2,  3,  4],\n",
      "         [ 5,  6,  7,  8,  9],\n",
      "         [10, 11, 12, 13, 14],\n",
      "         [15, 16, 17, 18, 19],\n",
      "         [20, 21, 22, 23, 24]],\n",
      "\n",
      "        [[25, 26, 27, 28, 29],\n",
      "         [30, 31, 32, 33, 34],\n",
      "         [35, 36, 37, 38, 39],\n",
      "         [40, 41, 42, 43, 44],\n",
      "         [45, 46, 47, 48, 49]],\n",
      "\n",
      "        [[50, 51, 52, 53, 54],\n",
      "         [55, 56, 57, 58, 59],\n",
      "         [60, 61, 62, 63, 64],\n",
      "         [65, 66, 67, 68, 69],\n",
      "         [70, 71, 72, 73, 74]]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# reshaping\n",
    "big_matrix = torch.arange(75).reshape(3, 5, 5)\n",
    "print(f'reshaped 3x5x5 matrix \\n{big_matrix}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc124f99-633b-432f-8cad-fd79f1354931",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sliced big_matrix[1:3, :, :-1:2] = \n",
      "tensor([[[25, 27],\n",
      "         [30, 32],\n",
      "         [35, 37],\n",
      "         [40, 42],\n",
      "         [45, 47]],\n",
      "\n",
      "        [[50, 52],\n",
      "         [55, 57],\n",
      "         [60, 62],\n",
      "         [65, 67],\n",
      "         [70, 72]]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# slicing\n",
    "# NOTE: negative step size is not supported in pytorch. The following will raise an error:\n",
    "#           `bir_matrix[::-1]`\n",
    "sliced_matrix = big_matrix[1:3, :, :-1:2]\n",
    "print(f'sliced big_matrix[1:3, :, :-1:2] = \\n{sliced_matrix}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "517f3d91-3117-4727-8330-d6d1e3c57a60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t1 + t2 = \n",
      "tensor([[-1,  1],\n",
      "        [ 1, -1]])\n",
      "\n",
      "t1 * t2 = \n",
      "tensor([[0, 0],\n",
      "        [0, 0]])\n",
      "\n",
      "t1 & t2 = \n",
      "tensor([[0, 0],\n",
      "        [0, 0]])\n",
      "\n",
      "t1 @ t2 = \n",
      "tensor([[ 0, -1],\n",
      "        [-1,  0]])\n",
      "\n",
      "torch.dot(t1[0],t2[1]) = t1[0] @ t2[1] = -1\n"
     ]
    }
   ],
   "source": [
    "# math operations\n",
    "\n",
    "# pointwise operations\n",
    "print(f't1 + t2 = \\n{t1 + t2}\\n')  # addition\n",
    "print(f't1 * t2 = \\n{t1 * t2}\\n')  # multiplication\n",
    "print(f't1 & t2 = \\n{t1 & t2}\\n')  # bitwise AND\n",
    "\n",
    "# matrix operations\n",
    "print(f't1 @ t2 = \\n{t1 @ t2}\\n')  # matrix multiplication\n",
    "print(f'torch.dot(t1[0],t2[1]) = t1[0] @ t2[1] = {t1[0] @ t2[1]}')  # vector dot product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "75b0a53a-bd27-4af0-a7a4-202e8e1c4de2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "if we ask `t1 + t2 == 1`, we get a boolean mask:\n",
      "tensor([[False,  True],\n",
      "        [ True, False]])\n",
      "is there a 1 in `t1 + t2`? True\n",
      "are all the values in `t1 + t2` equal to 1? False\n"
     ]
    }
   ],
   "source": [
    "# boolean querries\n",
    "print(f'if we ask `t1 + t2 == 1`, we get a boolean mask:\\n{(t1 + t2) == 1}')\n",
    "print(f'is there a 1 in `t1 + t2`? {torch.any((t1 + t2) == 1)}')\n",
    "print(f'are all the values in `t1 + t2` equal to 1? {torch.all((t1 + t2) == 1)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2922634d-cb5a-4b80-9e36-3d3cb559f431",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new t1 after mask assignment is\n",
      "tensor([[ 0, 10],\n",
      "        [10,  0]])\n"
     ]
    }
   ],
   "source": [
    "# masking\n",
    "# NOTE: this changes the object t1\n",
    "t1[t2 == 0] = 10\n",
    "print(f'new t1 after mask assignment is\\n{t1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6f003c93-bc1c-4b56-8f9c-1127108a1f85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "concatenate t1 and t2 over dimension 0:\n",
      "tensor([[ 0, 10],\n",
      "        [10,  0],\n",
      "        [-1,  0],\n",
      "        [ 0, -1]])\n",
      "\n",
      "concatenate t1 and t2 over dimension 1:\n",
      "tensor([[ 0, 10, -1,  0],\n",
      "        [10,  0,  0, -1]])\n"
     ]
    }
   ],
   "source": [
    "# concatenate over axis (corresponds to np.concatenate)\n",
    "print(f'concatenate t1 and t2 over dimension 0:\\n{torch.cat([t1, t2], dim=0)}\\n')\n",
    "print(f'concatenate t1 and t2 over dimension 1:\\n{torch.cat([t1, t2], dim=1)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c853623-3256-40e9-bdc9-54a0da201da2",
   "metadata": {},
   "source": [
    "## Creating tensors\n",
    "We can create tensors from python list data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "69378f76-bc05-4a2d-9f9f-881b0d68420d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2.],\n",
       "        [3., 4.]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create tensor from list data\n",
    "data = [[1, 2], [3, 4]]\n",
    "x_data = torch.tensor(data, dtype=torch.float)  # overrides default dtype\n",
    "x_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ac8799-d3ca-40e6-b40f-e9eccf7e9c5e",
   "metadata": {},
   "source": [
    "and fro numpy data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6b39f517-8508-478d-a5af-19dd52beef1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2],\n",
       "        [3, 4]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# create tensor from numpy\n",
    "np_data = np.array(data)\n",
    "x_np = torch.from_numpy(np_data)\n",
    "x_np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78b78f9f-c50a-4103-8d93-08893430d8de",
   "metadata": {},
   "source": [
    "Default value initialization of a given shape and type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cc7246af-06e1-490e-b333-01e2c1bdaadf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Tensor: \n",
      " tensor([[0.5967, 0.7970, 0.5328],\n",
      "        [0.1592, 0.5540, 0.3452]]) \n",
      "\n",
      "Ones Tensor: \n",
      " tensor([[1, 1, 1, 1]], dtype=torch.int32) \n",
      "\n",
      "Zeros Tensor: \n",
      " tensor([[[0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[0., 0.],\n",
      "         [0., 0.]]])\n"
     ]
    }
   ],
   "source": [
    "shape = (2, 3,)\n",
    "rand_tensor = torch.rand((2, 3))\n",
    "ones_tensor = torch.ones((1, 4), dtype=torch.int)  # overide default type\n",
    "zeros_tensor = torch.zeros(3, 2, 2)\n",
    "\n",
    "print(f\"Random Tensor: \\n {rand_tensor} \\n\")\n",
    "print(f\"Ones Tensor: \\n {ones_tensor} \\n\")\n",
    "print(f\"Zeros Tensor: \\n {zeros_tensor}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f314e9c6-e8a9-432c-8cbf-8e1286a1422a",
   "metadata": {},
   "source": [
    "or use the properties of another tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a290ed3b-ac9e-4b42-9cb2-a06bc61c77fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tens Tensor: \n",
      " tensor([[10., 10.],\n",
      "        [10., 10.]]) \n",
      "\n",
      "Random Tensor: \n",
      " tensor([[0.9186, 0.7270],\n",
      "        [0.3564, 0.4363]]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "x_ones = torch.full_like(x_data, 10) # retains the properties of x_data\n",
    "print(f\"Tens Tensor: \\n {x_ones} \\n\")\n",
    "\n",
    "x_rand = torch.rand_like(x_data, dtype=torch.float) # overrides the datatype of x_data\n",
    "print(f\"Random Tensor: \\n {x_rand} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e7791f8-cd97-4f9c-bbea-22bdd8ce44c2",
   "metadata": {},
   "source": [
    "When a tensor is created, it is bound to a numpy array containing all its values. we can reference this array using the `numpy` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0661c6f2-9688-410d-b3d9-37daa968f6d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 2.],\n",
       "       [3., 4.]], dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_data_arr = x_data.numpy()\n",
    "x_data_arr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2daeb4af-c8c3-446c-a343-0b764fef8c77",
   "metadata": {},
   "source": [
    "Changing this array will change the values in the tensor as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d820ff66-3de0-4359-b1fb-6f80977a642d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-100.,    2.],\n",
       "        [   3.,    4.]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_data_arr[0, 0] = -100\n",
    "x_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b54c2f3-51f4-4dfb-9a8a-05062ca9009f",
   "metadata": {},
   "source": [
    "When creating a tensor from a numpy array, the tensor is bound to that array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "009616d3-9947-46a2-a2c3-feceda627f0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-200,    2],\n",
       "        [   3,    4]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np_data[0,0] = -200\n",
    "x_np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a13354-7c5e-49a5-a813-6d28ad7736d3",
   "metadata": {},
   "source": [
    "## Changing device\n",
    "\n",
    "Tensor operations are typically executed faster using [CUDA](https://developer.nvidia.com/cuda-zone), a parallel computing platform and programming model developed by NVIDIA for general computing on graphical processing units (GPUs). By default, tensors are stored in the CPU memory, but can easily be transfered to the GPU. Performing operations between two tensors that are stored in the GPU memory will run via CUDA. To run CUDA locally, one must connect a compatible NVIDIA GPU and install the [CUDA toolkit](https://developer.nvidia.com/cuda-toolkit). However, anyone get (somewhat limmited) access to a GPU through [Google Colab](https://research.google.com/colaboratory/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b16109cb-6a76-47e0-9e3b-2656409033e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chosen device: cpu\n"
     ]
    }
   ],
   "source": [
    "# choose device based on availability\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'Chosen device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e13f18-f99d-4f9e-8b03-d4428086c41c",
   "metadata": {},
   "source": [
    "Tensors can be initialized directly in your specified device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e2e8cce4-4ddd-4863-b667-f794a0dfbbff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.rand_like(x_data, device=device)\n",
    "t.device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b45c14f7-a2fd-4cc6-ab2a-776691a3c18e",
   "metadata": {},
   "source": [
    "and can also be moved back and forth between devices using the `to` mehtod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4aec6a95-a732-4d5a-84a3-ebdf9e047f0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_cpu = t.to('cpu')\n",
    "t_cpu.device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b47ab84-7682-469e-b914-545f084d08f1",
   "metadata": {},
   "source": [
    "GPU memory is expensive and therefore scarse. Pytorch gives the user full control over which tensors are stored in which device. While this is great for memory management, it requires some maintenance. More on that when we [train a classifier](#section:classifier).\n",
    "\n",
    "**Note**: tensor operations between tensors on different devices is not supported. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ba29097-513e-4007-a7d0-ea1c3a6d089e",
   "metadata": {},
   "source": [
    "# Automatic differentiation\n",
    "\n",
    "Modern day neural networks are heavily reliant on the [gradient descent](https://en.wikipedia.org/wiki/Gradient_descent) algorithm for parameter optimization. To support this, pytorch offers \"automatic differentiation\". This mechanism keeps track of gradients of functions with respect to specified tensors. Below is an example of linear regression with gradient descent.\n",
    "\n",
    "We start by choosing creating two random tensors: `x` for a single input, `y` for a single output, and `W` for the weights. Because we are going to optimize `W`, we would like to keep track of its gradients. To have pytorch do this automatically, we must set the `requires_grad` parameter to `True` (defaults to `False`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "97afd4fe-d572-4f9c-a9e5-697f56868efb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1., 0., 0.]),\n",
       " tensor(0.5421),\n",
       " tensor([0.7292, 0.7482, 0.2716], requires_grad=True))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.eye(3)[0]  # simple input: [1, 0, 0]\n",
    "y = torch.rand([])  # empty shpae gives a scalar tensor (no shape)\n",
    "W = torch.rand(3, requires_grad=True)  # requires_grad defaults to \n",
    "x, y, W"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "006ba1de-4e24-4697-8dd2-9fb2f23d9fb5",
   "metadata": {},
   "source": [
    "We can use our tensors to to compute the squared loss of predicting `y` as a linear function of `x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "801ca328-3f81-4460-86a8-902c36c9bdba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.7292, grad_fn=<DotBackward0>),\n",
       " tensor(0.0350, grad_fn=<PowBackward0>))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Wx = W @ x\n",
    "squared_loss = (Wx - y) ** 2\n",
    "Wx, squared_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd27dfae-bc9f-4234-912a-6c21bb9ec35f",
   "metadata": {},
   "source": [
    "We note that the outputs of these operations are `Tensor` objects, not `float` (as with numpy). This is because pytorch needs to maintain the gradient that it must use to obtain the gradient of that operation. For example, the `Wx` tensor will use the dot product gradient function. The squared loss uses uses the power gradient function since the last opperation performed was the \"squared\" operation. Notice that the \"subtraction\" operation is unaccounted for, but trust that it is not forgotten and is maintained behind the scenes.\n",
    "\n",
    "We can now call the `backward` method to invoke the chain rule and propagate the gradient to the  tensors that require a gradient. After the `backward` operation has been completed, the `grad` property of these tensors will be the gradient tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9c522b17-a1fe-48c1-bc03-f66838905459",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.3742, 0.0000, 0.0000])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "squared_loss.backward()\n",
    "W.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d95cbfd-2463-4b07-83b7-8d3fb6982c8c",
   "metadata": {},
   "source": [
    "The `grad` property will accumulate the gradients over all operations until it is reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c74329f2-d1d4-4019-8c44-5b0ef7773add",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cur grad: tensor([0.3742, 0.0000, 0.0000])\n",
      "cur grad: tensor([0.7485, 0.0000, 0.0000])\n",
      "cur grad: tensor([1.1227, 0.0000, 0.0000])\n",
      "cur grad: tensor([1.4969, 0.0000, 0.0000])\n",
      "cur grad: tensor([1.8712, 0.0000, 0.0000])\n"
     ]
    }
   ],
   "source": [
    "# reset gradient\n",
    "W.grad = None\n",
    "\n",
    "# call to `backward` reset the computation graph, so we must reset rerun the operations\n",
    "squared_loss = (W @ x - y) ** 2\n",
    "\n",
    "# compute gradient 5 times\n",
    "for i in range(5):\n",
    "    squared_loss.backward(retain_graph=True)  # keeps computation graph, no need to rerun ops\n",
    "    print(f'cur grad: {W.grad}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7178c1a-a180-4d0a-8e53-c360d3f4ca5f",
   "metadata": {},
   "source": [
    "Lastly, some operations should not be a part of the computation graph. To exclude them, we sue the `no_grad` context manager. This will exclude the operations from the computation graph and avoid expensinve gradient computations. An example of operations we may want to exclue is the weight update step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1f7f250a-180a-4257-8f78-ea79b793a1c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.7292)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    Wx = W @ x\n",
    "\n",
    "Wx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f43ba656-475a-463f-9ac2-6227fefa539f",
   "metadata": {},
   "source": [
    "We now have everything we need to implement linear regression on the single example `x` using gradient descent with automatic differentiation. You can play around with the parameters to fine-tune the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c9a31426-25e2-4f0f-bfb2-cd7f88606235",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 10 loss: 0.01679203286767006\n",
      "epoch 20 loss: 0.007422118913382292\n",
      "epoch 30 loss: 0.0032806028611958027\n",
      "epoch 40 loss: 0.0014500346733257174\n",
      "epoch 50 loss: 0.0006409179768525064\n",
      "epoch 60 loss: 0.00028328390908427536\n",
      "epoch 70 loss: 0.00012521336611825973\n",
      "epoch 80 loss: 5.53442987438757e-05\n",
      "epoch 90 loss: 2.4462260626023635e-05\n",
      "epoch 100 loss: 1.0812713298946619e-05\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 100  # number of iterations\n",
    "LEARNING_RATE = 2e-2  # weight update scaling\n",
    "PRINT_EVERY = 10\n",
    "\n",
    "losses = []\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    squared_loss = (W @ x - y) ** 2\n",
    "    \n",
    "    # reset W's gradient\n",
    "    W.grad = None\n",
    "    \n",
    "    #backward operation\n",
    "    squared_loss.backward()\n",
    "    \n",
    "    # update weights\n",
    "    with torch.no_grad():\n",
    "        W -= W.grad * LEARNING_RATE\n",
    "    \n",
    "    \n",
    "    # save epoch loss for later visualisation\n",
    "    # use `item()` to get the float value (cannot use tensor for plotting)\n",
    "    losses.append(squared_loss.item())\n",
    "    \n",
    "    # report on fitting progress\n",
    "    if epoch % PRINT_EVERY == 0:\n",
    "        print(f'epoch {epoch} loss: {losses[-1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca46d3f1-2c19-4839-91b5-086a79fbd616",
   "metadata": {},
   "source": [
    "During training, we collected the loss value at every epoch. We can plot these losses to show that our gradient descent algorithm is indeed fitting and that the automatic differentiation mechanism works. Notice that we must use the `item` method to unwrap the `Tensor` object and retrieve the actual `float` value since tensors cannot be plotted. We use the `matplotlib` library below, but we will later use the `tensorboard` to visualize our results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "252662cf-caa8-403f-a900-7cc0e545389d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.title('Linear Regression Training Loss on a Single Example')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Squared Loss')\n",
    "plt.plot(range(1, EPOCHS + 1), losses);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed10d4e-d5f8-4d6c-8187-85a2ecc713fc",
   "metadata": {},
   "source": [
    "# Neural networks\n",
    "\n",
    "Pytorch provides many tools and building blocks used in state-of-the-art neural networks. This is implemented in the `nn` package, which includes activations, losses, and commonly used layer modules such as fully connected layers and convolutional layers. If you are unfamiliar with these terms, consider reading [this blog post](https://adgefficiency.com/guide-deep-learning/). Furthermore, you can find everything this package has to offer in the [documentation](https://pytorch.org/docs/stable/nn.html).\n",
    "\n",
    "## Layers\n",
    "\n",
    "Pytorch implements many different layers, from basic linear and convolutional layers, to more complex recurrent and transformer layers, and even parameter-free layers such as batch-norm and dropout. Below are a few examples of these layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cd0c6195-7265-4647-b2bc-3fa86a19c810",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A fully connected (a.k.a. Linear, a.k.a Dense) layer\n",
    "# takes input of shape (num_samples, 100) and returns an output of (num_samples, 5)\n",
    "# bias is `True` by default.\n",
    "fully_connected_layer = torch.nn.Linear(in_features=100, out_features=5)\n",
    "\n",
    "# 1D convolutional layer\n",
    "# takes input of shape (num_samples, 2, N) where N is the input vector length.\n",
    "# the sliding kernel is of length 5.\n",
    "# output is of shape (num_sampels, 16, N - 5 + 1)\n",
    "# bias is `True` by default.\n",
    "convolution1d_layer = torch.nn.Conv1d(in_channels=2, out_channels=16, kernel_size=5)\n",
    "\n",
    "# 1D convolutional layer\n",
    "# takes input of shape (num_samples, 3, N, M) where NxM is the shape of the input matrix.\n",
    "# the sliding kernel is of shape 3x3.\n",
    "# output is of shape (num_sampels, 32, N - 3 + 1, M - 3 + 1)\n",
    "# bias parameter omited.\n",
    "convolution2d_layer = torch.nn.Conv2d(in_channels=3, out_channels=32, kernel_size=(3,3),\n",
    "                                      bias=False)\n",
    "\n",
    "# A pooling layer accoring to the maximal value in the neighborhood.\n",
    "# pooling with a 2x2 sliding window with a non-overlapping stride.\n",
    "# takes input of shape (num_samples, num_channels, N, M)\n",
    "# output is of shape (num_samples, num_channels, N//2, M//2)\n",
    "# no parameters\n",
    "maxpool_layer = torch.nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "# A recurrent neural network layer\n",
    "# the input is of shape (L, num_samples, 1000) where L is the sequence length\n",
    "# output is a batch of hidden states of shape (L, num_samples, 100)\n",
    "# hidden states are of size (2, 100)\n",
    "recurrent_layer = torch.nn.RNN(input_size=1000, hidden_size=100, num_layers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5dd8a32-43c1-4954-be4c-a5070d482619",
   "metadata": {},
   "source": [
    "## Activations\n",
    "\n",
    "Activation functions, sometimes called non-linearities, are used to break the linearity between basic layers. Below are some popular activation functions in `nn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6d1691bb-5666-4c9d-b1c2-49d0a07c8b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rectified Linear Unit\n",
    "# f(x) = max(x, 0)\n",
    "relu_activation = torch.nn.ReLU()\n",
    "\n",
    "# Sigmoid\n",
    "# f(x) = 1 / (1 + e^(-x))\n",
    "sigmoid_activation = torch.nn.Sigmoid()\n",
    "\n",
    "# Hyperbolic tangent\n",
    "# f(x) = (e^x - e^(-x)) / (e^x + e^(-x))\n",
    "tanh_activation = torch.nn.Tanh()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c7eea9-e35f-42d7-b473-82fde015ee76",
   "metadata": {},
   "source": [
    "## Losses\n",
    "\n",
    "Loss functions are also implemented in `nn`, letting users optimize many popular objectives. Below are some comonly used loss functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b011db77-e522-482c-a1a5-f672c888bd17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean squared error loss for regression\n",
    "# mse(y, y') = (y - y')^2\n",
    "# where y is the ground truth and y' is the prediction\n",
    "mse_loss = torch.nn.MSELoss()\n",
    "\n",
    "# negative log likelihood loss\n",
    "# NLL(y, y') = log(p(y' = y))\n",
    "nll_loss = torch.nn.NLLLoss()\n",
    "\n",
    "# hinge loss (for SVM)\n",
    "# hinge(y, y') = max(1 - y*y', 0)\n",
    "hinge_loss = torch.nn.HingeEmbeddingLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee56de2e-08bf-4f20-ba88-8a5bb0190f0f",
   "metadata": {},
   "source": [
    "## The Module class\n",
    "\n",
    "The basic neural network building block in `nn` is the `Module` object. This object encapsulates a layer's parameters and helps maintain a record of required gradients for back propagation. In fact, layers, activations, and losses are all examples of `Module` objects.\n",
    "\n",
    "Modules are callable. Invoking them will call their `forward` method which runs a forward pass of the network's computational graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "42e02ef4-ade1-478b-ac41-1908ef7a503d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([15, 5])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fully_connected_output = fully_connected_layer(torch.rand(15, 100))\n",
    "fully_connected_output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "998004d9-1cdb-4e4f-9385-a8d64947f950",
   "metadata": {},
   "source": [
    "Note that most modules are designed to run on **batches** of inputs to encourage parallel computation.\n",
    "\n",
    "We can view a modules's parameters by using the `parameters` method. This is a generator of all parameters in the module. For example, a linear layer with no bias has exactly 1 parameter (which is `W`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "18f8164e-0045-4831-b277-ddae49524d71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "no_bias_linear = torch.nn.Linear(100, 100, bias=False)\n",
    "len(list(no_bias_linear.parameters()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01892da8-329f-4e6b-b56b-d89be14f7690",
   "metadata": {},
   "source": [
    "We can chain layers together using `nn.Sequential`. For example, the below cahins 2 linear layers and a leaky ReLU activation into a single module. The chained modules each have 1 parameter, but the sequential model his aware of both. Furthermore, invoking the module will result in an output shape of the final layer. Layers must be compatible, i.e., the first layer's output must be the same size as the second layer's input, and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2df49b6f-d827-46f0-8093-3970c0a64f40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output shape:   torch.Size([10, 10])\n",
      "num parameters: 2\n"
     ]
    }
   ],
   "source": [
    "two_layer_fc = torch.nn.Sequential(\n",
    "    torch.nn.Linear(100, 50, bias=False),\n",
    "    torch.nn.LeakyReLU(negative_slope=1e-3),\n",
    "    torch.nn.Linear(50, 10, bias=False)\n",
    ")\n",
    "out = two_layer_fc(torch.rand(10, 100))\n",
    "\n",
    "print(f'output shape:   {out.shape}')\n",
    "print(f'num parameters: {len(list(two_layer_fc.parameters()))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e94784ab-6b84-4322-b064-2dc450ac321f",
   "metadata": {},
   "source": [
    "Modules are also stored in a particulary device. We can move a module from one device to another using the `to` method, jsut as we would a tensor. This moves all parameters to the parameters of this module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d51709e3-a3af-4da4-96e8-63c9f9583f3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parameter 1 device: cpu\n",
      "parameter 2 device: cpu\n"
     ]
    }
   ],
   "source": [
    "fully_connected_layer.to(device)\n",
    "for i, parameter in enumerate(fully_connected_layer.parameters(), 1):\n",
    "    print(f'parameter {i} device: {parameter.device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca8c2d9-ac7d-43ef-bf0d-0b0f7ad0a12e",
   "metadata": {},
   "source": [
    "## Custom Modules\n",
    "\n",
    "We can create custom modules by extending the `Module` class. A subclass of `Module` should register its inner parameters and modules in the `__init__` function, and use them to compute the `forward` method's output. Registered parameters with `requires_grad=True` and modules will be included in the `backward` call when using automatic differentiation. Furthermore, they will be transfered to the desired device when calling the `to` method.\n",
    "\n",
    "Let us create our very own convolutional neural network (CNN):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "65c819a6-ca76-4bf7-b6e1-ee579e790449",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MyCNN(\n",
       "  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (relu1): ReLU()\n",
       "  (conv2_and_relu2): Sequential(\n",
       "    (0): Conv2d(16, 4, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (1): ReLU()\n",
       "  )\n",
       "  (classifier): Linear(in_features=400, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for easy coding, import nn directly\n",
    "from torch import nn\n",
    "\n",
    "class MyCNN(nn.Module):\n",
    "    def __init__(self, num_input_channels):\n",
    "        super().__init__()\n",
    "        \n",
    "        # regirstration option 1:\n",
    "        # save basic layers as public instance variables\n",
    "        self.conv1 = nn.Conv2d(num_input_channels, 16, 3)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        \n",
    "        # registration option 2:\n",
    "        # combine basic layers to a single public instance variable\n",
    "        self.conv2_and_relu2 = nn.Sequential(nn.Conv2d(16, 4, 3), nn.ReLU())\n",
    "        \n",
    "        # registration option 3:\n",
    "        # use the `add_module` function\n",
    "        self.add_module('classifier', nn.Linear(400, 3))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # invoke first layer module by module\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu1(x)\n",
    "        \n",
    "        # invoke sequential module\n",
    "        x = self.conv2_and_relu2(x)\n",
    "        \n",
    "        # invoke internally registered module\n",
    "        x = x.flatten(start_dim=1)\n",
    "        x = dict(self.named_modules())['classifier'](x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "my_cnn = MyCNN(num_input_channels=3).to(device)  # use model with chosen device\n",
    "my_cnn  # printing a module will show all registered modules in order of registration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d4e18487-cff2-4b74-8275-5e38fc551dc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output shape: torch.Size([14, 3])\n"
     ]
    }
   ],
   "source": [
    "out = my_cnn(torch.rand(14, 3, 14, 14))\n",
    "print(f'output shape: {out.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d37c0603-c92c-43ed-9f2a-bd66a5dd00ca",
   "metadata": {},
   "source": [
    "## Visualising modules\n",
    "\n",
    "We already saw that printing a module gives a general sense of its structure. To visualize the computational graph in a more interactive fashion, we can use `tensorboard`. This is an extension to pytorch (and other tensor-centric libraries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "112099e6-b35d-4640-8c96-f3d42f3ab93b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load tensorboard extension to enable showing tensorboard directly in the notebook\n",
    "%load_ext tensorboard\n",
    "\n",
    "# use a SummaryWriter to save the graph\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# use pathlib to get the current home folder where the dataset will be saved\n",
    "from pathlib import Path\n",
    "\n",
    "# initialize summary_writer\n",
    "writer = SummaryWriter(Path('demo_boards') / 'net_graph_demo' / 'graph')\n",
    "writer.add_graph(my_cnn, torch.rand(14, 3, 14, 14))\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1aea86e2-1502-47a9-943c-6a76d7019ada",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-64816b8cf3149b24\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-64816b8cf3149b24\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir=demo_boards/net_graph_demo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1351b64-b949-41ac-b18d-ad3249b3b4c5",
   "metadata": {},
   "source": [
    "More on tensorboard in the [documentation](https://pytorch.org/tutorials/intermediate/tensorboard_tutorial.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b264cf-1bff-4f37-bc71-ffb726473288",
   "metadata": {},
   "source": [
    "<a id='section:classifier'></a>\n",
    "\n",
    "# Training a classifier\n",
    "\n",
    "In machine learning, classification is the task of separating data into different classes.\n",
    "\n",
    "<img src='https://www.herevego.com/wp-content/uploads/2020/06/linear_vs_nonlinear.jpg' />\n",
    "\n",
    "Formally, given a sample set $S = \\{(x_i, y_i)\\}_{i=1}^m \\in \\mathcal{X}\\times\\mathcal{Y}$ where $\\mathcal{X}$ is the feature space, $\\mathcal{Y} = \\{1, ..., n - 1\\}$ is the label space, and all $(x_i, y_i)$ were sampled I.I.D. from $\\mathcal{X}\\times\\mathcal{Y}\\sim D$. Our objective is to find a classifier $h:\\mathcal{X}\\rightarrow\\mathcal{Y}$ that minimizes the generalization error:\n",
    "$$\\mathcal{L}_D(h) = \\mathbb{E}_{(x,y)\\sim D}[\\mathbf{1}_{h(x)=y}]$$\n",
    "that is, we want to want to generalize to the entire distribution even though we only have a sample $S$ from that distribution.\n",
    "\n",
    "Below we implement a classifier for [MNIST](http://yann.lecun.com/exdb/mnist/), a dataset of grayscale images of handwritten digits."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b2b647d-769e-4de7-a7bb-6c2c0229da68",
   "metadata": {},
   "source": [
    "## Torch extensions and torchvision\n",
    "\n",
    "There are some very popular extensions to pytorch. Some examples include\n",
    "- [torchaudio](https://pytorch.org/audio/stable/index.html) - a library for audio and signal processing.\n",
    "- [torchtext](https://pytorch.org/text/stable/index.html) - an extension for tensor-driven natural language processing.\n",
    "- [pytorch-geometric](https://pytorch-geometric.readthedocs.io/en/latest/) - a library built upon PyTorch to easily write and train Graph Neural Networks.\n",
    "\n",
    "Some extensions maintained by the pytorch developers and others are not. We will be using [torchvision](https://pytorch.org/vision/stable/index.html), a deep learning framework for computer vision, to load the MNIST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "43b34d45-5fa5-4f64-9b6e-aed6661967d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torchvision and torchvision.\n",
    "import torchvision\n",
    "\n",
    "# import transforms for casting PIL images to pytorch tensors\n",
    "import torchvision.transforms as tf\n",
    "\n",
    "# load the training dataset\n",
    "ds_train = torchvision.datasets.MNIST(root=Path.home() / '.pytorch-datasets',\n",
    "                                      train=True,  # training set\n",
    "                                      download=True,  # download if doesn't exist\n",
    "                                      transform=tf.ToTensor())  # conver PIL to tensor\n",
    "\n",
    "# load the test dataset\n",
    "ds_test = torchvision.datasets.MNIST(root=Path.home() / '.pytorch-datasets',\n",
    "                                     train=False,  # test set\n",
    "                                     download=True,  # download if doesn't exist\n",
    "                                     transform=tf.ToTensor())  # conver PIL to tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51bd207f-66dd-467b-a5eb-3edb01369ad3",
   "metadata": {},
   "source": [
    "Torchvision also implements select neural network architectures that come packaged with weights pre-trained on the [ImageNet](https://www.image-net.org/) dataset. For example, below we initialize a resnet18 CNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6699f12d-4d80-4809-8cb2-5158138dc539",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resnet = torchvision.models.resnet18()\n",
    "resnet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe0746bd-18d6-4adc-be49-2a415bc9636b",
   "metadata": {},
   "source": [
    "With MNIST, however, we do not need such a complex architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb4d3dc-3cb3-4cf6-993f-6f9479ca1a35",
   "metadata": {},
   "source": [
    "## Datasets and DataLoaders\n",
    "\n",
    "Loading the data in a way that is compatible with the deep learning model is trickier than it seems. For example:\n",
    "* data may be too large to fit in memory.\n",
    "* the raw data may need some type casting.\n",
    "* we may want to run a common preprocessing function on all the data.\n",
    "\n",
    "`torch.utils.data.Dataset` and `torch.utils.data.DataLoader` objects attempt to encapsulate some of the common requirements for data loading. Datasets enforce a unified API for any data type for loading data into memory and data access via indexing or iteration. The MNIST datasets that we loaded earlier are of the `Dataset` class. This means that they can be indexed and measuerd:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f5bc89ce-692c-4eca-b363-294b4589cd09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of training samples: 60000\n",
      "first training sample shape: torch.Size([1, 28, 28])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first training label: 5\n"
     ]
    }
   ],
   "source": [
    "print(f'num of training samples: {len(ds_train)}')\n",
    "\n",
    "sample, label = ds_train[0]\n",
    "print(f'first training sample shape: {sample.shape}')\n",
    "\n",
    "plt.axis('off')\n",
    "plt.title('first training sample as image')\n",
    "sample_img_format = sample.permute(1, 2, 0)  # move channels dim to the end\n",
    "plt.imshow(sample_img_format, cmap='gray')\n",
    "plt.show()  # force show before the label\n",
    "\n",
    "print(f'first training label: {label}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d0e3f6-9208-4433-a73b-931fab38c882",
   "metadata": {},
   "source": [
    "A DataLoader object is a batch loading tool for datasets. They offer batch size control, dataset shuffling, multi-threaded, specific sampling, and many more features. Below is a simple example of single-threaded DataLoaders for the training dataset that loads batches of 4 images and shuffles the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e7057fc4-5333-4d8f-9e78-b4dfaf91572d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x504 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def plot_4by4_image_batch(image_batch, label_batch):\n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(8, 7))\n",
    "    ax1.axis('off')\n",
    "    ax1.imshow(image_batch[0].squeeze(), cmap='gray')\n",
    "    ax1.set_title(label_batch[0].item(), fontsize=30)\n",
    "    ax2.axis('off')\n",
    "    ax2.imshow(image_batch[1].squeeze(), cmap='gray')\n",
    "    ax2.set_title(label_batch[1].item(), fontsize=30)\n",
    "    ax3.axis('off')\n",
    "    ax3.imshow(image_batch[2].squeeze(), cmap='gray')\n",
    "    ax3.set_title(label_batch[2].item(), fontsize=30)\n",
    "    ax4.axis('off')\n",
    "    ax4.imshow(image_batch[3].squeeze(), cmap='gray')\n",
    "    ax4.set_title(label_batch[3].item(), fontsize=30)\n",
    "    plt.show()\n",
    "\n",
    "dl_train = DataLoader(ds_train, batch_size=4, shuffle=True)\n",
    "\n",
    "for image_batch, label_batch in dl_train:\n",
    "    plot_4by4_image_batch(image_batch, label_batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9544d786-54e5-49ba-9a0a-7f82d3957a2e",
   "metadata": {},
   "source": [
    "Datasets and data-loaders have many options, features, and variations. Further reading can be found in the [documentation](https://pytorch.org/docs/stable/data.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ebdb08-47ad-4009-9153-ec31352019da",
   "metadata": {},
   "source": [
    "## Optimizers\n",
    "\n",
    "[Pytorch optimizers](https://pytorch.org/docs/stable/optim.html) handle parameter optimization. an optimizer is bound to a set of parameters. After a model has completed its forward pass, the optimizer will handle gradient step updates for all parameters that require a gradient. There are a number of different optimization techniques, each implemented as a different optimizer class. Below is an example of an `SGD` optimizer that implements stochastic gradient descent, bound to our previously initialized resnet model. The `SGD` optimizer requires an additional learning rate parameter `lr`, and implements other features such as [L2 regularization](https://en.wikipedia.org/wiki/Regularization_(mathematics)) via the `weight_decay` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a85e5ffa-eafb-40ff-a97f-47d8dd3a1c9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGD (\n",
       "Parameter Group 0\n",
       "    dampening: 0\n",
       "    lr: 0.001\n",
       "    maximize: False\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 1e-05\n",
       ")"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.optim import SGD\n",
    "\n",
    "optimizer = SGD(resnet.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d5a1957-772f-4f39-bbd1-e0eb38fe10e9",
   "metadata": {},
   "source": [
    "Optimizers have 2 main methods: `step` and `zero_grad`. `zero_grad` resets all bound parameter gradients before the `backward` call (since gradients are accumulated unless they are reset). `step` updates all bound parameters via the optimizer's update rule, e.g., gradient descent in SGD. In the next section, we will use these methods in the training code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6cd4d44-1276-45d6-9582-725c7f584f7f",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "We are now ready to train a classifier from scratch. We will use a basic linear classifier trained using stochastic gradient descent. Let us first define our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d87f52ad-a883-4fb6-bcda-97fec3807c31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=784, out_features=10, bias=True)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set random seed for reproducibility\n",
    "# This will cause the model parameters to be initializes the same way every time\n",
    "torch.manual_seed(24)\n",
    "\n",
    "# define our learning model\n",
    "# NOTE: we send the model to the correct device using the `to` method\n",
    "model = nn.Linear(in_features=torch.numel(ds_train[0][0]), out_features=10).to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e31d8b-d439-46fc-9477-77b22b1b4de8",
   "metadata": {},
   "source": [
    "Let us write a function for training a single epoch. Since each epoch will evaluate both the training and the test sets, we write a single function that calculates gradients and updates parameters only when providing an optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3e53ea41-659c-49e7-9484-8d80cfe765f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epoch(model, dl, loss_fn, optimizer):\n",
    "    # training step only performed if an optimizer is provided\n",
    "    training_mode = optimizer is not None\n",
    "\n",
    "    # set training mode for model\n",
    "    model.train(training_mode)\n",
    "    \n",
    "    # aggregators of loss and correct predictions (for calculating accuracy)\n",
    "    total_loss = 0\n",
    "    num_correct = 0\n",
    "    \n",
    "    # iterate batches\n",
    "    for batch_obs, batch_actions in dl:\n",
    "        # move batch to device\n",
    "        batch_obs, batch_actions = batch_obs.to(device), batch_actions.to(device)\n",
    "        \n",
    "        # gradients are required only if training\n",
    "        with torch.set_grad_enabled(mode=training_mode):\n",
    "            batch_obs = batch_obs.flatten(start_dim=1)\n",
    "            scores = model(batch_obs)\n",
    "            loss = loss_fn(scores, batch_actions)\n",
    "            preds = torch.argmax(scores, dim=-1)\n",
    "\n",
    "        # optimization step required only if training\n",
    "        if training_mode:\n",
    "            \n",
    "            # reset all gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # calculate gradients\n",
    "            loss.backward()\n",
    "            \n",
    "            # update parameters\n",
    "            optimizer.step()\n",
    "        \n",
    "        # aggregate loss and correct predictions\n",
    "        total_loss += loss.item()\n",
    "        num_correct += (batch_actions == preds).sum().item()\n",
    "    \n",
    "    # return mean loss and accuracy\n",
    "    return total_loss / len(dl), num_correct / len(dl.dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c75fb5-b91c-4a34-a229-337d786d849d",
   "metadata": {},
   "source": [
    "We now have define our training hyperparameters. Note that there can be many (many) more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "951ecc5c-43f3-4993-954b-6b796b0cb7a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 20\n",
    "BATCH_SIZE = 64\n",
    "SHUFFLE = False\n",
    "LEARNING_RATE=1e-3\n",
    "PRINT_EVERY = 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cce2023-6d8d-439d-beb1-c72fe0635727",
   "metadata": {},
   "source": [
    "Finally, we can train our linear classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d7b5cba2-b4f1-4c55-b3ff-feba17022b08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1:\n",
      "avg training loss       = 1.8975644101466198\n",
      "avg test loss           = 1.5668361817195917\n",
      "training acc            = 0.5835333333333333\n",
      "test acc                = 0.7642\n",
      "====================================================\n",
      "\n",
      "epoch 2:\n",
      "avg training loss       = 1.384784778552269\n",
      "avg test loss           = 1.2028603925826444\n",
      "training acc            = 0.77835\n",
      "test acc                = 0.8133\n",
      "====================================================\n",
      "\n",
      "epoch 3:\n",
      "avg training loss       = 1.1177420377222969\n",
      "avg test loss           = 1.0025153198059957\n",
      "training acc            = 0.8099\n",
      "test acc                = 0.8303\n",
      "====================================================\n",
      "\n",
      "epoch 4:\n",
      "avg training loss       = 0.9620047477262614\n",
      "avg test loss           = 0.8788066191278445\n",
      "training acc            = 0.8240166666666666\n",
      "test acc                = 0.8424\n",
      "====================================================\n",
      "\n",
      "epoch 5:\n",
      "avg training loss       = 0.8610462445948424\n",
      "avg test loss           = 0.7951256638499582\n",
      "training acc            = 0.8335833333333333\n",
      "test acc                = 0.8512\n",
      "====================================================\n",
      "\n",
      "epoch 6:\n",
      "avg training loss       = 0.7902043135181419\n",
      "avg test loss           = 0.7346186966273436\n",
      "training acc            = 0.8402333333333334\n",
      "test acc                = 0.8553\n",
      "====================================================\n",
      "\n",
      "epoch 7:\n",
      "avg training loss       = 0.7375480829716237\n",
      "avg test loss           = 0.6886670617920578\n",
      "training acc            = 0.8454166666666667\n",
      "test acc                = 0.8594\n",
      "====================================================\n",
      "\n",
      "epoch 8:\n",
      "avg training loss       = 0.6967013159921682\n",
      "avg test loss           = 0.6524521888820989\n",
      "training acc            = 0.85\n",
      "test acc                = 0.8641\n",
      "====================================================\n",
      "\n",
      "epoch 9:\n",
      "avg training loss       = 0.6639666029575791\n",
      "avg test loss           = 0.6230794608972634\n",
      "training acc            = 0.85415\n",
      "test acc                = 0.8673\n",
      "====================================================\n",
      "\n",
      "epoch 10:\n",
      "avg training loss       = 0.6370551736274762\n",
      "avg test loss           = 0.5987066421539161\n",
      "training acc            = 0.8566666666666667\n",
      "test acc                = 0.8705\n",
      "====================================================\n",
      "\n",
      "epoch 11:\n",
      "avg training loss       = 0.6144739745744764\n",
      "avg test loss           = 0.5781050858793745\n",
      "training acc            = 0.85965\n",
      "test acc                = 0.8727\n",
      "====================================================\n",
      "\n",
      "epoch 12:\n",
      "avg training loss       = 0.5952064996875172\n",
      "avg test loss           = 0.560423235319982\n",
      "training acc            = 0.8623166666666666\n",
      "test acc                = 0.8747\n",
      "====================================================\n",
      "\n",
      "epoch 13:\n",
      "avg training loss       = 0.5785358490815549\n",
      "avg test loss           = 0.5450515287696935\n",
      "training acc            = 0.8643166666666666\n",
      "test acc                = 0.8762\n",
      "====================================================\n",
      "\n",
      "epoch 14:\n",
      "avg training loss       = 0.5639412961820803\n",
      "avg test loss           = 0.5315417715698291\n",
      "training acc            = 0.8656166666666667\n",
      "test acc                = 0.8777\n",
      "====================================================\n",
      "\n",
      "epoch 15:\n",
      "avg training loss       = 0.551035039110987\n",
      "avg test loss           = 0.5195566700522307\n",
      "training acc            = 0.8673833333333333\n",
      "test acc                = 0.8798\n",
      "====================================================\n",
      "\n",
      "epoch 16:\n",
      "avg training loss       = 0.5395219960827817\n",
      "avg test loss           = 0.5088372383337871\n",
      "training acc            = 0.8687666666666667\n",
      "test acc                = 0.8802\n",
      "====================================================\n",
      "\n",
      "epoch 17:\n",
      "avg training loss       = 0.5291734568949448\n",
      "avg test loss           = 0.4991812170216232\n",
      "training acc            = 0.8703166666666666\n",
      "test acc                = 0.8812\n",
      "====================================================\n",
      "\n",
      "epoch 18:\n",
      "avg training loss       = 0.5198092489545025\n",
      "avg test loss           = 0.4904281323312954\n",
      "training acc            = 0.8718333333333333\n",
      "test acc                = 0.8824\n",
      "====================================================\n",
      "\n",
      "epoch 19:\n",
      "avg training loss       = 0.5112853798467213\n",
      "avg test loss           = 0.482448981920625\n",
      "training acc            = 0.8731333333333333\n",
      "test acc                = 0.883\n",
      "====================================================\n",
      "\n",
      "epoch 20:\n",
      "avg training loss       = 0.5034852779305565\n",
      "avg test loss           = 0.47513877590940257\n",
      "training acc            = 0.87455\n",
      "test acc                = 0.8844\n",
      "====================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# initialize data-loaders\n",
    "dl_train = DataLoader(ds_train, batch_size=BATCH_SIZE, shuffle=SHUFFLE)\n",
    "dl_test = DataLoader(ds_test, batch_size=BATCH_SIZE, shuffle=False)  # only for eval\n",
    "\n",
    "# initialize an SGD optimizer for the given model's parameters\n",
    "optimizer = SGD(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# initialize the loss function\n",
    "# use cross entropy loass for logistic regression (convex problem)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# log training performance for tensorboard\n",
    "writer = SummaryWriter(Path('demo_boards') / 'training' / 'results')\n",
    "\n",
    "\n",
    "# iterate through epochs\n",
    "for i in range(1, NUM_EPOCHS + 1):\n",
    "    # training epoch\n",
    "    train_loss, train_acc = run_epoch(model, dl_train, loss_fn, optimizer)\n",
    "    \n",
    "    # log training results for tensorboard\n",
    "    writer.add_scalar('training loss', train_loss, i)\n",
    "    writer.add_scalar('training accuracy', train_acc, i)\n",
    "\n",
    "    # validation epoch. no optimizer <==> no training\n",
    "    test_loss, test_acc = run_epoch(model, dl_test, loss_fn, optimizer=None)\n",
    "    \n",
    "    # log test results for tensorboard\n",
    "    writer.add_scalar('test loss', test_loss, i)\n",
    "    writer.add_scalar('test accuracy', test_acc, i)\n",
    "\n",
    "    # log epoch progress if necessary\n",
    "    if i % PRINT_EVERY == 0:\n",
    "        print(f'epoch {i}:')\n",
    "        print(f'avg training loss       = {train_loss}')\n",
    "        print(f'avg test loss           = {test_loss}')\n",
    "        print(f'training acc            = {train_acc}')\n",
    "        print(f'test acc                = {test_acc}')\n",
    "        print('====================================================')\n",
    "        print()\n",
    "\n",
    "# close writer and flush data\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "043c5a67-f7b0-43ba-b976-a50704b9ac0f",
   "metadata": {},
   "source": [
    "As we can see, a linear classifier does a fairly good job classifying handwritten digits, and can perform even better with some hyperparameter fine-tuning. Throughout training, we logged the model's performance on the training and test set to tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b9f06aa7-9472-42b5-a65c-289e40842901",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-c6de9e4cd4679b3a\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-c6de9e4cd4679b3a\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6007;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir=demo_boards/training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c33e4a2-ff52-41db-9724-e375e01bd4f8",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "Pytorch is a very powerful tool for deep learning. In this tutorial, we only scratched the surface of its capabilities. The [pytorch documentation](https://pytorch.org/docs/stable/index.html) is filled with additional features, some general, and some earily specific. There, you will also find documentation for extensions like torchtext and torchvision. However, as with many code libraries, best way to learn to use this library is by using it."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
