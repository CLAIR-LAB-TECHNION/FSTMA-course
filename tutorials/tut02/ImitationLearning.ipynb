{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e59cf269-97db-49f3-accd-4f7801beb692",
   "metadata": {},
   "source": [
    "Author: Guy Azran  \n",
    "E-mail: [guy.azran@campus.technion.ac.il](mailto:guy.azran@campus.technion.ac.il)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c04ad1-e844-4836-9de1-206dc454d870",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Imitation Learning\n",
    "\n",
    "<img src='https://upload.wikimedia.org/wikipedia/commons/1/1f/Makak_neonatal_imitation.png?1648499532601' width=1000/>\n",
    "\n",
    "<a id=\"section:intro\"></a>\n",
    "\n",
    "# Introduction\n",
    "Imitation Learning (IL) is a technique for learning a policy from demonstrations produced by an \"expert\" (in most cases, a human). There are several types of imitation learning methods, but the simplest approach is called Behavior Cloning (BC). In BC, we attempt to learn a classifier (or regressor if actions are continuous) where the feature space $\\mathcal{X}$ is some representation of the state and the label set $\\mathcal{Y}$ is the set of actions. The expert provides a \"correct\" action for a sample set of states by running in the environment and recording the actions taken at each state. This data is used to learn a classifier that predicts what action the expert would have taken at each state.\n",
    "\n",
    "In this notebook, we present the BC method and implement an example on the [gym taxi environment](https://gym.openai.com/envs/Taxi-v3/). Our expert will be an A* algorithm with an admissible heuristic (ensuring optimality). We will then learn from the collected expert data using a multi-layer perceptron neural network, implemented in pytorch.\n",
    "\n",
    "Recommended Lecture on IL: [Part 1](https://www.youtube.com/watch?v=kGc8jOy5_zY), [Part 2](https://www.youtube.com/watch?v=06uB13C5pxw), [Part 3](https://www.youtube.com/watch?v=a5wkzPa4fO4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2bcb5e2f-b9e0-4354-a24a-2f893b1313e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import itertools\n",
    "\n",
    "from collections import deque\n",
    "\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from AI_agents.Environments.gym_problem import GymProblem\n",
    "from AI_agents.Search.best_first_search import a_star\n",
    "from pettingzoo.mpe import simple_v2\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from il_utils.dataset import ImitationLearningDataset\n",
    "from il_utils.model import MLP\n",
    "from il_utils.training import train_torch_model_sgd\n",
    "from common.evaluation import evaluate_policy\n",
    "from common.ipython_vis import animate_policy\n",
    "from common.mpe_physics import PhysicsUtils\n",
    "from common.pz_utils import SingleAgentParallelEnvGymWrapper\n",
    "\n",
    "\n",
    "# initialize taxi env\n",
    "taxi_env = gym.make(\"Taxi-v3\").env\n",
    "taxi_env.reset()\n",
    "\n",
    "# constants for taxi env planning\n",
    "PASSENGER_IN_TAXI = 4  # passenger idx when in taxi\n",
    "LOCS = taxi_env.unwrapped.locs  # environment locations\n",
    "\n",
    "# initialize MPE env\n",
    "mpe_simple_env = simple_v2.parallel_env(continuous_actions=True)\n",
    "mpe_simple_env = SingleAgentParallelEnvGymWrapper(mpe_simple_env)  # make gym compatible\n",
    "\n",
    "# random seed\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "998cefdd-fc4a-458b-b33d-d87b8bde0943",
   "metadata": {},
   "source": [
    "# The problem\n",
    "\n",
    "We model the world as an MDP where the state space is the set of all possible environment configurations, and the action space is the list of actions the taxi agent can execute within the environment. A possible configuration is any combination of taxi location, passenger location (including \"in_taxi\" indication), and the destination location. The domain map is 5x5, the passenger and destination can be one of 4 possible locations, and the passenger can be either in the taxi or at the initial destination, adding up to a total of $5\\cdot 5\\cdot 5\\cdot 4 = 500$ states. This makes the taxi environment learnable via deterministic planning and state-value function estimation algorithms. Nevertheless, we are going to ignore the MDP model completely, and only use its notations to collect data using an expert policy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "815e3964-f4f0-4408-af68-6dcfd0e37e4a",
   "metadata": {},
   "source": [
    "# The expert\n",
    "\n",
    "As mentioned in the [introduction](#section:intro), the expert is an implementation of the A* algorithm for this environment. [AI_agents](https://github.com/sarah-keren/AI_agents) is a library in production that provides generalized search algorithms and supports the taxi environment. The environment is wrapped as a custom `Problem` object that can be solved using A*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c9f3b5d-0fc3-41f3-97ca-89c8526d9288",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(AI_agents.Search.problem.Problem,)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "taxi_problem = GymProblem(taxi_env, taxi_env.unwrapped.s)\n",
    "taxi_problem.__class__.__bases__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc9d3d27-9ce5-4f9c-8dd2-08a7b606f66c",
   "metadata": {},
   "source": [
    "A* requires an admissible heuristic if we want to guarantee optimality. Below we define the following heuristic:\n",
    "* if the passenger is in the taxi, calculate the Manhatten distance between the taxi and the destination and add 1 for the dropoff action\n",
    "* if the passenger is not in the taxi, calculate the Manhatten distances between the taxi and the passenger, and between the passenger and the destination. Add 2 for the pickup and dropoff actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4335b249-1802-4f92-9c36-a2cde3f5ab14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def manhatten_dist(r1, c1, r2, c2):\n",
    "    # calssic manhatten dist |row1 - row2| + |col1 - col2|\n",
    "    return abs(r1 - r2) + abs(c1 - c2)\n",
    "\n",
    "def taxi_heuristic(node):\n",
    "    # decode state integer to interpretable values\n",
    "    taxi_row, taxi_col, passenger_idx, dest_idx = taxi_env.decode(node.state.get_key())\n",
    "\n",
    "    # split to 2 cases where the passenger is in the taxi and not in the taxi.\n",
    "    if passenger_idx == PASSENGER_IN_TAXI:\n",
    "        # dist from the taxi to the destination\n",
    "        return manhatten_dist(taxi_row, taxi_col, *LOCS[dest_idx]) + 1  # include dropoff\n",
    "    elif passenger_idx == dest_idx:\n",
    "        # passenger has reached the destination. this is a goal state\n",
    "        return 0\n",
    "    else:\n",
    "        # dist from the taxi to the passenger and from the passenger to the destination\n",
    "        passenger_dist = manhatten_dist(taxi_row, taxi_col, *LOCS[passenger_idx])\n",
    "        dest_dist = manhatten_dist(*LOCS[passenger_idx], *LOCS[dest_idx])\n",
    "        return passenger_dist + dest_dist + 2  # include pickup and dropoff actions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb7437bd-81ec-401a-9501-e24c8a148936",
   "metadata": {},
   "source": [
    "A planner policy simply takes an observation and makes a plan. While there are still acitons in that plan, yield the next action in the plan. Otherwise, start a new plan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d9e1c657-3457-45b2-8d19-6cd659ad18fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TaxiAStarPolicy:\n",
    "    def __init__(self, heuristic):\n",
    "        self.heuristic = heuristic\n",
    "\n",
    "        # a container for the plan actions.\n",
    "        self.cur_plan = deque()\n",
    "\n",
    "    def __call__(self, obs):\n",
    "        # if out of actions (finished previous plan), or if observation is not in current plan,\n",
    "        # create a new plan.\n",
    "        if not self.cur_plan or self.cur_plan[0][0] != obs:\n",
    "            # refresh the problem with a new initial state\n",
    "            taxi_prob = GymProblem(taxi_env, taxi_env.unwrapped.s)\n",
    "\n",
    "            # find the solution with the A* algorithm\n",
    "            _, node, sol, _, _ = a_star(taxi_prob, heuristic_func=self.heuristic)\n",
    "\n",
    "            # get a list of expected states\n",
    "            state_lst = []\n",
    "            while node.parent:\n",
    "                node = node.parent\n",
    "                state_lst.append(node.state.key)\n",
    "            state_lst = reversed(state_lst)\n",
    "\n",
    "            # save the plan for later extraction\n",
    "            # a plan is a tuple of expected observations and the corresponding expert action\n",
    "            self.cur_plan = deque(list(zip(state_lst, map(int, sol))))\n",
    "\n",
    "        # pop the next action\n",
    "        return self.cur_plan.popleft()[1]\n",
    "\n",
    "taxi_expert = TaxiAStarPolicy(taxi_heuristic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cafba03-3c03-4824-90bf-a37b53826adf",
   "metadata": {},
   "source": [
    "Let's see how our policy performs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ab5a7cc-3a3e-4233-a326-e5b5a028c87f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num episodes completed:   10\n",
      "total rewards:            79\n",
      "mean rewards per episode: 7.90\n"
     ]
    }
   ],
   "source": [
    "# This code can be terminated early with an interruption\n",
    "animate_policy(taxi_env, taxi_expert, episode_limit=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b1226a-918f-4699-8935-48ecdf055375",
   "metadata": {},
   "source": [
    "As we can see, we can use planning to define a perfect policy for this domain that solves the problem in real time. This will be our expert with whom we will collect our demonstration data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a5b6d3a-010f-44e6-b7db-32b6d5bcf22c",
   "metadata": {},
   "source": [
    "# Trajectories\n",
    "\n",
    "Given an initial state $s$ and a policy $\\pi$, a **trajectory** (in a deterministic environment) is a collection of state action pairs $((s_1, a_1), ..., (s_n, a_n))$ where $s_1=s$, $a_i = \\pi(s_i)$, and $s_{i+1}=P(s_i, a_i)$. In other words, a trajectory is an ordered collection of states and actions as experienced by running policy $\\pi$ starting from state $s$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "60ccde5c-c748-4b6f-9f10-abc15bb64a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trajectory struct\n",
    "class Trajectory:\n",
    "    def __init__(self, observations=None, actions=None):\n",
    "        self.observations = observations or []\n",
    "        self.actions = actions or []\n",
    "\n",
    "    def add_step(self, observation, action):\n",
    "        self.observations.append(observation)\n",
    "        self.actions.append(action)\n",
    "\n",
    "    def __str__(self):\n",
    "        return 'trajectory: ' + str(list(zip(self.observations, self.actions)))\n",
    "\n",
    "    def __repr__(self):\n",
    "        return str(self)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21130258-da1b-4a09-aa3c-9d7b73790f48",
   "metadata": {},
   "source": [
    "We can collect trajectories with our expert by simply recording the observations and the actions taken at these observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d582bd5c-27e4-4a5c-bc15-19de5e1be943",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "trajectory: [(123, 1), (23, 3), (3, 4), (19, 2), (39, 0), (139, 0), (239, 2), (259, 2), (279, 0), (379, 0), (479, 5)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_trajectory(env, policy, max_trajectory_length=float('inf')):\n",
    "    # init trajectory object\n",
    "    trajectory = Trajectory()\n",
    "\n",
    "    # get first observation\n",
    "    obs = env.reset()\n",
    "\n",
    "    # iterate and step in environment.\n",
    "    # limit num actions for incomplete policies\n",
    "    for i in itertools.count(start=1):\n",
    "        action = policy(obs)\n",
    "        trajectory.add_step(obs, action)\n",
    "        obs, reward, done, info = env.step(action)\n",
    "\n",
    "        if done or i >= max_trajectory_length:\n",
    "            break\n",
    "\n",
    "    return trajectory\n",
    "\n",
    "trajectory = get_trajectory(taxi_env, taxi_expert)\n",
    "trajectory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e58454e6-b7ee-4059-98aa-f9a9eebfa8cf",
   "metadata": {},
   "source": [
    "# Data collection and preparation\n",
    "\n",
    "We will now collect the data with which we will train simply by collecting multiple trajectories. As with most supervised learning settings, we will collect 3 datasets: training, validation, and testing. Since this is a relatively simple environment with a small number of states, we will collect a small number of trajectories so we do not encounter the entire state space in training. This way, we can see if our model is generalizing to new, unseen states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cd9f4754-b820-4bde-8642-d928141dc143",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0dc0e50643ca47b19f6eba1f3f03336e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f47ff064fd8e45a89cc5cd12179ab993",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9a7f9e4873e4300bfbad6c357a4278f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[trajectory: [(386, 1), (286, 1), (186, 1), (86, 4), (98, 3), (78, 3), (58, 0), (158, 0), (258, 3), (238, 3), (218, 0), (318, 0), (418, 5)],\n",
       " trajectory: [(222, 1), (122, 1), (22, 3), (2, 4), (18, 0), (118, 0), (218, 0), (318, 0), (418, 5)],\n",
       " trajectory: [(387, 1), (287, 1), (187, 1), (87, 4), (99, 3), (79, 0), (179, 0), (279, 0), (379, 0), (479, 5)],\n",
       " trajectory: [(91, 3), (71, 3), (51, 0), (151, 0), (251, 3), (231, 3), (211, 0), (311, 0), (411, 4), (419, 1), (319, 1), (219, 2), (239, 2), (259, 2), (279, 0), (379, 0), (479, 5)],\n",
       " trajectory: [(329, 1), (229, 3), (209, 0), (309, 0), (409, 4), (417, 1), (317, 1), (217, 2), (237, 2), (257, 1), (157, 1), (57, 2), (77, 2), (97, 5)]]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def collect_data(env, policy, num_trajectories, max_trajectory_length=float('inf')):\n",
    "    trajectories = []\n",
    "    for _ in tqdm(range(num_trajectories)):\n",
    "        trajectories.append(get_trajectory(env, policy, max_trajectory_length))\n",
    "\n",
    "    return trajectories\n",
    "\n",
    "# get the same trajectories every time!\n",
    "taxi_env.seed(SEED)\n",
    "\n",
    "taxi_raw_train_data = collect_data(taxi_env, taxi_expert, num_trajectories=400)\n",
    "taxi_raw_val_data = collect_data(taxi_env, taxi_expert, num_trajectories=250)\n",
    "taxi_raw_test_data = collect_data(taxi_env, taxi_expert, num_trajectories=250)\n",
    "\n",
    "# show the first 5 training trajectories\n",
    "taxi_raw_train_data[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144f5c7c-a7c0-432a-a137-101737396243",
   "metadata": {},
   "source": [
    "The taxi environment's states are simply integers. This type of input is not very informative for supervised learning algorithms. A state's bits can be decomposed into the state attributes. We must use these attributes to create a good representation for the learning algorithm. The preprocessing function below represents the state as a vector of the taxi location, the passenger location, the destination location, and an indicator of whether the passenger is in the taxi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6484896a-0195-49fb-8159-2d7d15c69e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_taxi_state(state):\n",
    "    # decompose state bits\n",
    "    taxi_row, taxi_col, passenger_idx, destination_idx = taxi_env.decode(state)\n",
    "\n",
    "    # get destination true location coordinates\n",
    "    destination_row, destination_col = LOCS[destination_idx]\n",
    "\n",
    "    # get passenger true location coordinates\n",
    "    # add `in_taxi` indicator bit\n",
    "    if passenger_idx == PASSENGER_IN_TAXI:\n",
    "        passenger_row, passenger_col = taxi_row, taxi_col\n",
    "        passenger_in_taxi = 1\n",
    "    else:\n",
    "        passenger_row, passenger_col = LOCS[passenger_idx]\n",
    "        passenger_in_taxi = 0\n",
    "\n",
    "    # return all data as a flat Tensor object for pytorch compatibility\n",
    "    return torch.Tensor([taxi_row,\n",
    "                         taxi_col,\n",
    "                         passenger_row,\n",
    "                         passenger_col,\n",
    "                         passenger_in_taxi,\n",
    "                         destination_row,\n",
    "                         destination_col])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee3de395-9683-4cf5-9b1e-a617254ab5e5",
   "metadata": {},
   "source": [
    "We can package the trajectories and preprocessing function into a single `Dataset` object. For this, we implement the `ImitationLearningDataset`, which unwraps the trajectories and keeps only the state-action pairs required for supervised learning. The dataset preprocess the states when requested. Note that we do **NOT** remove duplicates. This is to maintain the true trajectory sample distribution of the expert policy. The dataset is compatible with DataLoaders used to batch the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "261b1d4f-8826-4729-94c7-a242967413dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([3., 4., 0., 4., 0., 4., 0.]), 1)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "taxi_ds_train = ImitationLearningDataset(taxi_raw_train_data, prep_obs=prep_taxi_state)\n",
    "taxi_ds_val = ImitationLearningDataset(taxi_raw_val_data, prep_obs=prep_taxi_state)\n",
    "taxi_ds_test = ImitationLearningDataset(taxi_raw_test_data, prep_obs=prep_taxi_state)\n",
    "\n",
    "taxi_ds_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75e8b14e-c05f-4316-8b06-2a6e9e6ddcaa",
   "metadata": {},
   "source": [
    "# Learning model\n",
    "\n",
    "The learning model we will be using is the [multi-layer perceptron](https://www.sciencedirect.com/topics/computer-science/multilayer-perceptron#:~:text=Multi%20layer%20perceptron%20(MLP)%20is,input%20signal%20to%20be%20processed.) (MLP). This was one of the first neural network architectures. It uses multiple fully connected linear layers separated by non-linear activation functions ([ReLU](https://machinelearningmastery.com/rectified-linear-activation-function-for-deep-learning-neural-networks/) in our case). MLP's excell at finding statistical correlations in vector data that is strongly ordered and real valued, especially if these corelations are continuous or near-continuous. Since our state representation matches this description, we expect the MLP model to handle this task with aplomb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6c08529f-70dd-4e4e-89b5-7165641413f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLP(\n",
       "  (fc_layers): Sequential(\n",
       "    (0): Linear(in_features=7, out_features=32, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=32, out_features=64, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=64, out_features=128, bias=True)\n",
       "    (5): ReLU()\n",
       "    (6): Linear(in_features=128, out_features=6, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the input vector length from a training example\n",
    "in_features = len(taxi_ds_train[0][0])\n",
    "\n",
    "# The output vector length is the number of actions\n",
    "num_actions = taxi_env.action_space.n\n",
    "\n",
    "# seed to always get the same model parameter initialization\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "# create MLP model with 3 hidden layers\n",
    "mlp_taxi = MLP(in_features=in_features, hidden_dims=[32, 64, 128], out_features=num_actions)\n",
    "mlp_taxi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa726b59-d185-4584-9f4c-3158acc50c02",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "We will train the classifier with [stochastic gradient descent](https://towardsdatascience.com/stochastic-gradient-descent-clearly-explained-53d239905d31) optimization. We train using the Cross-Entropy loss which punishes the classifier for giving low scores to the true classes and higher scores to wrong classes (reminder, in this case a class is an action)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4419959e-1e16-4a69-b7b1-a4f37e39fecb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 10:\n",
      "avg training loss       = 0.7415909432893828\n",
      "avg validation loss     = 0.6625609495290896\n",
      "training acc            = 0.7384030418250951\n",
      "validation acc          = 0.7738095238095238\n",
      "timestamp: 02:03:48.088352\n",
      "====================================================\n",
      "\n",
      "epoch 20:\n",
      "avg training loss       = 0.22372974528956557\n",
      "avg validation loss     = 0.20323084589184784\n",
      "training acc            = 0.9309885931558936\n",
      "validation acc          = 0.9355921855921856\n",
      "timestamp: 02:03:51.305386\n",
      "====================================================\n",
      "\n",
      "epoch 30:\n",
      "avg training loss       = 0.10380267806770138\n",
      "avg validation loss     = 0.09135746266920029\n",
      "training acc            = 0.9678707224334601\n",
      "validation acc          = 0.9713064713064713\n",
      "timestamp: 02:03:54.603609\n",
      "====================================================\n",
      "\n",
      "epoch 40:\n",
      "avg training loss       = 0.059762057920459286\n",
      "avg validation loss     = 0.05956391345982145\n",
      "training acc            = 0.9857414448669202\n",
      "validation acc          = 0.9856532356532357\n",
      "timestamp: 02:03:57.957689\n",
      "====================================================\n",
      "\n",
      "epoch 50:\n",
      "avg training loss       = 0.03883942080262516\n",
      "avg validation loss     = 0.032670291512068815\n",
      "training acc            = 0.9899239543726236\n",
      "validation acc          = 0.9920634920634921\n",
      "timestamp: 02:04:01.170014\n",
      "====================================================\n",
      "\n",
      "epoch 60:\n",
      "avg training loss       = 0.025300384980363144\n",
      "avg validation loss     = 0.023367325223346308\n",
      "training acc            = 0.9937262357414449\n",
      "validation acc          = 0.9951159951159951\n",
      "timestamp: 02:04:04.396751\n",
      "====================================================\n",
      "\n",
      "epoch 70:\n",
      "avg training loss       = 0.019387099912954852\n",
      "avg validation loss     = 0.03232098173395526\n",
      "training acc            = 0.9946768060836502\n",
      "validation acc          = 0.989010989010989\n",
      "timestamp: 02:04:07.619476\n",
      "====================================================\n",
      "\n",
      "epoch 80:\n",
      "avg training loss       = 0.013397213915153477\n",
      "avg validation loss     = 0.017025646155631943\n",
      "training acc            = 0.9967680608365019\n",
      "validation acc          = 0.9954212454212454\n",
      "timestamp: 02:04:10.924171\n",
      "====================================================\n",
      "\n",
      "epoch 90:\n",
      "avg training loss       = 0.011457646860219864\n",
      "avg validation loss     = 0.013191527690278457\n",
      "training acc            = 0.997148288973384\n",
      "validation acc          = 0.9975579975579976\n",
      "timestamp: 02:04:14.153958\n",
      "====================================================\n",
      "\n",
      "epoch 100:\n",
      "avg training loss       = 0.008915963373925412\n",
      "avg validation loss     = 0.013084601802984253\n",
      "training acc            = 0.9979087452471483\n",
      "validation acc          = 0.996947496947497\n",
      "timestamp: 02:04:17.346277\n",
      "====================================================\n",
      "\n",
      "epoch 110:\n",
      "avg training loss       = 0.006560715908286555\n",
      "avg validation loss     = 0.011648655620193472\n",
      "training acc            = 0.9988593155893536\n",
      "validation acc          = 0.9978632478632479\n",
      "timestamp: 02:04:20.533992\n",
      "====================================================\n",
      "\n",
      "epoch 120:\n",
      "avg training loss       = 0.004892894885935155\n",
      "avg validation loss     = 0.012631781172417615\n",
      "training acc            = 0.9994296577946769\n",
      "validation acc          = 0.9975579975579976\n",
      "timestamp: 02:04:23.777454\n",
      "====================================================\n",
      "\n",
      "epoch 130:\n",
      "avg training loss       = 0.0038234993666275987\n",
      "avg validation loss     = 0.008962710388892091\n",
      "training acc            = 0.999809885931559\n",
      "validation acc          = 0.9981684981684982\n",
      "timestamp: 02:04:26.989645\n",
      "====================================================\n",
      "\n",
      "epoch 140:\n",
      "avg training loss       = 0.0030806481731718684\n",
      "avg validation loss     = 0.009218088457844982\n",
      "training acc            = 1.0\n",
      "validation acc          = 0.9984737484737485\n",
      "timestamp: 02:04:30.233438\n",
      "====================================================\n",
      "\n",
      "epoch 150:\n",
      "avg training loss       = 0.0025462507083888184\n",
      "avg validation loss     = 0.007891252753289314\n",
      "training acc            = 1.0\n",
      "validation acc          = 0.9984737484737485\n",
      "timestamp: 02:04:33.438699\n",
      "====================================================\n",
      "\n",
      "epoch 160:\n",
      "avg training loss       = 0.0021899507605183392\n",
      "avg validation loss     = 0.007460454271619333\n",
      "training acc            = 1.0\n",
      "validation acc          = 0.9984737484737485\n",
      "timestamp: 02:04:36.615096\n",
      "====================================================\n",
      "\n",
      "epoch 170:\n",
      "avg training loss       = 0.0017890283168437906\n",
      "avg validation loss     = 0.008613020926323526\n",
      "training acc            = 1.0\n",
      "validation acc          = 0.9984737484737485\n",
      "timestamp: 02:04:39.814192\n",
      "====================================================\n",
      "\n",
      "epoch 180:\n",
      "avg training loss       = 0.0015515929735003826\n",
      "avg validation loss     = 0.008017538776358578\n",
      "training acc            = 1.0\n",
      "validation acc          = 0.9984737484737485\n",
      "timestamp: 02:04:43.043680\n",
      "====================================================\n",
      "\n",
      "epoch 190:\n",
      "avg training loss       = 0.00137821892788156\n",
      "avg validation loss     = 0.00838971855296333\n",
      "training acc            = 1.0\n",
      "validation acc          = 0.9984737484737485\n",
      "timestamp: 02:04:46.293700\n",
      "====================================================\n",
      "\n",
      "epoch 200:\n",
      "avg training loss       = 0.0012744220619142624\n",
      "avg validation loss     = 0.007052963180112562\n",
      "training acc            = 1.0\n",
      "validation acc          = 0.9987789987789988\n",
      "timestamp: 02:04:49.592635\n",
      "====================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(train_losses,\n",
    " val_losses,\n",
    " train_accs,\n",
    " val_accs) = train_torch_model_sgd(model=mlp_taxi,\n",
    "                                   ds_train=taxi_ds_train,\n",
    "                                   ds_val=taxi_ds_val,\n",
    "                                   loss_fn=torch.nn.CrossEntropyLoss(),\n",
    "                                   batch_size=16,\n",
    "                                   shuffle_data=True,\n",
    "                                   num_epochs=200,\n",
    "                                   learning_rate=1e-2,\n",
    "                                   weight_decay=1e-5,\n",
    "                                   print_every=10,\n",
    "                                   include_accs=True,\n",
    "                                   seed=SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2716d33a-2d06-4ed9-8247-58485e509bd5",
   "metadata": {},
   "source": [
    "Now let us visualize the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6b5a211e-f625-4fef-be08-fef584144037",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x504 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot 1x2 figure grid\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 7))\n",
    "\n",
    "# plot losses\n",
    "ax1.set_title('Cross-Entropy Loss per Epoch')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Cross-Entropy Loss')\n",
    "ax1.plot(train_losses, label='train')\n",
    "ax1.plot(val_losses, label='validation')\n",
    "ax1.legend()\n",
    "\n",
    "# plot accuracies\n",
    "ax2.set_title('Accuracy per Epoch')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Accuracy')\n",
    "ax2.plot(train_accs, label='train')\n",
    "ax2.plot(val_accs, label='validation')\n",
    "ax2.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d43509-2f4f-46ba-b644-936c65eca555",
   "metadata": {},
   "source": [
    "We see a relatiely short and stable training process. Seemingly, our learning algorithm is able to clone the expert's behavior almost perfectly. Below are the final accuracies of our model on our datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "85f84f6c-597a-48f1-b420-947cc0607946",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train acc      = 1.0\n",
      "validation acc = 0.9987789988517761\n",
      "test acc       = 0.9978671669960022\n"
     ]
    }
   ],
   "source": [
    "def ds_acc(ds, model):\n",
    "    # get the entire dataset at once.\n",
    "    # this is will cause memory issues with large datasets\n",
    "    ds_data, ds_labels = next(iter(DataLoader(ds, batch_size=len(ds))))\n",
    "\n",
    "    # run get model predictions\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        ds_preds = model(ds_data)\n",
    "\n",
    "    # calculate prediction accuracies\n",
    "    return torch.mean((torch.argmax(ds_preds, dim=-1) == ds_labels).float()).item()\n",
    "\n",
    "# print test set accuracy\n",
    "print(f'train acc      = {ds_acc(taxi_ds_train, mlp_taxi)}')\n",
    "print(f'validation acc = {ds_acc(taxi_ds_val, mlp_taxi)}')\n",
    "print(f'test acc       = {ds_acc(taxi_ds_test, mlp_taxi)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53effb1b-18fc-478c-88ae-b65d95bb123f",
   "metadata": {},
   "source": [
    "The above reults are considered to be excellent results in classification tasks, surpassing human annotators. Does this mean we have an excellent policy? What happens when a predicted action is not optimal? Can our agent recover from such mistakes?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb418e3-7d83-4e21-b19e-40ed29e6d127",
   "metadata": {},
   "source": [
    "# Evaluating the policy\n",
    "\n",
    "Our newly trained classifier can be used as a policy for the taxi environment. With every input observation, the classifier gives each action a score. The action that was scored the highest is the one returned by the policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a6eefb67-0387-49cc-9317-f703551aa54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassifierPolicy:\n",
    "    def __init__(self, model, prep_fn=None):\n",
    "        self.model = model\n",
    "\n",
    "        # if no preprocessing function is given, use the identity function\n",
    "        if prep_fn is None:\n",
    "            self.prep_fn = lambda x: x\n",
    "        else:\n",
    "            self.prep_fn = prep_fn\n",
    "\n",
    "    def __call__(self, observation):\n",
    "        # preprocess observation\n",
    "        prepped_obs = self.prep_fn(observation)\n",
    "        one_obs_batch = prepped_obs[None]  # convert to batch of size 1\n",
    "\n",
    "        # run model to get action scores\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            batch_scores = self.model(one_obs_batch)\n",
    "\n",
    "        # get scores for single observation\n",
    "        obs_score = batch_scores[0]\n",
    "\n",
    "        # choose the action with the highest score\n",
    "        return torch.argmax(obs_score).item()\n",
    "\n",
    "# create a policy driven by the MLP model that uses the same observation preprocessing\n",
    "# function as in training\n",
    "taxi_il_policy = ClassifierPolicy(mlp_taxi, prep_fn=taxi_ds_train.prep_obs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c14618f4-a050-4682-b0be-4ba1a27171fd",
   "metadata": {},
   "source": [
    "## How good is can an immitation policy be?\n",
    "\n",
    "Let us compare the performance of the expert policy and the classifier policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c35862a6-7a16-401a-a208-ed59ca2705e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7eb728a16e54798ae3f5e7b6935f5ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A* Policy\n",
      "---------\n",
      "total reward over all episodes: 77458\n",
      "mean reward per episode:        7.7458\n"
     ]
    }
   ],
   "source": [
    "total_reward, mean_reward = evaluate_policy(taxi_env, taxi_expert, num_episodes=10_000,\n",
    "                                            seed=SEED)\n",
    "print('A* Policy')\n",
    "print('---------')\n",
    "print(f'total reward over all episodes: {total_reward}')\n",
    "print(f'mean reward per episode:        {mean_reward}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9efa2eb1-851b-4eed-b7be-6b9b837bf2ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d011be857cec4a3f992ff271e4edf32d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier Policy\n",
      "-----------------\n",
      "total reward over all episodes: 75359\n",
      "mean reward per episode:        7.5359\n"
     ]
    }
   ],
   "source": [
    "total_reward, mean_reward = evaluate_policy(taxi_env, taxi_il_policy, num_episodes=10_000, seed=SEED)\n",
    "print('Classifier Policy')\n",
    "print('-----------------')\n",
    "print(f'total reward over all episodes: {total_reward}')\n",
    "print(f'mean reward per episode:        {mean_reward}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd910fc-6e4e-4f7c-ba6e-aae94da2dac7",
   "metadata": {},
   "source": [
    "We note that, in expectation, our imitation policy performs almost as well as the expert. Sadly, this is not the only measure of a policy.\n",
    "\n",
    "## Imitation learning failures\n",
    "\n",
    "Like we did before, let us see our policy in action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6b80f26b-4d0c-4e69-8505-005edcae27eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num episodes completed:   10\n",
      "total rewards:            88\n",
      "mean rewards per episode: 8.80\n"
     ]
    }
   ],
   "source": [
    "# This code can be terminated early with an interruption\n",
    "animate_policy(taxi_env, taxi_il_policy, episode_limit=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb4a4ab-2a23-4c5a-9a25-eb3283b8d6d6",
   "metadata": {},
   "source": [
    "In the vast majority of cases, our policy acts identically to the optimal A* policy. However, during the above animation, you may have seen the policy fail miserably in one or more starting position. Let us recreate one such a scenario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "19b4b943-41f6-47bd-afa9-2ae19014e2a6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "|\u001b[43m \u001b[0m| : | : |\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (East)\n"
     ]
    }
   ],
   "source": [
    "# reset env\n",
    "taxi_env.reset()\n",
    "\n",
    "# set current state to be the failure state\n",
    "failure_obs = taxi_env.encode(3, 0, 3, 2)\n",
    "taxi_env.unwrapped.s = failure_obs\n",
    "\n",
    "# step using\n",
    "failure_action = taxi_il_policy(failure_obs)\n",
    "taxi_env.step(failure_action)\n",
    "\n",
    "# render the environment\n",
    "taxi_env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "583ab2a2-3e58-4e54-aa03-a130c790c619",
   "metadata": {
    "tags": []
   },
   "source": [
    "As we can see, the taxi should go North on its way to location B to pick up the passenger. However, our classifier policy chooses to go East, thus crashing into the wall. Worse yet, as long as the episode is live, the state will remain the same and our policy will choose the same action over and over. In a real-world situation, we could not afford to repeatedly crash a car into the wall. Obviously, our expert can solve this problem with ease, but our clasifier is unable to generalize. This kind of failure is caused by two phenomena, discussed below.\n",
    "\n",
    "### Distributional shift\n",
    "\n",
    "<img src=\"https://futureoflife.org/wp-content/uploads/2019/06/distributional-shift.png\"/>\n",
    "\n",
    "In the classification problem, we assume our finite sample set $S\\subset \\mathcal{X}\\times \\mathcal{Y}$ was sampled I.I.D. from some distribution $D$. In reality, the sample set is a collection of trajectories sampled from distribution $D_{\\pi_{\\text{expert}}}$ that is dependent on the expert policy. When deploying our algorithm, we sample data from $D_{\\pi}$ that is dependant on our policy $\\pi$. However, unless  $\\pi_{\\text{expert}} = \\pi$, then $D_{\\pi_{\\text{expert}}} \\neq D_{\\pi}$, and so incoming data is sampled from outside the expected distribution.\n",
    "\n",
    "When the agent observes a previously unseen state, it may act differently from the expert. When this happens, the next observation is sampled from $D_\\pi$ and not $D_{\\pi_{\\text{expert}}}$, on which our agent is even more likely to make a mistake. This issue compounds as the distribution of the samples \"shifts\" from $D_{\\pi_{\\text{expert}}}$ to $D_\\pi$.\n",
    "\n",
    "Distributional shift is hard to demonstrate on the single taxi domain due to its simplicity. In the above example, the taxi begins from an unseen state (we know it is unseen because the training accuracy is 1). Since the policy's failure leaves the state unchanged, the distribution has nowhere to shift.\n",
    "\n",
    "One interesting solution to distributional shift is [DAGGER](https://www.cs.cmu.edu/~sross1/publications/Ross-AIStats11-NoRegret.pdf) (Dataset Aggregation). In DAGGER, the aim is to try to converge $\\pi_{\\text{expert}}$ to $\\pi$ via an iterative algorithm. The working assumption is that if $\\pi_{\\text{expert}} \\sim \\pi$ then $D_{\\pi_{\\text{expert}}} \\sim D_{\\pi}$. The idea is to perform behavior cloning, deploy the policy to collect more data, and then have an expert annotate the observations with the correct action. The new data is added to the old data and the process starts over. The most glaring issue with this technique is the need for a human annotator, which can be very expensive or simply unsafe. This kind of imitation learning is called policy aggregation.\n",
    "\n",
    "\n",
    "### The Markov assumption\n",
    "\n",
    "We previously noticed that in the above example, our taxi continuously chooses the East action, casuing it to hit the wall and remain in place. Why does the agent not realize this action is not helping it advance toward the goal? This is due to the ***Markovian assumption***. Under it, the next state is determined only by the current state and action, regardless of any previous states visited or actions taken. In other words, our MDP model does account for any memory of the past. Specifically, the agent has no recollection of hitting the wall, and so it has no access to information that could hint to East being a bad action.\n",
    "\n",
    "Remember that after training, neural networks are nothing more than functions. In our case, we have a deterministic model, i.e., for any observation $s$, $\\pi(s)$ will always yield the same output. Since our model parameters will no longer change, we cannot hope to surpass this problem state. To break the Markov assumption, it is customary to use recurrent neural networks which have a built-in memory mechanism in the form of hidden states."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f175852a-f29b-430a-97b8-6565bc3a503f",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "IL, and specifically BC, is a simple solution to solving basic RL problems. Although it is not perfect, it can achieve some impressive results in expectation. However, we notice two massive issues that manage to cripple even as simple a domain as single-taxi.\n",
    "\n",
    "Our learner is unable to generalize to unseen states and so one mistake can lead to complete failure. A straight forward solution is to collect more data. Due to the small number of states, we will most likely collect optimal actions for all possible states if we collect enough trajectories. Our deep classifier is able to completely fit the training data, and so in this case our classifier correctly chooses the optimal action (as would the expert) at every state.\n",
    "\n",
    "The above solution is only relevant for a simple domain such as this, and is not feasible in much larger state spaces or continuous spaces, in which our algorithms must be able to generalize to unseen observations if they are to be useful. If that is the case, then a planning algorithm such as A* is far more practical than BC or any other learning algorithm.\n",
    "\n",
    "There are other forms of imitation learning (see [this blog bost](https://smartlabai.medium.com/a-brief-overview-of-imitation-learning-8a8a75c44a9c)) besides BC that were not discussed in this presentation. These techniques are aimed at solving issues in BC."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0ac9c3e-8ed2-4274-bfb1-dc5e68d561fe",
   "metadata": {},
   "source": [
    "# Continuous Space Imitation Learning\n",
    "\n",
    "We now present imitation learning in the case of continuous observation and action spaces. For this we use the [MPE Simple environment](https://www.pettingzoo.ml/mpe/simple), which is a 2d navigator environment where a single agent must reach a goal position in a continuous world.\n",
    "\n",
    "There are two key differences between this example and the taxi example:\n",
    "1. We cannot use A* as the expert due to the infinite state and action spaces.\n",
    "2. Since the actions are continuous, we want to perform regression (not classification)\n",
    "\n",
    "## Expert\n",
    "\n",
    "To address the expert issue, we implement a navigation trajectory planning policy for this environment in which there are no obstacles into which the agent can collide, meaning we simply need the fastest route to the destination. Note that the shortest path might not be the fastest! Since the force cap is per axis and not a global magnitude of the force (i.e., the norm), then we can solve the problem for both axes separately and simultaneously. Our expert policy is defined as follows:\n",
    "* for each axis:\n",
    "    * if the agent is at the target (on the current axis)\n",
    "        * if the agent can stop, then stop.\n",
    "        * otherwise, full break in the opposite direction of the axis velocity.\n",
    "    * if the agent can reach the target in the next step and then stop, apply the force required to reach the target.\n",
    "    * if the agent can accelerate with maximal force and is still able to stop without overshooting the target, apply full force toward the target.\n",
    "    * otherwise, apply full force away from the target (i.e., break)\n",
    "* apply the force chosen force on each axis\n",
    "\n",
    "To implement this policy, we wrote a physics utility that implements useful tools for handling the environment's world physics. This can be found under `common.mpe_physics.PhysicsUtils`.\n",
    "\n",
    "A big thanks to [**Adi Amuzig**](https://www.linkedin.com/in/adiamuzig/) for formulating and solving the optimization problem on which this algorithm is based."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "41608827-f401-4db4-b1a0-c15f9c2dda8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NavigatorMaxAccelerationPolicy:\n",
    "    def __init__(self, env):\n",
    "        # physics utility\n",
    "        self.__utils = PhysicsUtils(env)\n",
    "\n",
    "    def __call__(self, obs):\n",
    "        # extract axis velocity and target information\n",
    "        v_target_cutoff = len(obs) // 2\n",
    "        obs_v, obs_target = obs[:v_target_cutoff], obs[v_target_cutoff:]\n",
    "\n",
    "        # find optimal force for each axis\n",
    "        force_per_axix = [self.policy_1d(v, x) for v, x in zip(obs_v, obs_target)]\n",
    "\n",
    "        # set the force as a valid action\n",
    "        return np.concatenate([[0]] + [self.__force_to_action(f) for f in force_per_axix],\n",
    "                              dtype=np.float32)\n",
    "\n",
    "    def policy_1d(self, v, x_target):\n",
    "        # currently on target\n",
    "        if x_target == 0:\n",
    "            if self.__utils.can_stop(v):  # can stop, then stop.\n",
    "                return self.__utils.force_to_stop(v)\n",
    "            else:  # can't stop, then break as hard as possible.\n",
    "                return self.__utils.max_force_in_dir(-v)\n",
    "\n",
    "        # target is reachable in one move and then stop is possible in the next, then do it.\n",
    "        if self.__utils.two_moves_from_target(0, v, x_target):\n",
    "            return self.__utils.force_to_target(0, v, x_target)\n",
    "\n",
    "        # if full acceleration will not cause us to overshoot the target, accelerate.\n",
    "        if self.__utils.can_full_accelerate(v, x_target):\n",
    "            return self.__utils.max_force_in_dir(x_target)\n",
    "\n",
    "        # push away from the target hoping that the agent won't overshoot.\n",
    "        return self.__utils.max_force_in_dir(-x_target)\n",
    "\n",
    "    def __force_to_action(self, f):\n",
    "        # force applied as a separate value for positive and negative force.\n",
    "        mag = abs(f)\n",
    "        if f > 0:\n",
    "            return np.array([mag, 0])\n",
    "        else:\n",
    "            return np.array([0, mag])\n",
    "\n",
    "mpe_simple_expert = NavigatorMaxAccelerationPolicy(mpe_simple_env)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e99bd5c-d769-40cc-81fc-3e570f12ad9c",
   "metadata": {},
   "source": [
    "Thanks to the environment wrapper, all of our previous functions are compatible with all of our previous functions. Let us see the policy in action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a55125a8-69f5-48d4-80d8-7eccf2370083",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num episodes completed:   100\n",
      "total rewards:            -397.80526693972695\n",
      "mean rewards per episode: -3.98\n"
     ]
    }
   ],
   "source": [
    "# This code can be terminated early with an interruption\n",
    "animate_policy(mpe_simple_env, mpe_simple_expert, episode_limit=100, sleep=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f447e0-008c-4164-8ad7-48a7ed56d003",
   "metadata": {},
   "source": [
    "## Data Collection\n",
    "\n",
    "We can also collect the data and package it exactly as we did for the taxi environment. Since the state space in this environment is larger, we will collect more trajectories. Keep in mind that each trajectories in this environment are far longer than those in the taxi environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ea8f710a-9318-4b72-bae5-d87661630b18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0580dcdf10bc46f7b8ca844e6529a272",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "457f1111e755412dbe52f5c644d9ac85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6036b668a6ce4975a21acb80bbc8a64e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mpe_simple_env.seed(SEED)\n",
    "mpe_simple_raw_train_data = collect_data(mpe_simple_env, mpe_simple_expert,\n",
    "                                         num_trajectories=1_000)\n",
    "mpe_simple_raw_val_data = collect_data(mpe_simple_env, mpe_simple_expert,\n",
    "                                       num_trajectories=200)\n",
    "mpe_simple_raw_test_data = collect_data(mpe_simple_env, mpe_simple_expert,\n",
    "                                        num_trajectories=200)\n",
    "\n",
    "mpe_simple_ds_train = ImitationLearningDataset(mpe_simple_raw_train_data,\n",
    "                                               prep_obs=torch.from_numpy,\n",
    "                                               prep_action=torch.from_numpy)\n",
    "mpe_simple_ds_val = ImitationLearningDataset(mpe_simple_raw_val_data,\n",
    "                                             prep_obs=torch.from_numpy,\n",
    "                                             prep_action=torch.from_numpy)\n",
    "mpe_simple_ds_test = ImitationLearningDataset(mpe_simple_raw_test_data,\n",
    "                                              prep_obs=torch.from_numpy,\n",
    "                                              prep_action=torch.from_numpy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d046b8e-bab3-472e-999d-b95c11d0524e",
   "metadata": {},
   "source": [
    "## Learning Model\n",
    "\n",
    "This kind of environment where all the observations are continuous is perfect for MLP models. They tend to fit quickly and generalize well on such data, and so we will use a similar learning model to the one used in the taxi example.\n",
    "\n",
    "However, we must remember that this is a multi-value regression problem. In regression, the target set $\\mathcal{Y}$ is now a subset of $\\mathbb{R}$ instead of a finite set of labels. In multi-value regression, we have $\\mathcal{Y}\\subseteq\\mathbb{R}^n$ where $n$ is the number of real numbers we must estimate. In the case of our environment configuration, the values are between 0 and 1. To enforce this on our network's output, we add a [sigmoid activation](https://en.wikipedia.org/wiki/Sigmoid_function) (A.K.A. \"expit\" or \"inverse logit\") to the output layer. Note that for a different limit, we can just multiply the sigmoid output by this constant value and achieve the desired output constraint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6af76cec-5178-4def-87c5-ba5632ddc0bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLP(\n",
       "  (fc_layers): Sequential(\n",
       "    (0): Linear(in_features=4, out_features=32, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=32, out_features=64, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=64, out_features=128, bias=True)\n",
       "    (5): ReLU()\n",
       "    (6): Linear(in_features=128, out_features=5, bias=True)\n",
       "    (7): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the input vector length from a training example\n",
    "in_features = len(mpe_simple_ds_train[0][0])\n",
    "\n",
    "# the output vector should be the size of an action vector\n",
    "action_size = mpe_simple_env.action_space.shape[0]\n",
    "\n",
    "# seed to always get the same model parameter initialization\n",
    "torch.manual_seed(SEED)\n",
    "mlp_mpe_simple = MLP(in_features=in_features, hidden_dims=[32, 64, 128],\n",
    "                     out_features=action_size, output_activation=torch.nn.Sigmoid())\n",
    "mlp_mpe_simple"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edbe62e2-6bfe-40d8-a98b-fa7a5cceb43c",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "To train the MLP network, we optimize the [mean absolute error](https://en.wikipedia.org/wiki/Mean_absolute_error) (MAE) loss, implmented in `torch.nn.L1Loss` and defined:\n",
    "$$MSE(h, (X, Y)) = \\frac{1}{|X|}\\sum_{i=1}^m\\sum_{j=1}^n(h(x_i)_j - y_{i,j})^2$$\n",
    "where $h$ is the regression function, $h(x_i)_j$ is the $j$'th output of the regressor for input $x_i$, and $y_{i,j}$ is the $j$'th value of the $i$'th target vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1ad6d184-f155-4740-a8d3-4838ce6fb678",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 10:\n",
      "avg training loss       = 0.016590159201799223\n",
      "avg validation loss     = 0.014842526567506658\n",
      "timestamp: 02:07:04.523622\n",
      "====================================================\n",
      "\n",
      "epoch 20:\n",
      "avg training loss       = 0.010222604201646293\n",
      "avg validation loss     = 0.009660319408852678\n",
      "timestamp: 02:07:10.471345\n",
      "====================================================\n",
      "\n",
      "epoch 30:\n",
      "avg training loss       = 0.009623062330112933\n",
      "avg validation loss     = 0.009371708475155636\n",
      "timestamp: 02:07:17.010429\n",
      "====================================================\n",
      "\n",
      "epoch 40:\n",
      "avg training loss       = 0.00925452368395865\n",
      "avg validation loss     = 0.009017583737624395\n",
      "timestamp: 02:07:23.140492\n",
      "====================================================\n",
      "\n",
      "epoch 50:\n",
      "avg training loss       = 0.009020705469460447\n",
      "avg validation loss     = 0.008961514181273568\n",
      "timestamp: 02:07:29.245180\n",
      "====================================================\n",
      "\n",
      "epoch 60:\n",
      "avg training loss       = 0.008826102936220423\n",
      "avg validation loss     = 0.008449703148824969\n",
      "timestamp: 02:07:36.934545\n",
      "====================================================\n",
      "\n",
      "epoch 70:\n",
      "avg training loss       = 0.00870250749987993\n",
      "avg validation loss     = 0.008665047660206777\n",
      "timestamp: 02:07:46.311057\n",
      "====================================================\n",
      "\n",
      "epoch 80:\n",
      "avg training loss       = 0.008476666845203361\n",
      "avg validation loss     = 0.008276420084379205\n",
      "timestamp: 02:07:59.353444\n",
      "====================================================\n",
      "\n",
      "epoch 90:\n",
      "avg training loss       = 0.008376610627633256\n",
      "avg validation loss     = 0.008671531351031082\n",
      "timestamp: 02:08:15.189616\n",
      "====================================================\n",
      "\n",
      "epoch 100:\n",
      "avg training loss       = 0.008235574455347026\n",
      "avg validation loss     = 0.009366244790262905\n",
      "timestamp: 02:08:29.809195\n",
      "====================================================\n",
      "\n",
      "epoch 110:\n",
      "avg training loss       = 0.008054427607581936\n",
      "avg validation loss     = 0.008181711829843587\n",
      "timestamp: 02:08:43.729023\n",
      "====================================================\n",
      "\n",
      "epoch 120:\n",
      "avg training loss       = 0.007933081238294023\n",
      "avg validation loss     = 0.007986686059909985\n",
      "timestamp: 02:08:56.661493\n",
      "====================================================\n",
      "\n",
      "epoch 130:\n",
      "avg training loss       = 0.007780717240180225\n",
      "avg validation loss     = 0.007840662654567137\n",
      "timestamp: 02:09:10.741630\n",
      "====================================================\n",
      "\n",
      "epoch 140:\n",
      "avg training loss       = 0.007602835779127878\n",
      "avg validation loss     = 0.008027205424771457\n",
      "timestamp: 02:09:22.585163\n",
      "====================================================\n",
      "\n",
      "epoch 150:\n",
      "avg training loss       = 0.007475710838962117\n",
      "avg validation loss     = 0.007330485595505874\n",
      "timestamp: 02:09:34.841304\n",
      "====================================================\n",
      "\n",
      "epoch 160:\n",
      "avg training loss       = 0.0072658611957811215\n",
      "avg validation loss     = 0.007276999447895154\n",
      "timestamp: 02:09:48.211706\n",
      "====================================================\n",
      "\n",
      "epoch 170:\n",
      "avg training loss       = 0.007097346050085832\n",
      "avg validation loss     = 0.008042779889504224\n",
      "timestamp: 02:10:01.399695\n",
      "====================================================\n",
      "\n",
      "epoch 180:\n",
      "avg training loss       = 0.006843279786099789\n",
      "avg validation loss     = 0.007146211088728456\n",
      "timestamp: 02:10:13.547648\n",
      "====================================================\n",
      "\n",
      "epoch 190:\n",
      "avg training loss       = 0.0066284736030331045\n",
      "avg validation loss     = 0.006433204084465719\n",
      "timestamp: 02:10:23.545395\n",
      "====================================================\n",
      "\n",
      "epoch 200:\n",
      "avg training loss       = 0.0064572287499225795\n",
      "avg validation loss     = 0.0063085671465889885\n",
      "timestamp: 02:10:30.252433\n",
      "====================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_losses, val_losses = train_torch_model_sgd(model=mlp_mpe_simple,\n",
    "                                                 ds_train=mpe_simple_ds_train,\n",
    "                                                 ds_val=mpe_simple_ds_val,\n",
    "                                                 loss_fn=torch.nn.MSELoss(),\n",
    "                                                 batch_size=32,\n",
    "                                                 shuffle_data=True,\n",
    "                                                 num_epochs=200,\n",
    "                                                 learning_rate=1e-1,\n",
    "                                                 weight_decay=0,\n",
    "                                                 print_every=10,\n",
    "                                                 include_accs=False,\n",
    "                                                 seed=SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f03a01-1b93-4a3b-bffc-581a1b607dc2",
   "metadata": {},
   "source": [
    "We can see a smoothe training curve indicating stable training and great generalization capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bf967f9a-5b52-408e-a16d-1f6850736b6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAxS0lEQVR4nO3deZhcVZ3/8fe31t6zdDpJp7NDgCyEJMaAogiCSkBWUYKgggsPKAPo8BtxG2V+4zw64zCIMvBDBUWRRRZBZVNkVYIkQEJCgIQspLN0urN0d3rvqu/vj3s7VJrqTmeprib9eT1PPbn33O1btyv1rXPOveeauyMiItJdJN8BiIjIwKQEISIiWSlBiIhIVkoQIiKSlRKEiIhkpQQhIiJZKUGIyD4xsyfN7Iv5jkNyRwlC8sbM1ppZu5mN6Fb+spm5mU0M58ea2b1mVmdm9Wb2ipldGC6bGK67s9vr3B6OeVB+qZnZL8NzmXkOluQ7Lnl3i+U7ABn01gDnAT8BMLMjgcJu6/waWAJMANqAI4HR3dYZ6u6duQ11YDCzWA/v9T/d/dv9HpActFSDkHz7NfDZjPnPAbd1W+e9wC/dvcndO939JXd/+EAGYWYRM/u2ma0zsy1mdpuZDQmXFZjZb8xsq5ntMLMXzGxUuOxCM1ttZo1mtsbMzu9h/98zs3vM7K5w3RfN7KiM5WPCWlJtuJ/Ls2z7GzNrAC7cy/fWVcu62Mw2mtkmM/vnjOVJM7suXLYxnE5mLD8jrNU1mNmbZnZyxu4nmNnfwvf0WPfaoLy7KUFIvi0EysxsqplFgXOB32RZ5wYzW2Bm43MUx4Xh6wRgMlAC/DRc9jlgCDAOKAcuAVrMrBi4Hpjv7qXA+4GXeznGGcDvgOHAb4Hfm1nczCLAHwhqSVXAicCVZvaxbtveAwwFbt/H93gCMAX4KHC1mZ0Uln8LOAaYBRwFzAO+DWBm8wgS9v8Jj30csDZjn58GLgJGAgngqn2MTQYgJQgZCLpqER8BXgM2dFv+SeAZ4DvAmvDX7Hu7rVMX/rrvek3dyxjOB65199XuvhP4BrDAzGJAB0FiONTdU+6+2N0bwu3SwAwzK3T3Te6+vJdjLHb3e9y9A7gWKCD4Yn4vUOHu/+bu7e6+GvgZsCBj2+fc/ffunnb3lh72f1W3c/CrbsuvCWthrwC3EjTtdb33f3P3Le5eC1wDfCZc9gXgFnf/c3jsDe7+WsY+b3X3N8KY7iZIMnKQUIKQgeDXBL9EL+SdzUu4+3Z3v9rdpwOjCH6l/97MLGO1Ee4+NOO1Yi9jGAOsy5hfR9BHNyqM71HgzrAJ5j/NLO7uTQQ1nkuATWb2JzM7opdjrM94T2mgOjzuBGBM5pc78M3w2O/Ythc/6nYOPtfT8cP3N6aX9961bBzwZi/H3Jwx3UxQ85KDhBKE5J27ryPorD4FuG8P69YBPyL4Aht+AMPYSPBF3WU80AnUuHuHu1/j7tMImpE+Tthv4u6PuvtHgEqC2s/PejnGuK6JsFlpbHjc9cCabl/upe5+Ssa2B2LY5XEZ0+PDY0P29961bD1wyAE4trwLKUHIQPEF4MPhr/LdmNkPzWyGmcXMrBS4FFjl7lv38VixsOO56xUH7gC+amaTzKwE+A/gLnfvNLMTzOzIsI+kgaDJKWVmo8zs9LAvog3YCaR6Oe57zOzssNnqynCbhcA/gAYz+7qZFZpZNHy/3ZvR9td3zKzIzKYT9BvcFZbfAXzbzCrCTuZ/5e1+oF8AF5nZiWFHftUeaklyEFGCkAHB3d9090U9LC4C7gd2AKsJfu2e3m2dHd3uAfhaL4e7EWjJeN0K3ELQlPQ0QW2mFfincP3RBB3EDcAK4CmCL9AI8M8Ev7a3AR8CvtzLcR8gaJLaTtDGf3ZYO0kBpxG0368B6oCfE3SM741/6XYO6rotfwpYBTxO0Bz1WFj+78AiYCnwCvBiWIa7/4MgmfwPUB/uYwIyKJgeGCSSe2b2PYJO7gvycOyJBIknPljuFZEDQzUIERHJSglCRESyUhOTiIhkpRqEiIhkdVAN1jdixAifOHFivsMQEXnXWLx4cZ27V2RbdlAliIkTJ7JoUU9XSoqISHdmtq6nZWpiEhGRrJQgREQkKyUIERHJ6qDqgxCRg0dHRwfV1dW0trbmO5SDQkFBAWPHjiUej/d5GyUIERmQqqurKS0tZeLEiew+srvsLXdn69atVFdXM2nSpD5vpyYmERmQWltbKS8vV3I4AMyM8vLyva6NKUGIyICl5HDg7Mu5VIIArn98JU+9UZvvMEREBhQlCOCmp97kGSUIEcmwY8cO/vd//3evtzvllFPYsWPHgQ8oD5QggGQsQnsqne8wRGQA6SlBpFK9PTQQHnroIYYOHZqjqPqXrmICkrEobR1KECLytquvvpo333yTWbNmEY/HKSkpobKykpdffplXX32VM888k/Xr19Pa2soVV1zBxRdfDLw95M/OnTuZP38+H/jAB/j73/9OVVUVDzzwAIWFhXl+Z32nBAEk4xHaOnv/VSAi+XPNH5bz6saGA7rPaWPK+O5p03tc/oMf/IBly5bx8ssv8+STT3LqqaeybNmyXZeJ3nLLLQwfPpyWlhbe+9738olPfILy8vLd9rFy5UruuOMOfvazn/GpT32Ke++9lwsu6PeHCu4zJQiCJqa2TtUgRKRn8+bN2+0eguuvv577778fgPXr17Ny5cp3JIhJkyYxa9YsAN7znvewdu3a/gr3gFCCIGxiUoIQGbB6+6XfX4qLi3dNP/nkk/zlL3/hueeeo6ioiOOPPz7rPQbJZHLXdDQapaWlpV9iPVDUSQ0kYmpiEpHdlZaW0tjYmHVZfX09w4YNo6ioiNdee42FCxf2c3T9QzUIwiYmdVKLSIby8nKOPfZYZsyYQWFhIaNGjdq17OSTT+amm25i5syZHH744RxzzDF5jDR3lCAIEsTOts58hyEiA8xvf/vbrOXJZJKHH34467KufoYRI0awbNmyXeVXXXXVAY8v19TEhC5zFRHJRgkCXeYqIpKNEgS6zFVEJBslCHSZq4hINkoQdF3FpCYmEZFMOU0QZnaymb1uZqvM7Oosy48ws+fMrM3MrtqbbQ+khJqYRETeIWcJwsyiwA3AfGAacJ6ZTeu22jbgcuBH+7DtAZOMRelMO6m05+oQInKQKykpAWDjxo2cc845Wdc5/vjjWbRoUa/7ue6662hubt41n8/hw3NZg5gHrHL31e7eDtwJnJG5grtvcfcXgI693fZASsaD09CuWoSI7KcxY8Zwzz337PP23RNEPocPz2WCqALWZ8xXh2UHdFszu9jMFpnZotrafXvoTzIWnAZd6ioiXb7+9a/v9jyI733ve1xzzTWceOKJzJkzhyOPPJIHHnjgHdutXbuWGTNmANDS0sKCBQuYOXMm55577m5jMV166aXMnTuX6dOn893vfhcIBgDcuHEjJ5xwAieccAIQDB9eV1cHwLXXXsuMGTOYMWMG11133a7jTZ06lS996UtMnz6dj370owdszKdc3kmd7QGofW3D6fO27n4zcDPA3Llz96mNKBmLAqgfQmSgevhq2PzKgd3n6CNh/g96XLxgwQKuvPJKvvzlLwNw991388gjj/DVr36VsrIy6urqOOaYYzj99NN7fN7zjTfeSFFREUuXLmXp0qXMmTNn17Lvf//7DB8+nFQqxYknnsjSpUu5/PLLufbaa3niiScYMWLEbvtavHgxt956K88//zzuztFHH82HPvQhhg0blrNhxXNZg6gGxmXMjwU29sO2e21XDUJ3U4tIaPbs2WzZsoWNGzeyZMkShg0bRmVlJd/85jeZOXMmJ510Ehs2bKCmpqbHfTz99NO7vqhnzpzJzJkzdy27++67mTNnDrNnz2b58uW8+uqrvcbz7LPPctZZZ1FcXExJSQlnn302zzzzDJC7YcVzWYN4AZhiZpOADcAC4NP9sO1e6+qDUBOTyADVyy/9XDrnnHO455572Lx5MwsWLOD222+ntraWxYsXE4/HmThxYtZhvjNlq12sWbOGH/3oR7zwwgsMGzaMCy+8cI/7ce+5gSRXw4rnrAbh7p3AZcCjwArgbndfbmaXmNklAGY22syqga8B3zazajMr62nbXMWaiHYlCNUgRORtCxYs4M477+See+7hnHPOob6+npEjRxKPx3niiSdYt25dr9sfd9xx3H777QAsW7aMpUuXAtDQ0EBxcTFDhgyhpqZmt4H/ehpm/LjjjuP3v/89zc3NNDU1cf/99/PBD37wAL7bd8rpaK7u/hDwULeymzKmNxM0H/Vp21xJxtUHISLvNH36dBobG6mqqqKyspLzzz+f0047jblz5zJr1iyOOOKIXre/9NJLueiii5g5cyazZs1i3rx5ABx11FHMnj2b6dOnM3nyZI499thd21x88cXMnz+fyspKnnjiiV3lc+bM4cILL9y1jy9+8YvMnj07p0+ps96qLe82c+fO9T1dY5zNwtVbWXDzQn77paN5/yEj9ryBiOTcihUrmDp1ar7DOKhkO6dmttjd52ZbX0NtkHmZq2oQIiJdlCDIuMxVVzGJiOyiBIGuYhIZqA6mJvB825dzqQSBmphEBqKCggK2bt2qJHEAuDtbt26loKBgr7bTM6nRndQiA9HYsWOprq5mX4fQkd0VFBQwdmzWi0Z7pARBMNw3aLA+kYEkHo8zadKkfIcxqKmJCQ3WJyKSjRIEGotJRCQbJQiCsVL0VDkRkd0pQYSSsYiamEREMihBhJKxqGoQIiIZlCBCyVhEfRAiIhmUIELJeIT2lBKEiEgXJQiAnbUMs2baOtQHISLSRQkC4H+m8+mOe9UHISKSQQkCIFFMibXqKiYRkQxKEACJEopoVQ1CRCSDEgRAoohC2nQVk4hIBiUIgERxWINQE5OISBclCIBEMQXeqstcRUQyKEEAJEoo8BY1MYmIZFCCAEgUk3R1UouIZFKCAIgXkUy3qA9CRCSDEgRAooREqoW2zrSefysiElKCAEgUE0+34O50pJQgRERACSKQKMZwCmhXM5OISEgJAiBRDEAxrbRowD4REUAJIhAmiCJrpaVdCUJEBHKcIMzsZDN73cxWmdnVWZabmV0fLl9qZnMyln3VzJab2TIzu8PMCnIW6K4aRBtNbUoQIiKQwwRhZlHgBmA+MA04z8ymdVttPjAlfF0M3BhuWwVcDsx19xlAFFiQq1h31SBopbm9M2eHERF5N8llDWIesMrdV7t7O3AncEa3dc4AbvPAQmComVWGy2JAoZnFgCJgY84iTZQAUGRtNKuJSUQEyG2CqALWZ8xXh2V7XMfdNwA/At4CNgH17v5YziLN6KRWDUJEJJDLBGFZyrrfZJB1HTMbRlC7mASMAYrN7IKsBzG72MwWmdmi2trafYs0XgQETUzqgxARCeQyQVQD4zLmx/LOZqKe1jkJWOPute7eAdwHvD/bQdz9Znef6+5zKyoq9i3SsImp2FSDEBHpkssE8QIwxcwmmVmCoJP5wW7rPAh8Nrya6RiCpqRNBE1Lx5hZkZkZcCKwImeRhk1MhbTRpD4IEREg6AjOCXfvNLPLgEcJrkK6xd2Xm9kl4fKbgIeAU4BVQDNwUbjseTO7B3gR6AReAm7OVaxdTUxBDUIJQkQEcpggANz9IYIkkFl2U8a0A1/pYdvvAt/NZXy7RCIQL2KIt7OhTU1MIiKgO6nfliimLNquJiYRkZASRJdEMaXWpk5qEZGQEkSXRAklEQ21ISLSRQmiS6KYYmujpUM1CBERUIJ4W7yIYt0oJyKyixJEl0QxhRpqQ0RkFyWILokSClw1CBGRLkoQXRLFJL1FT5QTEQnl9Ea5d5VEMcl0C01qYhIRAVSDeFuimHi6jY7OTjpT6XxHIyKSd0oQXTKfKqdmJhERJYhdup4qRxvN6qgWEVGC2CVZCkCpNetSVxERlCDeliwDoJQWDfktIoISxNvCGkSJtdCkIb9FRJQgdulKEKpBiIgAShBvy+iD0L0QIiJ9SBBmdqyZFYfTF5jZtWY2Ifeh9TPVIEREdtOXGsSNQLOZHQX8C7AOuC2nUeVDZoJQH4SISJ8SRGf47OgzgB+7+4+B0tyGlQfROB4vCjqpVYMQEelTgmg0s28AFwB/MrMoEM9tWHmSLKXUWnQfhIgIfUsQ5wJtwBfcfTNQBfxXTqPKE0uWMjTSqj4IERH6NpprI0HTUsrMDgOOAO7IbVh5kixlSKSVna2qQYiI9KUG8TSQNLMq4HHgIuCXuQwqb8IEUd/Ske9IRETyri8Jwty9GTgb+Im7nwVMz21YeZIso8RalCBEROhjgjCz9wHnA38Ky6K5CymPkqWUoAQhIgJ9SxBXAt8A7nf35WY2GXgip1HlS7KUQm9WghARoQ+d1O7+FPCUmZWaWYm7rwYuz31oeZAspSDdxI7m9nxHIiKSd30ZauNIM3sJWAa8amaLzewg7YMoJeop0h2ttHXqUlcRGdz60sT0/4CvufsEdx8P/DPws9yGlSddA/apH0JEpE8Jotjdd/U5uPuTQHFfdm5mJ5vZ62a2ysyuzrLczOz6cPlSM5uTsWyomd1jZq+Z2Yqwozy3wocGlVgzDUoQIjLI9eVGudVm9h3g1+H8BcCaPW0UDslxA/ARoBp4wcwedPdXM1abD0wJX0cTDAx4dLjsx8Aj7n6OmSWAoj7Eun8yBuxTDUJEBru+1CA+D1QA94WvEcCFfdhuHrDK3Ve7eztwJ8GAf5nOAG7zwEJgqJlVmlkZcBzwCwB3b3f3HX045v7Z9UyIFnY0K0GIyODWl6uYttPtqiUzu4tgjKbeVAHrM+arebt20Ns6VUAnUAvcGg4zvhi4wt2buh/EzC4GLgYYP378nt5O71SDEBHZZV+fKNeX/gDLUuZ9XCcGzAFudPfZQBPwjj4MAHe/2d3nuvvcioqKPoTVi12d1M2qQYjIoJfLR45WA+My5scCG/u4TjVQ7e7Ph+X3ECSM3NrVSa0ahIhIj01MmVcUdV9E354H8QIwxcwmARuABcCnu63zIHCZmd1J0PxU7+6bwuOvN7PD3f114ETgVXItrEGUx9vYrgQhIoNcb30Q/93Lstf2tGN37zSzy4BHCcZuuiUcquOScPlNwEPAKcAqoJlgpNgu/wTcHl7BtLrbstyIJSGaoNzaWKsEISKDXI8Jwt1P2N+du/tDBEkgs+ymjGkHvtLDti8Dc/c3hr2WLGVYSkN+i4jksg/i3aloBBWRBo3HJCKDnhJEd2WVVPg21SBEZNBTguiudAzDUlupb9FjR0VkcOsxQZjZBRnTx3Zbdlkug8qrskrKOrbS0NJK0EUiIjI49VaD+FrG9E+6Lft8DmIZGEoriZBiSKqelg4N+S0ig1dvCcJ6mM42f/AorQRglG1ju+6mFpFBrLcE4T1MZ5s/eJR1JYjt1DW25TkYEZH86e1GuSPMbClBbeGQcJpwfnLOI8uXsAYx2razuaGVo/IcjohIvvSWIKb2WxQDSfFI3CKMsm3UNLTmOxoRkbzp7U7qdZnzZlZO8IyGt9x9ca4Dy5toDEpGUbljO2vqlSBEZPDq7TLXP5rZjHC6ElhGcPXSr83syv4JLz+sdDTjYvVsVg1CRAax3jqpJ7n7snD6IuDP7n4awairB+9lrgClY6iMbFcTk4gMar0liMxrPE8kHHTP3RuBdC6DyruySkb4NjariUlEBrHeOqnXm9k/ETy8Zw7wCICZFdK350G8e5WOpjjdyI6GxnxHIiKSN73VIL4ATAcuBM519x1h+THArbkNK89KxwBQ3F7LzjaNySQig1NvVzFtAS7JUv4E8EQug8q78Ga50QSXupZUlOQ5IBGR/tfbI0cf7G1Ddz/9wIczQIQ1iFG2nZr6Vg5RghCRQai3Poj3AeuBO4DnOZjHX+qudDQQJAhd6ioig1VvCWI08BHgPODTwJ+AO9x9eX8EllcFQ/B4EaM7tylBiMig1WMntbun3P0Rd/8cQcf0KuDJ8Mqmg5sZVlrJ2NgOanSpq4gMUr3VIDCzJHAqQS1iInA9cF/uwxoASiupqlcNQkQGr946qX8FzAAeBq7JuKt6cCirZBSrqGnQkN8iMjj1VoP4DNAEHAZcbrarj9oAd/eyHMeWX6WVDEtvo6a+Jd+RiIjkRW/3QfR2E93Br7SSuLfTvnMrqbQTjQyei7hERKD3O6kHt/BmuQrfxtadamYSkcFHCaIn4c1yo00d1SIyOClB9CS8WW6kbdeoriIyKClB9KTr2dTouRAiMjgpQfQklsCLRoQPDlIfhIgMPjlNEGZ2spm9bmarzOzqLMvNzK4Ply81szndlkfN7CUz+2Mu4+yJlVUyLrZDfRAiMijlLEGYWRS4AZgPTAPOM7Np3VabD0wJXxcDN3ZbfgWwIlcx7lFpJZXRHWpiEpFBKZc1iHnAKndf7e7twJ3AGd3WOQO4zQMLgaFmVglgZmMJhvn4eQ5j7F1pJRW+VZ3UIjIo5TJBVBEMF96lOizr6zrXAf9CPp9/XTaGstQO6hp25i0EEZF8yWWCyHbrsfdlHTP7OLDF3Rfv8SBmF5vZIjNbVFtbuy9x9iy81LWwtY7mdj16VEQGl1wmiGpgXMb8WGBjH9c5FjjdzNYSNE192Mx+k+0g7n6zu89197kVFRUHKvZA5s1yamYSkUEmlwniBWCKmU0yswSwAOj+GNMHgc+GVzMdA9S7+yZ3/4a7j3X3ieF2f3X3C3IYa3bhcBujTJe6isjg0+vzIPaHu3ea2WXAo0AUuMXdl5vZJeHym4CHgFMIHkbUDFyUq3j2SenbCWJLo2oQIjK45CxBALj7QwRJILPspoxpB76yh308CTyZg/D2rKgcjyYY3anhNkRk8NGd1L0xw0pHUxVVE5OIDD5KEHtSOoaxsXrdLCcig44SxJ6UjmaUBuwTkUFICWJPysZQnq6jpkGPHhWRwUUJYk9KK0l6K02NOwj61EVEBgcliD0JL3UdnqpjR3NHnoMREek/ShB7UtZ1N/V2DfstIoOKEsSehAmi0raqo1pEBhUliD3pqkGwjS26F0JEBhEliD2JJfHiCiptq5qYRGRQUYLoAysbw/iYniwnIoOLEkRflFWFw20oQYjI4KEE0RdlVYz0rWzYoQQhIoOHEkRflI2hON3Itu3b8h2JiEi/UYLoi7LgMdnFbVtobNXNciIyOChB9MWQIEGMtm1s2KExmURkcFCC6Iuum+XYxobtShAiMjgoQfRF6dt3U6sGISKDhRJEX8QL8KJyqqLbVYMQkUFDCaKPrKyKSfHtVKsGISKDhBJEXw2byHirUQ1CRAYNJYi+Gj6ZkanNbNq+M9+RiIj0CyWIvho+mZh3Em/aRGtHKt/RiIjknBJEXw2fDMAE28ymeg25ISIHPyWIvio/BICJVsNb25rzHIyISO4pQfRVyWg8VshE28zKmsZ8RyMiknNKEH0ViWDDJzEltoU3lCBEZBBQgtgbwydzaGwLb9ToSiYROfgpQeyN4ZMZldrMqpp63D3f0YiI5JQSxN4YPpm4t1PaXqcxmUTkoJfTBGFmJ5vZ62a2ysyuzrLczOz6cPlSM5sTlo8zsyfMbIWZLTezK3IZZ5+FVzIdGtnASjUzichBLmcJwsyiwA3AfGAacJ6ZTeu22nxgSvi6GLgxLO8E/tndpwLHAF/Jsm3/Gz0TgCNttTqqReSgl8saxDxglbuvdvd24E7gjG7rnAHc5oGFwFAzq3T3Te7+IoC7NwIrgKocxto3hUNh+CHMS6xVR7WIHPRymSCqgPUZ89W880t+j+uY2URgNvB8toOY2cVmtsjMFtXW1u5vzHtW9R5mRlazfGN97o8lIpJHuUwQlqWs+6U/va5jZiXAvcCV7t6Q7SDufrO7z3X3uRUVFfscbJ9VzWF4qo7tNW9R36LnU4vIwSuXCaIaGJcxPxbY2Nd1zCxOkBxud/f7chjn3ql6DwAz7U0Wr9uW52BERHInlwniBWCKmU0yswSwAHiw2zoPAp8Nr2Y6Bqh3901mZsAvgBXufm0OY9x7o4/EIzFmR1fz/GolCBE5eOUsQbh7J3AZ8ChBJ/Pd7r7czC4xs0vC1R4CVgOrgJ8BXw7LjwU+A3zYzF4OX6fkKta9Ei/ERk7lA4VreX6NEoSIHLxiudy5uz9EkAQyy27KmHbgK1m2e5bs/RMDw8QPMq3m56zasIWmtk6Kkzk9jSIieaE7qffFlI8Q83bey3JeWKtahIgcnJQg9sWEY/F4ER9LLOHBJd373UVEDg5KEPsilsQmfYiPxl/hkWWbaG7vzHdEIiIHnBLEvpryEYZ3bKKyYz2PLa/JdzQiIgecEsS+Onw+HolxSdGT3L1o/Z7XFxF5l1GC2FdlY7CZ53KWP85rb67h2ZV1+Y5IROSAUoLYH8deSTTdxhUlf+Hf//QqqbQeIiQiBw8liP1RcRg27Qwu8D/SVvMGNzyxKt8RiYgcMEoQ++vkHxCJF/DLIb/gx39eocteReSgoQSxv8oqsY9fy4TWV/lD6Q/58V0P8ZPHV6q5SUTe9ZQgDoQZn4Azb2RqdD0PJb/F6sd/wfwfP82flm6ivTOd7+j2TjoFKd3XISI5HotpUJn1aWzyCSTu/QL/s+5Gnt/5Dx64axZtiXU0jXkfyaM+xdGHlDN+eBHBYLXd1K2Ev10Hx38ThuzFw/NefQBefxjOvBGy7XdvPXg51K6AL/11//clIu9qFoyXd3CYO3euL1q0KL9BpDph4Q34s/+DtWyn3RIkvJ0l6cksTE/ljcghbCmbQax8EocOSXNUZDVFow/jfX/7PIWN6+gcOpnIh79JpO4NOOZSKBoOO9bDkLHvTACd7XD9bGiohs8+AJOP37/Ym+rgv4+AdAdc+hyMyv9jwEUkt8xssbvPzbZMNYgDLRqDY6/A5n4eGjaSGH4I/tKvOfz5W5lR9xhR74CdsLplIiPX1VBCCwDtHuWazs9w1fa7Kb7viwAsf+Y+XosdwSc6/siSxByeHfJxxqU3sL1kCpuGz2N2w1/5WEM1KYuz5bHrWHH8VEqScUoLYpQkY7v+jUX72JL48u1BcsBg+X27J4j6atj8Chx2cs81le1r4eU74ANXQrxwn0/hLqkOaNkOJSP3f1991bwN1j/f+/vcW+kURKIHZl8i/Ug1iP6U6oDa12DN0/Dan6CsiqZDT6V59UJ2lM+ipvIkat96jbYdG4k213H2m98mSorFRR9gWstiCr1l167aPE4bMdb5KJ5Mz+Ir0Qf439TpTLAaOomyMl1FA8WcHX2GTYzkifgHOZdHieK8EJ2FxRI0JsdQVzKFjzT9kSHewOSml9iZGEk6mqCsrYbfzruPRCxKSWo7H//HZyht2UB11XxenfWvWHE5hfEoRdEOku0N7KSIIx8+i6L6lWyZ8SXq3v+vJOMRkrEIiViERMRIxKMkohFi0QidqTTpulXEV/8Fm3z8O2sr6/8Bf7giaHr75K0w9bS+n2f34BXJkhjd4cXb4JXfQaodLrgXkqXhSW2EX54Km5bAyT8IanD766/fhyV3wOcfCWqBHa3w5+/AISfC4Sfv//4zpdPwzH8HNclx7z2w+84ld6heBGNmBz+wpF/1VoNQghjIVj0O7U0w7XRo3Bz8Qq84Aja9DG88hq/7G60f+g47Sw9hxC/mYel2WkrG4+kURc0bANhaNJmy1g3E023UR4fTHC2lsn3dbofpJEIzhZTRxFXpfyKZbuH7sZ+zOj2aYmulgxgjqOeO1If5TPTPpIjyj/ThDLOdTLFqktbJNi9hCE38Iz2VoyMr+GP6GKbbWlZ7JcOtkRm2hkfS81idruRD0aWMtq2MsbeHSn+Jw2myEt7yCkZ5LSfaYmooZ3tkKIem1/ByZAbNkVIWF76PbfFKKlMbeH/r0yTooDo+iSUlH2BLcjxT25fzibqbiNHB8xMvZW3lfGKFpRQlYqRTKY565fscvv4uthdOYEjLelaOPpWFM/+dYR2bed/Sb1O+7UUah02ndPty1h15OenEEMrqXsRwWsunkZr1GQqHVICBYZhB1IySghjx7jW12tfhxvdDuhPGzIHTfgx/+R68+TjEi+CLf4HC4VA4NKhxvbUQCodBxeH79nl58dfw4GWQKIXPPQhVc/ZtP5lqlsOIwyAaD+Y3LIY3/wpHfgqGTQjKlt0Lo4+CEYfu2zGW3g33fQk+dDWc8I39jzmXUp1Q98ZB1fyqBDEYbH0TkmVQUhHMN9VBw0YYfSQ0bgq+fA77GCSKoWlr0Hyy+ZXgP/y0M2Do+GAfFYdD6w78tjNJF40gXTgc27aGne+9jJ0TPwpbXqNk6a0kN79IW8EIGsum0JoYTkXNs2wf/xE2TTyTWQ+dTqJ1K3Xl76Fo53raY8VsKz6USTWPEku1srFkBvXFE9leNJk3hryPKTWPMLbhReKpFka0byBtUZ6r+BSPD/0k7ek0n9zyUyra1jGkfQvDU7W73nJNdDQ7bCgTO9eQpG1X+Rs+ngYvYG7kDTo9wlbKKA2b8oqsjZs6T+MHnQu4MnYvV8bu4410FeMs2O83Or7IX9JzuD3xHxwVWR0cx4fSQYyxVkeDF/FsegZxOknSQcI6iZBmWXoSG2w0w6yRMfEmCmNwSOcqxqQ2cVPyQq5quwGANMZPI+dzXvpPDKGRBJ1sjwxjZcGRzGt+mg6Lc1/FZbxW9n6mtizmqIYn2VA8ndVDj6GlYBQjm1dR2rGVdCTO6PZ1FKR3UpsYT3PJeE587bu0lIwj3rqNZPt2Now/jeqxp7I9NpLR1Y9AWRWRGWeSiCeJRoxYBCKRCLGIETUorFlEvLmWdOVRRCIRiv7+X8RfuYP0pA/hn/o1kZpXsNs/BR1NgMGxl8OwSfDHK2HIeLjk6SDBNW4OLpyYeS7ECoLPX9mY4I+zYx0MnQA73oK//l8YdzQ88f2gKbGoHL66PEiWGxbDW8/DvIt3r1Us/z28+Cs46+bgs57q7LnWkfkDqy+2rQmaF2eem715cecWuOfzsPaZoOZ56El92+8ApwQh/au9GSwC8YLdy1sbgmad4hE9b+sOns7eZp9Ow8aXoK0eiitg1IzgP3J7E6x8LEiKQ8bBoSfhFqHzzafwNU+TbqyhPVaKmUPFEfisC0jGo0Q8hT/+b7B1NR0F5WydcxnNhZW0d6aD+1ja6rG2nbQUjMaB6LbXGfvitRQ3riYdSZCKxElFEpinKG9YQSzdhmM0Rctwh9J0PXdUXMHfh5/F5LYVDOusY0dyLDXFU6hsXMaxtXeyKjaFo5r+xmEdK7g/eQYTO9fwntSSXW95MyMYyVYivPP/aYdHaSFJmTUD0OkRTm3/D5oo5Kuxezg1spAC69htm80+jJXpKkbbdibaZv6ens56r2BmZDUzI2t2P91u/CH9Pk6JPE+ENFFz3vQxfD31ZRZEH+ccewKAFZEpTEmv5kWbzgsF7+P8trsYmt7Bxtg42okxsXMNL5ccR8xSzGj8G68XHMWozg2UdW4jQpoOS/Dn8V/llHU/5MnJV9FcVMlHX/0WsXQrNeXzeGnO90mVjGVMzV856rkriHgnDZXvp2PooQxb+TtqT7yOjsNPo6BlM4lkMfHSchI1LxH91anB5+3Mm2DWeb18YIHaN+BXH4edNXD2z2HmJ3dfvu7v8LuLoLUeYsmgOeyzv+99n+8SShAiudbZFiTAwmFv/6JNdbzdNNMb96BzvLg86NBe8zRsXQXDJga/Uhs3B4mxYUNQwxs6AVLttJdU0UGcgvatdGxeQXM6zvbyWUG/TzRCQ/02iqufoahpPX7EaTRXv0L01XtJ7txAe2IoTcXjqNj8FIn2BhqLx7Om6gxqS49gaMPrpBxqiw+npnQaI7ctZnzd0+yMl7NsxMk0RoeSTqWZXXMvk3f8nZ9XfIOjmx7nEzU/IUKa6ug47kqczefabqfDErxcMI8Tm/5Imgh/SZ7Ece3P0E6Mr8a/S1GqnvYULEwdzp32LY6KvAnA8vQE7kodz7divyVOJ5sYTpVtZWl6EvemjuOa+K8AeCtdwVir27UcYIcXA1BPMRupYJ69yiYq2GmlNEZK2RkpoylSSmu0hEQkzejUZo5sXUxHJEl9bATDO2t4cNxVFNJBS8FIpu54kpmbf09DYRV/nvGfjKt9hmPW3sDDH7yPdOkYEiXDGdWwjAnLfkL9tPNpP/Rk4tGw7y0aIR7+m4hGiEQG3pOUlSBEJPfadkLTFigbC7FEkCAtGlwssH0tRGJBR33bzqCWWFC2+/Y1r5J+/WE6i0fTcsh82qPFpLa/RXLpb4jVraCh6ji2TDyd1kgx5a/dTlNyJJuHz+OIJd8n3rqVjcOPIZ3qZEjDSoY0rebRSV+nNjaaozfcRlnbJgo66ynobKAo1UBRqpHi9E46ibE9MpQlsZncHj+LjpRzS+vXKKB9V1jtHuV3qeP5Qed5NFLEUBpZmLyMKGnilmJdeiRjwuQUtxQr01VU2A4avIhNlLPFhzLGtlJOA1sYxkqbwAo7lProMJIRKIu0MNRaONxXcVhqJaXpBqI4ndFCOqMFbCuYwKrhH6SubAYl6Xombn+OkvY6mpIjWTr2PIhEGNlZw+kn79tFD0oQIiLduWfva6hbCe07IVECDRvw8kNpLw6aHiNmRCNGetGvYMNiWoqriG9aTEuinJUzvsqoN26nbOsSdhZUEu1opLB5E4VttTQmRrIzXk5R2xZGNb1BIt3yjsM2RUpYlZjKjuhw2lJARwvJdAvTfCUV7Ni1Xle/WgX1tBEnSQfbrYzy767bp0uzlSBERAaKVGdQo2quC2pVydLgVTKq5763zUuhZjnpWAF+6El4ohSvWU5k8S2ki0eRrppHcsrx2S/t3gMlCBERyaq3BKHB+kREJCslCBERyUoJQkREslKCEBGRrJQgREQkKyUIERHJSglCRESyUoIQEZGsDqob5cysFli3xxWzGwHUHcBwDhTFtfcGamyKa+8orr23L7FNcPeKbAsOqgSxP8xsUU93E+aT4tp7AzU2xbV3FNfeO9CxqYlJRESyUoIQEZGslCDednO+A+iB4tp7AzU2xbV3FNfeO6CxqQ9CRESyUg1CRESyUoIQEZGsBn2CMLOTzex1M1tlZlfnMY5xZvaEma0ws+VmdkVY/j0z22BmL4evU/IU31ozeyWMYVFYNtzM/mxmK8N/h/VzTIdnnJeXzazBzK7Mxzkzs1vMbIuZLcso6/H8mNk3ws/c62b2sTzE9l9m9pqZLTWz+81saFg+0cxaMs7dTf0cV49/u/46Zz3EdVdGTGvN7OWwvD/PV0/fEbn7nLn7oH0BUeBNYDKQAJYA0/IUSyUwJ5wuBd4ApgHfA64aAOdqLTCiW9l/AleH01cDP8zz33IzMCEf5ww4DpgDLNvT+Qn/rkuAJDAp/AxG+zm2jwKxcPqHGbFNzFwvD+cs69+uP89Ztri6Lf9v4F/zcL56+o7I2edssNcg5gGr3H21u7cDdwJn5CMQd9/k7i+G043ACqAqH7HshTOAX4XTvwLOzF8onAi86e77eif9fnH3p4Ft3Yp7Oj9nAHe6e5u7rwFWEXwW+y02d3/M3TvD2YXA2Fwdf2/i6kW/nbPe4jIzAz4F3JGLY/eml++InH3OBnuCqALWZ8xXMwC+lM1sIjAbeD4suixsCrilv5txMjjwmJktNrOLw7JR7r4Jgg8vMDJPsQEsYPf/tAPhnPV0fgba5+7zwMMZ85PM7CUze8rMPpiHeLL97QbKOfsgUOPuKzPK+v18dfuOyNnnbLAnCMtSltfrfs2sBLgXuNLdG4AbgUOAWcAmguptPhzr7nOA+cBXzOy4PMXxDmaWAE4HfhcWDZRz1pMB87kzs28BncDtYdEmYLy7zwa+BvzWzMr6MaSe/nYD5Zydx+4/RPr9fGX5juhx1Sxle3XOBnuCqAbGZcyPBTbmKRbMLE7wh7/d3e8DcPcad0+5exr4GTlsiuiNu28M/90C3B/GUWNmlWHslcCWfMRGkLRedPeaMMYBcc7o+fwMiM+dmX0O+DhwvoeN1mFzxNZwejFBu/Vh/RVTL3+7vJ8zM4sBZwN3dZX19/nK9h1BDj9ngz1BvABMMbNJ4a/QBcCD+QgkbNv8BbDC3a/NKK/MWO0sYFn3bfshtmIzK+2aJujgXEZwrj4XrvY54IH+ji2026+6gXDOQj2dnweBBWaWNLNJwBTgH/0ZmJmdDHwdON3dmzPKK8wsGk5PDmNb3Y9x9fS3y/s5A04CXnP36q6C/jxfPX1HkMvPWX/0vg/kF3AKwdUAbwLfymMcHyCo/i0FXg5fpwC/Bl4Jyx8EKvMQ22SCqyGWAMu7zhNQDjwOrAz/HZ6H2IqArcCQjLJ+P2cECWoT0EHwy+0LvZ0f4FvhZ+51YH4eYltF0D7d9Vm7KVz3E+HfeAnwInBaP8fV49+uv85ZtrjC8l8Cl3Rbtz/PV0/fETn7nGmoDRERyWqwNzGJiEgPlCBERCQrJQgREclKCUJERLJSghARkayUIET2gpmlbPcRZA/YCMDhyKD5umdD5B1i+Q5A5F2mxd1n5TsIkf6gGoTIARA+I+CHZvaP8HVoWD7BzB4PB5973MzGh+WjLHgOw5Lw9f5wV1Ez+1k43v9jZlaYtzclg54ShMjeKezWxHRuxrIGd58H/BS4Liz7KXCbu88kGBDv+rD8euApdz+K4NkDy8PyKcAN7j4d2EFwp65IXuhOapG9YGY73b0kS/la4MPuvjocUG2zu5ebWR3BcBEdYfkmdx9hZrXAWHdvy9jHRODP7j4lnP86EHf3f++HtybyDqpBiBw43sN0T+tk05YxnUL9hJJHShAiB865Gf8+F07/nWCUYIDzgWfD6ceBSwHMLNrPz1wQ6RP9OhHZO4UWPrA+9Ii7d13qmjSz5wl+eJ0Xll0O3GJm/weoBS4Ky68AbjazLxDUFC4lGEFUZMBQH4TIARD2Qcx197p8xyJyoKiJSUREslINQkREslINQkREslKCEBGRrJQgREQkKyUIERHJSglCRESy+v/yqH9PqEiTGQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot losses\n",
    "plt.title('MSE Loss per Epoch')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MSE Loss')\n",
    "plt.plot(train_losses, label='train')\n",
    "plt.plot(val_losses, label='validation')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ad2467-1f53-4f44-9404-8ee95f92aa68",
   "metadata": {},
   "source": [
    "## Policy Evaluation\n",
    "\n",
    "Our policy is identical to the classifier policy for the taxi environment, except that the outputs are the values for each entry in the action space vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7588a131-2e34-4980-aef4-cfa7677f4e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiRegressionPolicy:\n",
    "    def __init__(self, model, prep_fn=None):\n",
    "        self.model = model\n",
    "\n",
    "        # if no preprocessing function is given, use the identity function\n",
    "        if prep_fn is None:\n",
    "            self.prep_fn = lambda x: x\n",
    "        else:\n",
    "            self.prep_fn = prep_fn\n",
    "\n",
    "    def __call__(self, observation):\n",
    "        # preprocess observation\n",
    "        prepped_obs = self.prep_fn(observation)\n",
    "        one_obs_batch = prepped_obs[None]  # convert to batch of size 1\n",
    "\n",
    "        # run model to get action values\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            batch_out = self.model(one_obs_batch)\n",
    "\n",
    "        # get values for single observation\n",
    "        obs_out = batch_out[0]\n",
    "\n",
    "        # return continuous action\n",
    "        return obs_out.numpy()\n",
    "\n",
    "mpe_simple_il_policy = MultiRegressionPolicy(mlp_mpe_simple,\n",
    "                                             prep_fn=mpe_simple_ds_train.prep_obs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9809272-15fe-47f3-9729-50e54b626a34",
   "metadata": {},
   "source": [
    "Now let us compare the expert policy to our newly created deep regression policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8369653f-dbf2-4524-b7ca-dc5c6330e1c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92f8fe7ee86b4d909729cc957ddf55d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Acceleration per Axis Policy\n",
      "---------\n",
      "total reward over all episodes: -43899.756110528026\n",
      "mean reward per episode:        -4.389975611052803\n"
     ]
    }
   ],
   "source": [
    "total_reward, mean_reward = evaluate_policy(mpe_simple_env, mpe_simple_expert,\n",
    "                                            num_episodes=10_000, seed=SEED)\n",
    "print('Max Acceleration per Axis Policy')\n",
    "print('---------')\n",
    "print(f'total reward over all episodes: {total_reward}')\n",
    "print(f'mean reward per episode:        {mean_reward}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "55d8881c-8552-43aa-937d-713dec2fa2f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed8a7d95b4a545e98fa462e6f638dbbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multi-Value Regression Policy\n",
      "-----------------\n",
      "total reward over all episodes: -44620.60107545008\n",
      "mean reward per episode:        -4.462060107545009\n"
     ]
    }
   ],
   "source": [
    "total_reward, mean_reward = evaluate_policy(mpe_simple_env, mpe_simple_il_policy,\n",
    "                                            num_episodes=10_000, seed=SEED)\n",
    "print('Multi-Value Regression Policy')\n",
    "print('-----------------')\n",
    "print(f'total reward over all episodes: {total_reward}')\n",
    "print(f'mean reward per episode:        {mean_reward}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0987d86-9d7b-40bc-a5cb-0e240a5f50ab",
   "metadata": {},
   "source": [
    "As we can see our model almost matches the expert's performance (in some runs the imitation learning model got better results than the expert!). This is also clear when seeing at the policy in action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9f245f9d-b3eb-44c8-a8ac-bdd6a70b849b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num episodes completed:   100\n",
      "total rewards:            -414.6161236566214\n",
      "mean rewards per episode: -4.15\n"
     ]
    }
   ],
   "source": [
    "# This code can be terminated early with an interruption\n",
    "animate_policy(mpe_simple_env, mpe_simple_il_policy, episode_limit=100, sleep=0.017)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56cce3e3-d52a-4948-affc-3bf42180368f",
   "metadata": {},
   "source": [
    "## Extending the Problem\n",
    "\n",
    "It appears that the IL agent is able to mimic the expert almost perfectly (if not so). However, this is the simplest possible setting. It would be interesting to see how IL is affected by certain configuration changes. For example, adding just a small amount of noise could lead to severe distributional shift that can cause catastrophic failure."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
