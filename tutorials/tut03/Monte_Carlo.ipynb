{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1d75303-a4f3-4da5-b936-f26277e6839e",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Monte Carlo Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c2ecdd-9d24-4ba9-99c7-22d400e05244",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"images/Drunk.jpg\" width=\"500\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f71b50bd-cace-4fed-ae34-8afbcf3e0421",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86acaaf7-46d5-403a-99f3-63de26ccc981",
   "metadata": {},
   "source": [
    "Monte Carlo methods are ways of solving the reinforcement learning problem based on averaging sample returns.  We assume experience is divided into episodes, and that all episodes eventually terminate no matter what actions are selected. Only on the completion of an episode are value estimates and policies changed. Monte Carlo methods can thus be incremental in an episode-by-episode sense, but not in a step-by-step (online) sense. The term “Monte Carlo” is often used more broadly for any estimation method whose operation involves a significant random component."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf3763f1-c8fb-4483-b63b-03c07fa3731c",
   "metadata": {},
   "source": [
    "In the following example we use a Monte Carlo method to estimate pi: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "401312f7-ebe8-4865-8e99-1afb6a856aa8",
   "metadata": {},
   "source": [
    "![Monte Carlo Prediction](images/pi_30k.webp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac811c2-b68e-45dc-9b80-31cd10bca1eb",
   "metadata": {},
   "source": [
    "## Monte Carlo Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d9a462e-ee75-462e-8b71-5a06f600fa52",
   "metadata": {},
   "source": [
    "Monte Carlo methods can be used for learning the state-value\n",
    "function for a *given* policy. The value of a state is the expected\n",
    "return — expected cumulative future (discounted) reward — starting from that\n",
    "state. An obvious way to estimate it from experience, is simply to\n",
    "average the returns observed after visits to that state. As more returns are\n",
    "observed, the average should converge to the expected value.    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "502f7bc6-70aa-4092-be09-67fde5c021d6",
   "metadata": {},
   "source": [
    "![Monte Carlo Prediction](images/mc-pred.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9297b5f4-267f-41b4-87d0-ff2f53242aaa",
   "metadata": {},
   "source": [
    "## Monte Carlo Control"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5678a7a2-fe56-404c-ba42-262e108f9b3b",
   "metadata": {},
   "source": [
    "Monte Carlo estimation can be used in control, that is, to approximate optimal policies. The overall idea is to proceed according to the idea of generalized policy iteration (GPI). In GPI one maintains both an\n",
    "approximate policy and an approximate value function. The value function is\n",
    "repeatedly altered to more closely approximate the value function for the current policy, and the policy is repeatedly improved with respect to the current value function.\n",
    "\n",
    "We will implement a simple version of Policy improvement, which is done by making the policy greedy with respect to the current value function. In this case we have an action-value function, and\n",
    "therefore no model is needed to construct the greedy policy. For any action-value function q, the corresponding greedy policy is the one that, for each s ∈ S, deterministically chooses an action with maximal action-value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54397c78-2dfc-416a-b352-083242651b32",
   "metadata": {
    "tags": []
   },
   "source": [
    "![MCC](images/mcc-alg.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f4cdca-2d96-4ce0-a568-8a7d3ef8c3c7",
   "metadata": {},
   "source": [
    "### Implementaion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6a8a989-081d-4147-b357-6e3c5fcda74c",
   "metadata": {},
   "outputs": [],
   "source": [
    "presentation = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "704fb8be-c0a2-4272-ab39-1933a8eac78e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "from collections import deque\n",
    "\n",
    "import random\n",
    "import gym\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "\n",
    "from AI_agents.Environments.gym_problem import GymProblem\n",
    "from AI_agents.Search.best_first_search import a_star\n",
    "\n",
    "from IL.dataset import ImitationLearningDataset\n",
    "from IL.evaluation import evaluate_policy\n",
    "from IL.ipython_vis import animate_policy\n",
    "from IL.model import MLP\n",
    "from IL.training import train_torch_classifier_sgd\n",
    "import AI_agents.Search.utils as utils\n",
    "\n",
    "\n",
    "# initialize env\n",
    "env = gym.make(\"Taxi-v3\").env\n",
    "env.reset()\n",
    "\n",
    "PASSENGER_IN_TAXI = 4  # passenger idx when in taxi\n",
    "locs = env.unwrapped.locs  # environment locations\n",
    "\n",
    "# random seed\n",
    "seed = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc383ef4-b0c2-4474-ade7-f93c8eaee2c3",
   "metadata": {},
   "source": [
    "Use a policy that chooses an action randomaly at each step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7cc5dc85-ed30-43c4-986d-58f4fbbbd8e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TaxiMonteCarloPolicy:\n",
    "    def __init__(self):\n",
    "        # a container for the plan actions.\n",
    "        self.cur_plan = deque()\n",
    "    \n",
    "    def __call__(self, obs):\n",
    "        # if out of actions (finished previous plan), or if observation is not in current plan,\n",
    "        # create a new plan.\n",
    "        taxi_prob = GymProblem(env, env.unwrapped.s)\n",
    "        actions = list(taxi_prob.get_applicable_actions(utils.Node(utils.State(obs, False), None, None, 0)))\n",
    "        chosen_action = random.choice(actions)\n",
    "        return chosen_action\n",
    "    \n",
    "RandomTraveler = TaxiMonteCarloPolicy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "081d228b-7f05-405a-a426-8dc89f9e60de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code will run forever until it is interrupted\n",
    "#animate_policy(env, RandomTraveler)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7806f383-c2ae-443c-9d56-117ca0d3cf6d",
   "metadata": {},
   "source": [
    "We added a reward compenant to the trajectory, that is necessary to calculate best action per state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd66e6d1-c780-44b2-9aac-35b7968596c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trajectory struct\n",
    "class Trajectory:\n",
    "    def __init__(self, observations=None, actions=None, rewards=None):\n",
    "        self.observations = observations or []\n",
    "        self.actions = actions or []\n",
    "        self.rewards = rewards or []\n",
    "    \n",
    "    def add_step(self, observation, action, reward):\n",
    "        self.observations.append(observation)\n",
    "        self.actions.append(action)\n",
    "        self.rewards.append(reward)\n",
    "        \n",
    "    def __str__(self):\n",
    "        return 'trajectory: ' + str(list(zip(self.observations, self.actions)))\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return str(self)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e8452223-60e3-4395-8108-3f94b9302952",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "trajectory: [(429, 1), (329, 2), (349, 1), (249, 4), (249, 1), (149, 3), (149, 3), (149, 4), (149, 2), (169, 1), (69, 5), (69, 5), (69, 4), (69, 2), (89, 4), (89, 0), (189, 3), (169, 3), (149, 0), (249, 0), (349, 2), (349, 1), (249, 1), (149, 5), (149, 2), (169, 5), (169, 3), (149, 3), (149, 1), (49, 1), (49, 4), (49, 4), (49, 3), (49, 3), (49, 1), (49, 5), (49, 5), (49, 0), (149, 5), (149, 1), (49, 0), (149, 5), (149, 4), (149, 1), (49, 1), (49, 0), (149, 2), (169, 4), (169, 0), (269, 0), (369, 3), (369, 5), (369, 5), (369, 1), (269, 4), (269, 4), (269, 4), (269, 3), (249, 3), (229, 3), (209, 4), (209, 5), (209, 3), (209, 4), (209, 1), (109, 0), (209, 1), (109, 0), (209, 2), (229, 4), (229, 2), (249, 0), (349, 4), (349, 5), (349, 2), (349, 3), (329, 3), (329, 0), (429, 2), (449, 4), (449, 5), (449, 0), (449, 5), (449, 0), (449, 1), (349, 4), (349, 1), (249, 2), (269, 5), (269, 1), (169, 3), (149, 3), (149, 5), (149, 0), (249, 0), (349, 3), (329, 3), (329, 3), (329, 3), (329, 1), (229, 4), (229, 4), (229, 5), (229, 1), (129, 4), (129, 5), (129, 3), (109, 1), (9, 5), (9, 4), (9, 2), (29, 3), (9, 3), (9, 5), (9, 5), (9, 4), (9, 4), (9, 5), (9, 3), (9, 1), (9, 1), (9, 5), (9, 1), (9, 1), (9, 3), (9, 5), (9, 1), (9, 0), (109, 1), (9, 0), (109, 4), (109, 4), (109, 0), (209, 3), (209, 1), (109, 3), (109, 0), (209, 0), (309, 3), (309, 3), (309, 4), (309, 0), (409, 0), (409, 2), (409, 2), (409, 0), (409, 4), (417, 3), (417, 4), (417, 1), (317, 0), (417, 2), (417, 5), (409, 3), (409, 0), (409, 1), (309, 2), (309, 0), (409, 4), (417, 1), (317, 4), (317, 4), (317, 2), (317, 3), (317, 2), (317, 1), (217, 5), (217, 2), (237, 0), (337, 0), (437, 1), (337, 2), (357, 3), (337, 4), (337, 4), (337, 4), (337, 5), (337, 5), (337, 3), (337, 1), (237, 3), (217, 5), (217, 4), (217, 4), (217, 1), (117, 4), (117, 4), (117, 5), (117, 4), (117, 5), (117, 2), (137, 4), (137, 0), (237, 5), (237, 2), (257, 2), (277, 5), (277, 0), (377, 5), (377, 0), (477, 4), (477, 4), (477, 3), (477, 0), (477, 1), (377, 3), (377, 1), (277, 3), (257, 0), (357, 3), (337, 0), (437, 3), (437, 5), (437, 0), (437, 5), (437, 5), (437, 0), (437, 2), (457, 3), (437, 4), (437, 1), (337, 0), (437, 4), (437, 0), (437, 5), (437, 1), (337, 2), (357, 0), (457, 5), (457, 1), (357, 3), (337, 2), (357, 5), (357, 1), (257, 1), (157, 5), (157, 1), (57, 0), (157, 0), (257, 3), (237, 3), (217, 4), (217, 3), (217, 3), (217, 1), (117, 2), (137, 2), (137, 1), (37, 3), (17, 4), (17, 5), (1, 3), (1, 5), (1, 1), (1, 2), (21, 4), (21, 2), (21, 2), (21, 1), (21, 3), (1, 1), (1, 2), (21, 5), (21, 3), (1, 0), (101, 5), (101, 3), (101, 0), (201, 0), (301, 4), (301, 2), (301, 4), (301, 1), (201, 3), (201, 4), (201, 0), (301, 3), (301, 3), (301, 4), (301, 5), (301, 3), (301, 4), (301, 3), (301, 5), (301, 0), (401, 0), (401, 2), (401, 3), (401, 1), (301, 2), (301, 5), (301, 5), (301, 3), (301, 3), (301, 1), (201, 0), (301, 5), (301, 5), (301, 2), (301, 4), (301, 0), (401, 0), (401, 2), (401, 5), (401, 1), (301, 1), (201, 3), (201, 3), (201, 4), (201, 2), (221, 4), (221, 2), (241, 4), (241, 3), (221, 0), (321, 3), (321, 1), (221, 5), (221, 3), (201, 5), (201, 4), (201, 5), (201, 4), (201, 5), (201, 5), (201, 2), (221, 0), (321, 2), (341, 5), (341, 5), (341, 0), (441, 0), (441, 5), (441, 2), (441, 2), (441, 2), (441, 3), (421, 4), (421, 1), (321, 5), (321, 3), (321, 1), (221, 2), (241, 4), (241, 3), (221, 4), (221, 4), (221, 4), (221, 4), (221, 5), (221, 3), (201, 5), (201, 3), (201, 4), (201, 1), (101, 0), (201, 4), (201, 5), (201, 1), (101, 3), (101, 2), (121, 2), (121, 2), (121, 4), (121, 1), (21, 0), (121, 0), (221, 3), (201, 2), (221, 5), (221, 5), (221, 4), (221, 1), (121, 2), (121, 3), (101, 4), (101, 1), (1, 4), (17, 0), (117, 2), (137, 0), (237, 4), (237, 0), (337, 3), (337, 3), (337, 1), (237, 0), (337, 0), (437, 1), (337, 5), (337, 5), (337, 1), (237, 5), (237, 3), (217, 5), (217, 4), (217, 5), (217, 0), (317, 3), (317, 2), (317, 3), (317, 0), (417, 1), (317, 4), (317, 4), (317, 2), (317, 4), (317, 5), (317, 3), (317, 5), (317, 3), (317, 0), (417, 0), (417, 5), (409, 2), (409, 1), (309, 3), (309, 4), (309, 1), (209, 3), (209, 2), (229, 1), (129, 0), (229, 5), (229, 0), (329, 0), (429, 5), (429, 0), (429, 0), (429, 3), (429, 4), (429, 2), (449, 5), (449, 5), (449, 1), (349, 1), (249, 3), (229, 4), (229, 0), (329, 1), (229, 1), (129, 4), (129, 2), (129, 2), (129, 5), (129, 3), (109, 3), (109, 4), (109, 1), (9, 1), (9, 4), (9, 2), (29, 1), (29, 3), (9, 0), (109, 4), (109, 3), (109, 1), (9, 2), (29, 2), (29, 2), (29, 3), (9, 0), (109, 1), (9, 0), (109, 2), (129, 4), (129, 5), (129, 2), (129, 4), (129, 2), (129, 2), (129, 5), (129, 1), (29, 2), (29, 2), (29, 3), (9, 5), (9, 4), (9, 5), (9, 2), (29, 2), (29, 5), (29, 1), (29, 0), (129, 5), (129, 0), (229, 0), (329, 0), (429, 2), (449, 2), (449, 4), (449, 3), (429, 3), (429, 5), (429, 1), (329, 2), (349, 5), (349, 4), (349, 0)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_trajectory(policy, max_trajectory_length=float('inf')):\n",
    "    # init trajectory object\n",
    "    trajectory = Trajectory()\n",
    "    \n",
    "    # get first observation\n",
    "    obs = env.reset()\n",
    "    \n",
    "    # init first reward\n",
    "    reward = 0\n",
    "    # iterate and step in environment.\n",
    "    # limit num actions for incomplete policies\n",
    "    for i in itertools.count(start=1):\n",
    "        action = policy(obs)\n",
    "        old_obs = obs\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        trajectory.add_step(old_obs, action, reward)\n",
    "        \n",
    "        if done or i >= max_trajectory_length:\n",
    "            break\n",
    "    \n",
    "    return trajectory\n",
    "\n",
    "trajectory = get_trajectory(RandomTraveler, 500)\n",
    "trajectory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f9fdbe-4b60-4929-a985-3f940af90992",
   "metadata": {},
   "source": [
    "Collect episodes/trajectories "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ac7dd953-07c5-4bcf-84eb-d209fae2defc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_data(policy, num_trajectories, max_trajectory_length=float('inf')):\n",
    "    trajectories = []\n",
    "    for _ in range(num_trajectories):\n",
    "        trajectories.append(get_trajectory(policy, max_trajectory_length))\n",
    "\n",
    "    return trajectories\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "074f1408-04f3-4fc2-815d-c45327128a29",
   "metadata": {},
   "source": [
    "For each state: calculate the action with the max average return and set that action as the policy for that state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "802378f8-f6da-421b-848e-2b565ce8c990",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def build_decision_dict(raw_data):\n",
    "    state_action_scores = defaultdict(lambda: defaultdict(lambda: []))\n",
    "    for trajectory in raw_data:\n",
    "        reward_sum = 0\n",
    "        for state, action, reward in reversed(list(zip(trajectory.observations, trajectory.actions, trajectory.rewards))):\n",
    "            reward_sum += reward\n",
    "            state_action_scores[state][action].append(reward_sum)\n",
    "            \n",
    "    for state, action_values in state_action_scores.items():\n",
    "        for action, values_list in action_values.items():\n",
    "            state_action_scores[state][action] = np.mean(values_list)\n",
    "        state_action_scores[state] = max(state_action_scores[state], key=state_action_scores[state].get)\n",
    "    return state_action_scores\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f0d4dfa3-adac-4f0a-a0f7-b49ecef66c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCCPolicy:\n",
    "    def __init__(self, state_action_map):\n",
    "        self.state_action_map = state_action_map\n",
    "    \n",
    "    def __call__(self, obs):\n",
    "        # preprocess observation\n",
    "        return self.state_action_map[obs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3d5ab59d-5747-4e4e-9a3c-ce398194ba7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def calc_final_policy(learning_policy, num_trajectories, json_name=None):\n",
    "    if presentation:\n",
    "        if json_name is None:\n",
    "            raise Exception(\"Can't present without filename\")\n",
    "        with open(json_name + \".json\", 'r') as fp:\n",
    "            state_action_map = json.load(fp)\n",
    "        policy = MCCPolicy({int(key):value for key, value in state_action_map.items()})\n",
    "    else:\n",
    "        raw_data = collect_data(learning_policy, num_trajectories)\n",
    "        policy = MCCPolicy(build_decision_dict(raw_data))\n",
    "        if json_name is not None:\n",
    "            with open(json_name + \".json\", 'w') as fp:\n",
    "                json.dump(policy.state_action_map, fp)\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6067b316-db5f-4bab-beff-34c1be9b5d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the same trajectories every time!\n",
    "env.seed(seed)\n",
    "\n",
    "policy = calc_final_policy(RandomTraveler, 3000, \"mcc_3000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6c684816-e314-4463-a610-6d743bf64313",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1332b975ab94d6f82aa2f7a10bad3f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Monte Carlo Policy\n",
      "---------\n",
      "total reward over all episodes: -981999\n",
      "mean reward per episode:        -98.1999\n"
     ]
    }
   ],
   "source": [
    "total_reward, mean_reward = evaluate_policy(env, RandomTraveler, num_episodes=10000, seed=seed)\n",
    "print('Monte Carlo Policy')\n",
    "print('---------')\n",
    "print(f'total reward over all episodes: {total_reward}')\n",
    "print(f'mean reward per episode:        {mean_reward}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "92ffad55-a287-4eee-8e02-8b8547ced147",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5992c49a96d54cc7bc29cb66b0934e68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Monte Carlo Control Policy\n",
      "-----------------\n",
      "total reward over all episodes: -1095707\n",
      "mean reward per episode:        -109.5707\n"
     ]
    }
   ],
   "source": [
    "total_reward, mean_reward = evaluate_policy(env, policy, num_episodes=10000, seed=seed)\n",
    "print('Monte Carlo Control Policy')\n",
    "print('-----------------')\n",
    "print(f'total reward over all episodes: {total_reward}')\n",
    "print(f'mean reward per episode:        {mean_reward}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "279c1b91-6bf4-44b5-b4c8-eae595fd3192",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code will run forever until it is interrupted\n",
    "# animate_policy(env, policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "76160830-1b2c-4c3f-aa65-d3befba9190e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the same trajectories every time!\n",
    "env.seed(seed)\n",
    "\n",
    "policy = calc_final_policy(RandomTraveler, 30000, \"mcc_30000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "86e126b6-efdd-4fe8-bdc8-77b23a170c17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1281567b3a24411e9400ef420b1f3dcc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Monte Carlo Control Policy\n",
      "-----------------\n",
      "total reward over all episodes: -428390\n",
      "mean reward per episode:        -42.839\n"
     ]
    }
   ],
   "source": [
    "total_reward, mean_reward = evaluate_policy(env, policy, num_episodes=10000, seed=seed)\n",
    "print('Monte Carlo Control Policy')\n",
    "print('-----------------')\n",
    "print(f'total reward over all episodes: {total_reward}')\n",
    "print(f'mean reward per episode:        {mean_reward}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ec7bb54b-cdac-4a1b-a2e6-79245a126349",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code will run forever until it is interrupted\n",
    "# animate_policy(env, policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25e0a02d-57a5-49c0-b666-27672a25a116",
   "metadata": {},
   "source": [
    "We introduce an improvement to this algorithm:\n",
    "If an action from a certain state results in a step to that same state (no change by that action),\n",
    "it is removed from the possible choices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9af75b62-4f16-4908-9f68-5bb2cbde82c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_non_stationary_actions(taxi_prob, obs):\n",
    "    node = utils.Node(utils.State(obs, False), None, None, 0)\n",
    "    actions = list(taxi_prob.get_applicable_actions(node))\n",
    "    applicable_actions = []\n",
    "    for action in actions:\n",
    "        if taxi_prob.get_successors(action, node)[0].state.get_key() != obs:\n",
    "            applicable_actions.append(action)\n",
    "    return applicable_actions\n",
    "\n",
    "class TaxiMoneCarloNonStationaryPolicy:\n",
    "    def __init__(self):\n",
    "        # a container for the plan actions.\n",
    "        self.cur_plan = deque()\n",
    "    \n",
    "    def __call__(self, obs):\n",
    "        # if out of actions (finished previous plan), or if observation is not in current plan,\n",
    "        # create a new plan.\n",
    "        taxi_prob = GymProblem(env, env.unwrapped.s)\n",
    "        actions = get_non_stationary_actions(taxi_prob, obs)\n",
    "        chosen_action = random.choice(actions)\n",
    "        return chosen_action\n",
    "    \n",
    "nonstationary_policy = TaxiMoneCarloNonStationaryPolicy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0426bdb3-71e4-49bd-8863-772e50244d55",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "env.seed(seed)\n",
    "nonstationary_control_policy = calc_final_policy(nonstationary_policy, 3000, \"mcc_nonstationary_3000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3afed28f-074d-4169-97dd-afc9e42287e8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a76db863d59d4cf29f054b2a6da3a13f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Monte Carlo Control Nonstationary Policy\n",
      "-----------------\n",
      "total reward over all episodes: -76400\n",
      "mean reward per episode:        -7.64\n"
     ]
    }
   ],
   "source": [
    "total_reward, mean_reward = evaluate_policy(env, nonstationary_control_policy, num_episodes=10000, seed=seed)\n",
    "print('Monte Carlo Control Nonstationary Policy')\n",
    "print('-----------------')\n",
    "print(f'total reward over all episodes: {total_reward}')\n",
    "print(f'mean reward per episode:        {mean_reward}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4006e563-bd11-4c68-9bcf-77e951df2dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code will run forever until it is interrupted\n",
    "# animate_policy(env, nonstationary_control_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "75c28fe9-cf92-4479-a140-73f92164da7c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "env.seed(seed)\n",
    "nonstationary_control_policy = calc_final_policy(nonstationary_policy, 30000, \"mcc_nonstationary_30000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1c684820-4b2d-4320-90ae-68fb72289c71",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4de12a3e26fa408b904efc933dce0b0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Monte Carlo Control Nonstationary Policy\n",
      "-----------------\n",
      "total reward over all episodes: 49345\n",
      "mean reward per episode:        4.9345\n"
     ]
    }
   ],
   "source": [
    "total_reward, mean_reward = evaluate_policy(env, nonstationary_control_policy, num_episodes=10000, seed=seed)\n",
    "print('Monte Carlo Control Nonstationary Policy')\n",
    "print('-----------------')\n",
    "print(f'total reward over all episodes: {total_reward}')\n",
    "print(f'mean reward per episode:        {mean_reward}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4650ad0c-4e29-409f-b5e2-6017265b89e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code will run forever until it is interrupted\n",
    "# animate_policy(env, nonstationary_control_policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fae9107-314d-47f2-9d08-d29181e6a38e",
   "metadata": {},
   "source": [
    "This idea can be expanded to removing \"cycles\" from the the episode that do not result in positive reward - this requires a more complex implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c89294c1-d7b3-49ac-b096-f7691c3dfb5a",
   "metadata": {},
   "source": [
    "## Summation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e9e6d7-c896-4871-9413-d9f74d1989dd",
   "metadata": {},
   "source": [
    "We discussed using Monte Carlo methods for both prediction/evaluation and improvement of policies.\n",
    "The idea of Monte Carlo Control is to utilize both of these aspects in unison:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a73f4f4-4d69-4dac-aa0c-d6754cf587d0",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"images/mcc-cycle.png\" width=\"500\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50afabe4-e7bb-489b-a1ef-30e1041ed40c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
