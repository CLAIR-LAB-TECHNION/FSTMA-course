{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1d75303-a4f3-4da5-b936-f26277e6839e",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Monte Carlo Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c2ecdd-9d24-4ba9-99c7-22d400e05244",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"images/Drunk.jpg\" width=\"500\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f71b50bd-cace-4fed-ae34-8afbcf3e0421",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86acaaf7-46d5-403a-99f3-63de26ccc981",
   "metadata": {},
   "source": [
    "Monte Carlo methods are ways of solving the reinforcement learning problem based on averaging sample returns.  We assume experience is divided into episodes, and that all episodes eventually terminate no matter what actions are selected. Only on the completion of an episode are value estimates and policies changed. Monte Carlo methods can thus be incremental in an episode-by-episode sense, but not in a step-by-step (online) sense. The term “Monte Carlo” is often used more broadly for any estimation method whose operation involves a significant random component."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf3763f1-c8fb-4483-b63b-03c07fa3731c",
   "metadata": {},
   "source": [
    "In the following example we use a Monte Carlo method to estimate pi: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "401312f7-ebe8-4865-8e99-1afb6a856aa8",
   "metadata": {},
   "source": [
    "![Monte Carlo Prediction](images/pi_30k.webp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac811c2-b68e-45dc-9b80-31cd10bca1eb",
   "metadata": {},
   "source": [
    "## Monte Carlo Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d9a462e-ee75-462e-8b71-5a06f600fa52",
   "metadata": {},
   "source": [
    "Monte Carlo methods can be used for learning the state-value\n",
    "function for a *given* policy. The value of a state is the expected\n",
    "return — expected cumulative future (discounted) reward — starting from that\n",
    "state. An obvious way to estimate it from experience, is simply to\n",
    "average the returns observed after visits to that state. As more returns are\n",
    "observed, the average should converge to the expected value.    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "502f7bc6-70aa-4092-be09-67fde5c021d6",
   "metadata": {},
   "source": [
    "![Monte Carlo Prediction](images/mc-pred.png)\n",
    "[Reinforcement Learning: An Introduction - Sutton and Barto]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9297b5f4-267f-41b4-87d0-ff2f53242aaa",
   "metadata": {},
   "source": [
    "## Monte Carlo Control"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5678a7a2-fe56-404c-ba42-262e108f9b3b",
   "metadata": {},
   "source": [
    "Monte Carlo estimation can be used in control, that is, to approximate optimal policies. The overall idea is to proceed according to the idea of generalized policy iteration (GPI). In GPI one maintains both an\n",
    "approximate policy and an approximate value function. The value function is\n",
    "repeatedly altered to more closely approximate the value function for the current policy, and the policy is repeatedly improved with respect to the current value function.\n",
    "\n",
    "We will implement a simple version of Policy improvement, which is done by making the policy greedy with respect to the current value function. In this case we have an action-value function, and\n",
    "therefore no model is needed to construct the greedy policy. For any action-value function q, the corresponding greedy policy is the one that, for each s ∈ S, deterministically chooses an action with maximal action-value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54397c78-2dfc-416a-b352-083242651b32",
   "metadata": {
    "tags": []
   },
   "source": [
    "![MCC](images/mcc-alg.png)\n",
    "[Reinforcement Learning: An Introduction - Sutton and Barto]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f4cdca-2d96-4ce0-a568-8a7d3ef8c3c7",
   "metadata": {},
   "source": [
    "### Implementaion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6a8a989-081d-4147-b357-6e3c5fcda74c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Presentation mode: If true will load saved policy dictionaries, otherwise will generate\n",
    "## new trajectories to build a policy(may take a long time)\n",
    "presentation = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78ecb456-4c15-418e-b47f-89161d7ca009",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/sarah-keren/AI_agents\n",
      "  Cloning https://github.com/sarah-keren/AI_agents to c:\\users\\shyur\\appdata\\local\\temp\\pip-req-build-w00getx1\n",
      "  Resolved https://github.com/sarah-keren/AI_agents to commit e325c1e2ab248717665822a03c60c1dd8d067a90\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Installing backend dependencies: started\n",
      "  Installing backend dependencies: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: gym==0.23.0 in d:\\anaconda3\\envs\\fstma-tut03\\lib\\site-packages (from AI-agents==0.0.0) (0.23.0)\n",
      "Requirement already satisfied: pygame==2.1.2 in d:\\anaconda3\\envs\\fstma-tut03\\lib\\site-packages (from AI-agents==0.0.0) (2.1.2)\n",
      "Requirement already satisfied: gym-notices>=0.0.4 in d:\\anaconda3\\envs\\fstma-tut03\\lib\\site-packages (from gym==0.23.0->AI-agents==0.0.0) (0.0.6)\n",
      "Requirement already satisfied: numpy>=1.18.0 in d:\\anaconda3\\envs\\fstma-tut03\\lib\\site-packages (from gym==0.23.0->AI-agents==0.0.0) (1.22.3)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in d:\\anaconda3\\envs\\fstma-tut03\\lib\\site-packages (from gym==0.23.0->AI-agents==0.0.0) (2.0.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.10.0 in d:\\anaconda3\\envs\\fstma-tut03\\lib\\site-packages (from gym==0.23.0->AI-agents==0.0.0) (4.11.3)\n",
      "Requirement already satisfied: zipp>=0.5 in d:\\anaconda3\\envs\\fstma-tut03\\lib\\site-packages (from importlib-metadata>=4.10.0->gym==0.23.0->AI-agents==0.0.0) (3.8.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Running command git clone --filter=blob:none --quiet https://github.com/sarah-keren/AI_agents 'C:\\Users\\shyur\\AppData\\Local\\Temp\\pip-req-build-w00getx1'\n"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/sarah-keren/AI_agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "704fb8be-c0a2-4272-ab39-1933a8eac78e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "import random\n",
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "from AI_agents.Environments.gym_problem import GymProblem\n",
    "\n",
    "from Utilities.evaluation import evaluate_policy\n",
    "from Utilities.ipython_vis import animate_policy\n",
    "import AI_agents.Search.utils as utils\n",
    "\n",
    "\n",
    "# initialize env\n",
    "env = gym.make(\"Taxi-v3\").env\n",
    "env.reset()\n",
    "\n",
    "PASSENGER_IN_TAXI = 4  # passenger idx when in taxi\n",
    "locs = env.unwrapped.locs  # environment locations\n",
    "\n",
    "# random seed\n",
    "seed = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc383ef4-b0c2-4474-ade7-f93c8eaee2c3",
   "metadata": {},
   "source": [
    "Use a policy that chooses an action randomaly at each step in order to generate a trajectory database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7cc5dc85-ed30-43c4-986d-58f4fbbbd8e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TaxiMonteCarloPolicy:\n",
    "    def __call__(self, obs):\n",
    "        # if out of actions (finished previous plan), or if observation is not in current plan,\n",
    "        # create a new plan.\n",
    "        taxi_prob = GymProblem(env, env.unwrapped.s)\n",
    "        actions = list(taxi_prob.get_applicable_actions(utils.Node(utils.State(obs, False), None, None, 0)))\n",
    "        chosen_action = random.choice(actions)\n",
    "        return chosen_action\n",
    "    \n",
    "RandomTraveler = TaxiMonteCarloPolicy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d805f7-4f1d-43e0-9d32-36ed794ac049",
   "metadata": {},
   "source": [
    "Animation of a random trajectory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "081d228b-7f05-405a-a426-8dc89f9e60de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code will run forever until it is interrupted\n",
    "# animate_policy(env, RandomTraveler)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7806f383-c2ae-443c-9d56-117ca0d3cf6d",
   "metadata": {},
   "source": [
    "We use trajectory and episode interchangeably in this context.\n",
    "We added a reward component to the trajectory, that is necessary to calculate best action per state.\n",
    "\n",
    "Trajectory steps are represented as (observation/state, action, reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd66e6d1-c780-44b2-9aac-35b7968596c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trajectory struct\n",
    "class Trajectory:\n",
    "    def __init__(self, observations=None, actions=None, rewards=None):\n",
    "        self.observations = observations or []\n",
    "        self.actions = actions or []\n",
    "        self.rewards = rewards or []\n",
    "    \n",
    "    def add_step(self, observation, action, reward):\n",
    "        self.observations.append(observation)\n",
    "        self.actions.append(action)\n",
    "        self.rewards.append(reward)\n",
    "        \n",
    "    def __str__(self):\n",
    "        return 'trajectory: ' + str(list(zip(self.observations, self.actions, self.rewards)))\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return str(self)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b1e232-f555-491c-ab18-27c26e0c3ad0",
   "metadata": {},
   "source": [
    "A function for creating one episode/trajectory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e8452223-60e3-4395-8108-3f94b9302952",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "trajectory: [(92, 2, -1), (92, 4, -10), (92, 5, -10), (92, 0, -1), (192, 5, -10), (192, 3, -1), (172, 3, -1), (152, 0, -1), (252, 2, -1), (272, 4, -10), (272, 4, -10), (272, 5, -10), (272, 5, -10), (272, 1, -1), (172, 3, -1), (152, 2, -1), (172, 1, -1), (72, 4, -10), (72, 2, -1), (92, 0, -1), (192, 0, -1), (292, 2, -1), (292, 2, -1), (292, 4, -10), (292, 2, -1), (292, 2, -1), (292, 2, -1), (292, 0, -1), (392, 5, -10), (392, 1, -1), (292, 1, -1), (192, 0, -1), (292, 0, -1), (392, 3, -1), (372, 0, -1), (472, 3, -1), (472, 4, -1), (476, 1, -1), (376, 2, -1), (396, 4, -10), (396, 5, -10), (396, 5, -10), (396, 4, -10), (396, 5, -10), (396, 0, -1), (496, 4, -10), (496, 5, -10), (496, 1, -1), (396, 1, -1), (296, 1, -1), (196, 4, -10), (196, 2, -1), (196, 4, -10), (196, 5, -10), (196, 4, -10), (196, 3, -1), (176, 2, -1), (196, 5, -10), (196, 0, -1), (296, 2, -1), (296, 5, -10), (296, 5, -10), (296, 2, -1), (296, 0, -1), (396, 5, -10), (396, 0, -1), (496, 5, -10), (496, 4, -10), (496, 1, -1), (396, 4, -10), (396, 5, -10), (396, 0, -1), (496, 3, -1), (476, 1, -1), (376, 2, -1), (396, 1, -1), (296, 1, -1), (196, 5, -10), (196, 5, -10), (196, 0, -1), (296, 4, -10), (296, 1, -1), (196, 4, -10), (196, 4, -10), (196, 5, -10), (196, 2, -1), (196, 0, -1), (296, 4, -10), (296, 5, -10), (296, 5, -10), (296, 3, -1), (276, 1, -1), (176, 0, -1), (276, 0, -1), (376, 5, -10), (376, 4, -10), (376, 0, -1), (476, 1, -1), (376, 0, -1), (476, 4, -10), (476, 4, -10), (476, 3, -1), (476, 4, -10), (476, 2, -1), (496, 5, -10), (496, 1, -1), (396, 5, -10), (396, 0, -1), (496, 2, -1), (496, 2, -1), (496, 0, -1), (496, 0, -1), (496, 4, -10), (496, 0, -1), (496, 1, -1), (396, 1, -1), (296, 4, -10), (296, 0, -1), (396, 2, -1), (396, 0, -1), (496, 2, -1), (496, 2, -1), (496, 2, -1), (496, 1, -1), (396, 0, -1), (496, 4, -10), (496, 4, -10), (496, 1, -1), (396, 2, -1), (396, 0, -1), (496, 1, -1), (396, 2, -1), (396, 0, -1), (496, 2, -1), (496, 2, -1), (496, 1, -1), (396, 0, -1), (496, 2, -1), (496, 5, -10), (496, 0, -1), (496, 1, -1), (396, 4, -10), (396, 1, -1), (296, 3, -1), (276, 3, -1), (256, 3, -1), (236, 0, -1), (336, 0, -1), (436, 1, -1), (336, 0, -1), (436, 2, -1), (456, 3, -1), (436, 3, -1), (436, 2, -1), (456, 2, -1), (456, 4, -10), (456, 2, -1), (456, 0, -1), (456, 2, -1), (456, 3, -1), (436, 0, -1), (436, 3, -1), (436, 4, -10), (436, 2, -1), (456, 2, -1), (456, 3, -1), (436, 5, -10), (436, 5, -10), (436, 1, -1), (336, 1, -1), (236, 0, -1), (336, 5, -10), (336, 0, -1), (436, 2, -1), (456, 2, -1), (456, 4, -10), (456, 0, -1), (456, 4, -10), (456, 1, -1), (356, 4, -10), (356, 4, -10), (356, 2, -1), (356, 5, -10), (356, 5, -10), (356, 3, -1), (336, 3, -1), (336, 3, -1), (336, 3, -1), (336, 5, -10), (336, 1, -1), (236, 5, -10), (236, 0, -1), (336, 3, -1), (336, 4, -10), (336, 4, -10), (336, 2, -1), (356, 5, -10), (356, 4, -10), (356, 2, -1), (356, 0, -1), (456, 3, -1), (436, 0, -1), (436, 2, -1), (456, 1, -1), (356, 0, -1), (456, 4, -10), (456, 3, -1), (436, 0, -1), (436, 5, -10), (436, 3, -1), (436, 1, -1), (336, 1, -1), (236, 5, -10), (236, 2, -1), (256, 2, -1), (276, 3, -1), (256, 5, -10), (256, 1, -1), (156, 1, -1), (56, 4, -10), (56, 1, -1), (56, 0, -1), (156, 0, -1), (256, 3, -1), (236, 3, -1), (216, 0, -1), (316, 5, -10), (316, 3, -1), (316, 5, -10), (316, 1, -1), (216, 4, -10), (216, 0, -1), (316, 5, -10), (316, 4, -10), (316, 2, -1), (316, 3, -1), (316, 3, -1), (316, 5, -10), (316, 4, -10), (316, 2, -1), (316, 4, -10), (316, 2, -1), (316, 0, -1), (416, 4, -10), (416, 5, -1), (408, 5, -10), (408, 2, -1), (408, 4, -1), (416, 0, -1), (416, 1, -1), (316, 2, -1), (316, 0, -1), (416, 4, -10), (416, 0, -1), (416, 5, -1), (408, 4, -1), (416, 2, -1), (416, 0, -1), (416, 2, -1), (416, 5, -1), (408, 4, -1), (416, 1, -1), (316, 0, -1), (416, 2, -1), (416, 5, -1), (408, 1, -1), (308, 3, -1), (308, 4, -10), (308, 1, -1), (208, 4, -10), (208, 5, -10), (208, 2, -1), (228, 5, -10), (228, 0, -1), (328, 5, -10), (328, 1, -1), (228, 5, -10), (228, 5, -10), (228, 4, -10), (228, 4, -10), (228, 1, -1), (128, 0, -1), (228, 5, -10), (228, 1, -1), (128, 5, -10), (128, 1, -1), (28, 4, -10), (28, 0, -1), (128, 2, -1), (128, 0, -1), (228, 4, -10), (228, 4, -10), (228, 0, -1), (328, 5, -10), (328, 0, -1), (428, 2, -1), (448, 2, -1), (448, 0, -1), (448, 1, -1), (348, 1, -1), (248, 3, -1), (228, 2, -1), (248, 5, -10), (248, 4, -10), (248, 2, -1), (268, 2, -1), (288, 4, -10), (288, 2, -1), (288, 4, -10), (288, 5, -10), (288, 2, -1), (288, 4, -10), (288, 1, -1), (188, 2, -1), (188, 4, -10), (188, 0, -1), (288, 3, -1), (268, 3, -1), (248, 1, -1), (148, 3, -1), (148, 1, -1), (48, 1, -1), (48, 4, -10), (48, 1, -1), (48, 4, -10), (48, 5, -10), (48, 1, -1), (48, 1, -1), (48, 5, -10), (48, 4, -10), (48, 2, -1), (68, 3, -1), (48, 2, -1), (68, 3, -1), (48, 0, -1), (148, 1, -1), (48, 1, -1), (48, 5, -10), (48, 4, -10), (48, 5, -10), (48, 0, -1), (148, 0, -1), (248, 3, -1), (228, 3, -1), (208, 3, -1), (208, 0, -1), (308, 3, -1), (308, 4, -10), (308, 1, -1), (208, 5, -10), (208, 5, -10), (208, 4, -10), (208, 3, -1), (208, 3, -1), (208, 4, -10), (208, 2, -1), (228, 1, -1), (128, 3, -1), (108, 4, -10), (108, 2, -1), (128, 4, -10), (128, 0, -1), (228, 4, -10), (228, 1, -1), (128, 1, -1), (28, 1, -1), (28, 4, -10), (28, 3, -1), (8, 3, -1), (8, 5, -10), (8, 2, -1), (28, 1, -1), (28, 3, -1), (8, 0, -1), (108, 1, -1), (8, 1, -1), (8, 1, -1), (8, 3, -1), (8, 1, -1), (8, 4, -10), (8, 5, -10), (8, 2, -1), (28, 2, -1), (28, 0, -1), (128, 0, -1), (228, 3, -1), (208, 5, -10), (208, 2, -1), (228, 3, -1), (208, 1, -1), (108, 1, -1), (8, 2, -1), (28, 5, -10), (28, 2, -1), (28, 4, -10), (28, 1, -1), (28, 3, -1), (8, 1, -1), (8, 2, -1), (28, 0, -1), (128, 3, -1), (108, 0, -1), (208, 4, -10), (208, 5, -10), (208, 4, -10), (208, 1, -1), (108, 3, -1), (108, 3, -1), (108, 3, -1), (108, 5, -10), (108, 2, -1), (128, 4, -10), (128, 1, -1), (28, 1, -1), (28, 4, -10), (28, 4, -10), (28, 0, -1), (128, 2, -1), (128, 4, -10), (128, 3, -1), (108, 2, -1), (128, 1, -1), (28, 2, -1), (28, 0, -1), (128, 5, -10), (128, 3, -1), (108, 5, -10), (108, 5, -10), (108, 4, -10), (108, 4, -10), (108, 4, -10), (108, 5, -10), (108, 1, -1), (8, 5, -10), (8, 0, -1), (108, 5, -10), (108, 3, -1), (108, 0, -1), (208, 2, -1), (228, 3, -1), (208, 1, -1), (108, 3, -1), (108, 5, -10), (108, 0, -1), (208, 0, -1), (308, 0, -1), (408, 5, -10), (408, 0, -1), (408, 3, -1), (408, 1, -1), (308, 4, -10), (308, 3, -1), (308, 4, -10), (308, 1, -1), (208, 0, -1), (308, 5, -10), (308, 3, -1), (308, 3, -1), (308, 3, -1), (308, 1, -1), (208, 3, -1), (208, 2, -1), (228, 3, -1), (208, 1, -1), (108, 5, -10), (108, 2, -1), (128, 5, -10), (128, 5, -10), (128, 5, -10), (128, 1, -1), (28, 2, -1), (28, 2, -1), (28, 2, -1), (28, 5, -10), (28, 0, -1), (128, 1, -1), (28, 4, -10), (28, 4, -10), (28, 4, -10), (28, 5, -10), (28, 5, -10), (28, 0, -1), (128, 1, -1), (28, 2, -1), (28, 4, -10), (28, 0, -1), (128, 3, -1), (108, 0, -1), (208, 2, -1), (228, 0, -1), (328, 1, -1), (228, 0, -1), (328, 1, -1), (228, 0, -1), (328, 0, -1), (428, 5, -10), (428, 5, -10), (428, 1, -1), (328, 0, -1), (428, 3, -1)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_trajectory(policy, max_trajectory_length=float('inf')):\n",
    "    # init trajectory object\n",
    "    trajectory = Trajectory()\n",
    "    \n",
    "    # get first observation\n",
    "    obs = env.reset()\n",
    "    \n",
    "    # init first reward\n",
    "    reward = 0\n",
    "    # iterate and step in environment.\n",
    "    # limit num actions for incomplete policies\n",
    "    for i in itertools.count(start=1):\n",
    "        action = policy(obs)\n",
    "        # we register the observation with the action that acted upon it\n",
    "        # and the reward it got, and save the next observation\n",
    "        old_obs = obs\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        trajectory.add_step(old_obs, action, reward)\n",
    "        \n",
    "        if done or i >= max_trajectory_length:\n",
    "            break\n",
    "    \n",
    "    return trajectory\n",
    "\n",
    "trajectory = get_trajectory(RandomTraveler, 500)\n",
    "trajectory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f9fdbe-4b60-4929-a985-3f940af90992",
   "metadata": {},
   "source": [
    "Collect episodes/trajectories "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ac7dd953-07c5-4bcf-84eb-d209fae2defc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_data(policy, num_trajectories, max_trajectory_length=float('inf')):\n",
    "    trajectories = []\n",
    "    for _ in range(num_trajectories):\n",
    "        trajectories.append(get_trajectory(policy, max_trajectory_length))\n",
    "\n",
    "    return trajectories\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "074f1408-04f3-4fc2-815d-c45327128a29",
   "metadata": {},
   "source": [
    "For each state: calculate the action with the max average return and set that action as the policy for that state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "802378f8-f6da-421b-848e-2b565ce8c990",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def build_decision_dict(raw_data):\n",
    "    # Nested dictionary for: State -> Action -> Reward List \n",
    "    state_action_scores = defaultdict(lambda: defaultdict(lambda: []))\n",
    "    for trajectory in raw_data:\n",
    "        reward_sum = 0\n",
    "        # iterate backwards to calculate the return G of each observed state action pair\n",
    "        for state, action, reward in reversed(list(zip(trajectory.observations, trajectory.actions, trajectory.rewards))):\n",
    "            reward_sum += reward\n",
    "            state_action_scores[state][action].append(reward_sum)\n",
    "            \n",
    "    for state, action_values in state_action_scores.items():\n",
    "        for action, values_list in action_values.items():\n",
    "            # Calculate the mean of all returns for a state action pair\n",
    "            state_action_scores[state][action] = np.mean(values_list)\n",
    "        # For each state choose the action with the highest mean return\n",
    "        state_action_scores[state] = max(state_action_scores[state], key=state_action_scores[state].get)\n",
    "    return state_action_scores\n",
    "    \n",
    "class MCCPolicy:\n",
    "    def __init__(self, state_action_map):\n",
    "        self.state_action_map = state_action_map\n",
    "    \n",
    "    def __call__(self, obs):\n",
    "        # preprocess observation\n",
    "        return self.state_action_map[obs]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c50254c8-17fa-4a72-a57d-a9b35c91012d",
   "metadata": {},
   "source": [
    "Construct a learned policy from the state-action map we get from calculating mean returns with the training policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3d5ab59d-5747-4e4e-9a3c-ce398194ba7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def calc_final_policy(learning_policy, num_trajectories, json_name=None):\n",
    "    # If presentation flag is on, json_name will be loaded from the environment (if it exists)\n",
    "    # otherwise new num_trajectories trajectories will be generated to train the policy\n",
    "    if presentation:\n",
    "        if json_name is None:\n",
    "            raise Exception(\"Can't present without filename\")\n",
    "        with open(json_name + \".json\", 'r') as fp:\n",
    "            state_action_map = json.load(fp)\n",
    "        policy = MCCPolicy({int(key):value for key, value in state_action_map.items()})\n",
    "    else:\n",
    "        raw_data = collect_data(learning_policy, num_trajectories)\n",
    "        policy = MCCPolicy(build_decision_dict(raw_data))\n",
    "        if json_name is not None:\n",
    "            with open(json_name + \".json\", 'w') as fp:\n",
    "                json.dump(policy.state_action_map, fp)\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca62490-09d9-49f8-a870-73609896e4bb",
   "metadata": {},
   "source": [
    "Generate a policy with 3000 episodes and evaluate its performance. We can see that it's not very good:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6067b316-db5f-4bab-beff-34c1be9b5d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the same trajectories every time!\n",
    "env.seed(seed)\n",
    "\n",
    "policy = calc_final_policy(RandomTraveler, 3000, \"mcc_3000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6c684816-e314-4463-a610-6d743bf64313",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bc36498524141f3849dced0469233ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Monte Carlo Policy\n",
      "---------\n",
      "total reward over all episodes: -983168\n",
      "mean reward per episode:        -98.3168\n"
     ]
    }
   ],
   "source": [
    "total_reward, mean_reward = evaluate_policy(env, RandomTraveler, num_episodes=10000, seed=seed)\n",
    "print('Monte Carlo Policy')\n",
    "print('---------')\n",
    "print(f'total reward over all episodes: {total_reward}')\n",
    "print(f'mean reward per episode:        {mean_reward}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "92ffad55-a287-4eee-8e02-8b8547ced147",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a3e4130422942be9e431093d8ba53dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Monte Carlo Control Policy\n",
      "-----------------\n",
      "total reward over all episodes: -1095707\n",
      "mean reward per episode:        -109.5707\n"
     ]
    }
   ],
   "source": [
    "total_reward, mean_reward = evaluate_policy(env, policy, num_episodes=10000, seed=seed)\n",
    "print('Monte Carlo Control Policy')\n",
    "print('-----------------')\n",
    "print(f'total reward over all episodes: {total_reward}')\n",
    "print(f'mean reward per episode:        {mean_reward}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "279c1b91-6bf4-44b5-b4c8-eae595fd3192",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code will run forever until it is interrupted\n",
    "# animate_policy(env, policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f4a28c0-80ce-4b2a-aba2-7fc935ca38cd",
   "metadata": {},
   "source": [
    "Generate a policy with 30000. We can see it achieves better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "76160830-1b2c-4c3f-aa65-d3befba9190e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the same trajectories every time!\n",
    "env.seed(seed)\n",
    "\n",
    "policy = calc_final_policy(RandomTraveler, 30000, \"mcc_30000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "86e126b6-efdd-4fe8-bdc8-77b23a170c17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80aa8ad49cb44b92b2a8ff1d0e65d9db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Monte Carlo Control Policy\n",
      "-----------------\n",
      "total reward over all episodes: -428390\n",
      "mean reward per episode:        -42.839\n"
     ]
    }
   ],
   "source": [
    "total_reward, mean_reward = evaluate_policy(env, policy, num_episodes=10000, seed=seed)\n",
    "print('Monte Carlo Control Policy')\n",
    "print('-----------------')\n",
    "print(f'total reward over all episodes: {total_reward}')\n",
    "print(f'mean reward per episode:        {mean_reward}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ec7bb54b-cdac-4a1b-a2e6-79245a126349",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code will run forever until it is interrupted\n",
    "#animate_policy(env, policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25e0a02d-57a5-49c0-b666-27672a25a116",
   "metadata": {},
   "source": [
    "### Non-Stationary improvement\n",
    "\n",
    "We noticed that in the previous results the agent tends to get stuck on performing dead-end actions, i.e actions that result in transitioning to the same state.\n",
    "\n",
    "We introduce an improvement to the algorithm:\n",
    "If an action from a certain state results in a step to that same state (no change by that action), it is removed from the possible choices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9af75b62-4f16-4908-9f68-5bb2cbde82c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_non_stationary_actions(taxi_prob, obs):\n",
    "    node = utils.Node(utils.State(obs, False), None, None, 0)\n",
    "    actions = list(taxi_prob.get_applicable_actions(node))\n",
    "    applicable_actions = []\n",
    "    # if an action results in staying in the same state i.e it's stationary, we remove\n",
    "    # that action.\n",
    "    for action in actions:\n",
    "        if taxi_prob.get_successors(action, node)[0].state.get_key() != obs:\n",
    "            applicable_actions.append(action)\n",
    "    return applicable_actions\n",
    "\n",
    "class TaxiMoneCarloNonStationaryPolicy:\n",
    "    \n",
    "    def __call__(self, obs):\n",
    "        # if out of actions (finished previous plan), or if observation is not in current plan,\n",
    "        # create a new plan.\n",
    "        taxi_prob = GymProblem(env, env.unwrapped.s)\n",
    "        actions = get_non_stationary_actions(taxi_prob, obs)\n",
    "        chosen_action = random.choice(actions)\n",
    "        return chosen_action\n",
    "    \n",
    "nonstationary_policy = TaxiMoneCarloNonStationaryPolicy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f79b294-5815-4ecc-a29d-9e3113f877f0",
   "metadata": {},
   "source": [
    "Generate a policy with 3000. It is already much better than the original run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0426bdb3-71e4-49bd-8863-772e50244d55",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "env.seed(seed)\n",
    "nonstationary_control_policy = calc_final_policy(nonstationary_policy, 3000, \"mcc_nonstationary_3000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3afed28f-074d-4169-97dd-afc9e42287e8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8f1252436e140ea9f944dbe0db2ccee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Monte Carlo Control Nonstationary Policy\n",
      "-----------------\n",
      "total reward over all episodes: -76400\n",
      "mean reward per episode:        -7.64\n"
     ]
    }
   ],
   "source": [
    "total_reward, mean_reward = evaluate_policy(env, nonstationary_control_policy, num_episodes=10000, seed=seed)\n",
    "print('Monte Carlo Control Nonstationary Policy')\n",
    "print('-----------------')\n",
    "print(f'total reward over all episodes: {total_reward}')\n",
    "print(f'mean reward per episode:        {mean_reward}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4006e563-bd11-4c68-9bcf-77e951df2dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code will run forever until it is interrupted\n",
    "#animate_policy(env, nonstationary_control_policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a792010-c074-4bda-8ae0-3ebe008d0148",
   "metadata": {},
   "source": [
    "Generate a policy with 30000 trajectories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "75c28fe9-cf92-4479-a140-73f92164da7c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "env.seed(seed)\n",
    "nonstationary_control_policy = calc_final_policy(nonstationary_policy, 30000, \"mcc_nonstationary_30000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1c684820-4b2d-4320-90ae-68fb72289c71",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06367f47d40a45f28f3eb7deeaf6eb48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Monte Carlo Control Nonstationary Policy\n",
      "-----------------\n",
      "total reward over all episodes: 49345\n",
      "mean reward per episode:        4.9345\n"
     ]
    }
   ],
   "source": [
    "total_reward, mean_reward = evaluate_policy(env, nonstationary_control_policy, num_episodes=10000, seed=seed)\n",
    "print('Monte Carlo Control Nonstationary Policy')\n",
    "print('-----------------')\n",
    "print(f'total reward over all episodes: {total_reward}')\n",
    "print(f'mean reward per episode:        {mean_reward}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4650ad0c-4e29-409f-b5e2-6017265b89e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code will run forever until it is interrupted\n",
    "# animate_policy(env, nonstationary_control_policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fae9107-314d-47f2-9d08-d29181e6a38e",
   "metadata": {},
   "source": [
    "This idea can be expanded to removing \"cycles\" from the the episode that do not result in positive reward - this requires a more complex implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c89294c1-d7b3-49ac-b096-f7691c3dfb5a",
   "metadata": {},
   "source": [
    "## Summation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e9e6d7-c896-4871-9413-d9f74d1989dd",
   "metadata": {},
   "source": [
    "We discussed using Monte Carlo methods for both prediction/evaluation and improvement of policies.\n",
    "The idea of Monte Carlo Control is to utilize both of these aspects in unison:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a73f4f4-4d69-4dac-aa0c-d6754cf587d0",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"images/mcc-cycle.png\" width=\"400\"/>\n",
    "</div>\n",
    "[Reinforcement Learning: An Introduction - Sutton and Barto]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50afabe4-e7bb-489b-a1ef-30e1041ed40c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
